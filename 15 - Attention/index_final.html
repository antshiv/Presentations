<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title>Attention Is All You Need - The Definitive Implementation Guide</title>

    <link rel="stylesheet" href="../reveal.js/dist/reset.css">
    <link rel="stylesheet" href="../reveal.js/dist/reveal.css">
    <link rel="stylesheet" href="../reveal.js/dist/theme/black.css" id="theme">

    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="../reveal.js/plugin/highlight/monokai.css">
    <link rel="stylesheet" href="../css/styles.css">
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/3.9.1/gsap.min.js"></script>

    <style>
        :root { --r-main-font-size: 24px; }
        .reveal .slides section { font-size: 0.9em; text-align: left;}
        .reveal h1, .reveal h2, .reveal h3, .reveal h4 { text-transform: none; text-align: center;}
        .reveal p { text-align: center; }
        .formula-box { background-color: rgba(45, 51, 59, 0.8); border-radius: 15px; padding: 15px; margin-top: 15px; border: 1px solid #484f58; }
        .dim-table { margin: 15px auto; font-size: 0.75em; border-collapse: collapse; }
        .dim-table th, .dim-table td { border: 1px solid #484f58; padding: 8px 15px; }
        .dim-table th { background-color: #37474f; }
        .transformer-block { display: flex; flex-direction: column; align-items: center; gap: 10px; margin-top: 20px;}
        .block-component { border: 2px solid #484f58; border-radius: 10px; padding: 10px 20px; width: 400px; text-align: center; background-color: #2d333b; }
        .block-component.highlight { border-color: #ff6f00; background-color: #4d3c20; box-shadow: 0 0 15px #ff6f00; }
        .arrow-down { width: 0; height: 0; border-left: 15px solid transparent; border-right: 15px solid transparent; border-top: 20px solid #484f58; }

        /* Visualization & Memory Styles */
        .viz-container { display: flex; justify-content: space-around; align-items: center; gap: 20px; width: 100%; margin-top: 20px;}
        .grid-container { position: relative; text-align: center;}
        .heatmap-grid { display: grid; gap: 1px; border: 1px solid #666; margin: 0 auto;}
        .heatmap-cell { background-color: #4a90e2; }
        .legend { color: #ccc; font-size: 0.7em; margin-top: 5px;}
        .legend-y { writing-mode: vertical-rl; transform: rotate(180deg); position: absolute; left: -40px; top: 50%; transform-origin: center; }
        .legend-x { position: absolute; top: -30px; left: 50%; transform: translateX(-50%); }
        .op-label { font-size: 2.5em; color: #ccc; align-self: center;}
        .heads-container { display: grid; grid-template-columns: repeat(4, 1fr); gap: 15px; }
        .memory-layout { width: 48%; background-color: #1e1e1e; padding: 15px; border-radius: 10px; border: 1px solid #444;}
        .memory-bar { display: flex; flex-wrap: wrap; border: 2px solid #888; background: #111; padding: 2px; border-radius: 5px; min-height: 40px;}
        .mem-block { height: 20px; box-sizing: border-box; border: 1px solid #333; transition: all 0.3s;}
        .head-color-0 { background-color: #e57373; } .head-color-1 { background-color: #81c784; } .head-color-2 { background-color: #64b5f6; } .head-color-3 { background-color: #ffd54f; }
        .head-color-4 { background-color: #ba68c8; } .head-color-5 { background-color: #ff8a65; } .head-color-6 { background-color: #a1887f; } .head-color-7 { background-color: #90a4ae; }
        .code-block-small { font-size: 0.7em !important; }
        .code-block-tiny { font-size: 0.6em !important; line-height: 1.2; }
        .highlight-box { border: 3px solid #ff6f00 !important; box-shadow: 0 0 10px #ff6f00; }
        
        /* Memory Flow Visualization Styles */
        .memory-flow-container {
            display: flex;
            flex-direction: column;
            gap: 30px;
            margin: 20px 0;
        }
        
        .phase-container {
            display: flex;
            justify-content: space-between;
            align-items: flex-start;
            gap: 20px;
            padding: 20px;
            border: 2px solid #444;
            border-radius: 15px;
            background: rgba(68, 68, 68, 0.1);
        }
        
        .memory-layout-viz {
            width: 30%;
            background: #1a1a1a;
            border-radius: 10px;
            padding: 15px;
            border: 1px solid #555;
        }
        
        .core-distribution {
            width: 35%;
            background: #1a1a1a;
            border-radius: 10px;
            padding: 15px;
            border: 1px solid #555;
        }
        
        .computation-detail {
            width: 30%;
            background: #1a1a1a;
            border-radius: 10px;
            padding: 15px;
            border: 1px solid #555;
        }
        
        .memory-block {
            height: 25px;
            margin: 2px 0;
            border-radius: 3px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 0.7em;
            font-weight: bold;
            color: white;
        }
        
        .token-major { background: linear-gradient(90deg, #e57373, #81c784, #64b5f6, #ffd54f, #ba68c8, #ff8a65, #a1887f, #90a4ae); }
        .head-major-0 { background: #e57373; }
        .head-major-1 { background: #81c784; }
        .head-major-2 { background: #64b5f6; }
        .head-major-3 { background: #ffd54f; color: #000; }
        .head-major-4 { background: #ba68c8; }
        .head-major-5 { background: #ff8a65; }
        .head-major-6 { background: #a1887f; }
        .head-major-7 { background: #90a4ae; }
        
        .core-assignment {
            display: flex;
            align-items: center;
            margin: 8px 0;
            padding: 8px;
            border-radius: 5px;
            background: rgba(100, 181, 246, 0.1);
            border-left: 4px solid #64b5f6;
        }
        
        .core-label {
            width: 60px;
            font-weight: bold;
            color: #64b5f6;
            font-size: 0.8em;
        }
        
        .core-work {
            flex: 1;
            font-size: 0.7em;
            color: #ccc;
        }
        
        .avx-instruction {
            background: rgba(255, 111, 0, 0.1);
            border: 1px solid #ff6f00;
            border-radius: 5px;
            padding: 8px;
            margin: 5px 0;
            font-family: monospace;
            font-size: 0.6em;
            color: #ff6f00;
        }
        
        .phase-title {
            position: absolute;
            top: -15px;
            left: 20px;
            background: #000;
            padding: 5px 15px;
            border-radius: 15px;
            font-size: 0.9em;
            font-weight: bold;
        }
        
        .phase-1 { border-color: #e57373; }
        .phase-1 .phase-title { color: #e57373; border: 2px solid #e57373; }
        
        .phase-2 { border-color: #64b5f6; }
        .phase-2 .phase-title { color: #64b5f6; border: 2px solid #64b5f6; }
        
        .phase-3 { border-color: #ffd54f; }
        .phase-3 .phase-title { color: #ffd54f; border: 2px solid #ffd54f; }
        
        .phase-4 { border-color: #81c784; }
        .phase-4 .phase-title { color: #81c784; border: 2px solid #81c784; }
        
        .data-flow-arrow {
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 2em;
            color: #666;
            height: 50px;
        }
        
        /* Performance indicators */
        .perf-metric { 
            background: rgba(100, 181, 246, 0.2); 
            border: 1px solid #64b5f6; 
            border-radius: 10px; 
            padding: 15px; 
            margin: 10px 0; 
            text-align: center; 
        }
        .perf-number { 
            font-size: 2em; 
            color: #64b5f6; 
            font-weight: bold; 
        }
        
        /* Journey progress indicator */
        .journey-progress {
            display: flex;
            justify-content: center;
            align-items: center;
            margin: 20px 0;
            gap: 10px;
        }
        .progress-step {
            padding: 8px 16px;
            border-radius: 20px;
            font-size: 0.8em;
            font-weight: bold;
        }
        .progress-step.active { background: #ff6f00; color: #000; }
        .progress-step.inactive { background: #444; color: #ccc; }
        .progress-arrow { color: #666; font-size: 1.2em; }
    </style>
</head>
<body>
    <div class="reveal">
        <div class="slides">
            <!-- Title Slide -->
            <section>
<aside class="notes">
SLIDE 1: Title Slide - Attention Is All You Need
Welcome everyone! Today we're going on an incredible journey - from the mathematical theory behind attention mechanisms all the way to production-grade C code that achieves over 400 GFLOPS. This isn't just theory - by the end of this presentation, you'll understand exactly how to build world-class attention implementations. We're going to progress through five distinct phases: Theory, Intuition, Architecture, HPC, and finally Production. We'll start with the famous QK^T formula and end with code that achieves performance comparable to the best implementations in the world.
</aside>
                <h2>Attention Is All You Need</h2>
                <h3>The Definitive Implementation Guide</h3>
                <h4>From Mathematical Theory to 400+ GFLOPS Production Code</h4>
                <p>Math → Intuition → Memory Architecture → HPC → Performance</p>
                <div class="journey-progress">
                    <div class="progress-step active">Theory</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step inactive">Intuition</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step inactive">Architecture</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step inactive">HPC</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step inactive">Production</div>
                </div>
                <p><small>The complete journey from $QK^T$ to world-class performance</small></p>
            </section>

            <!-- PART I: MATHEMATICAL FOUNDATIONS -->
            <section>
<aside class="notes">
SLIDE 2: Part I - Mathematical Foundations
Let's begin with Part 1 - the mathematical foundations. We're now in the Theory phase of our journey. It's crucial to understand these mathematical building blocks before we move to implementation. We need to understand not just what the formulas do, but WHY they work. This mathematical understanding will inform all of our optimization decisions later. Without this foundation, we'd be optimizing blindly.
</aside>
                <h2>Part I: Mathematical Foundations</h2>
                <div class="journey-progress">
                    <div class="progress-step active">Theory</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step inactive">Intuition</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step inactive">Architecture</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step inactive">HPC</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step inactive">Production</div>
                </div>
                <p>Understanding the mathematical building blocks that make attention work</p>
            </section>

            <!-- Why Attention Matters -->
            <section>
<aside class="notes">
SLIDE 3: Why Attention Changed Everything
Before attention mechanisms, sequence models were fundamentally limited. Traditional RNNs had to process sequences one step at a time - that's O(T) time complexity with no possibility of parallelization. They suffered from information bottlenecks where all context had to squeeze through hidden states, and gradient vanishing problems meant they couldn't handle long sequences effectively. Self-attention changed everything by introducing parallel processing - O(1) time complexity if you have enough cores. It creates direct connections between all positions in the sequence with constant path length for information flow. The core insight is revolutionary: instead of sequential hidden states, we compute attention weights between ALL pairs of positions simultaneously.
</aside>
                <h3>Why Attention Changed Everything</h3>
                <p>Before attention, sequence models were fundamentally limited by sequential processing. Attention introduced parallelization and global context understanding.</p>
                <div style="display: flex; justify-content: space-around; margin-top: 30px;">
                    <div style="width: 45%;">
                        <h4>Traditional RNNs</h4>
                        <ul style="font-size: 0.8em;">
                            <li>Sequential processing: O(T) time complexity</li>
                            <li>Information bottleneck through hidden states</li>
                            <li>Gradient vanishing over long sequences</li>
                            <li>No parallelization possible</li>
                        </ul>
                    </div>
                    <div style="width: 45%;">
                        <h4>Self-Attention</h4>
                        <ul style="font-size: 0.8em;">
                            <li>Parallel processing: O(1) with sufficient cores</li>
                            <li>Direct connections between all positions</li>
                            <li>Constant path length for information flow</li>
                            <li>Highly parallelizable computation</li>
                        </ul>
                    </div>
                </div>
                <div class="formula-box fragment">
                    <strong>The Core Insight:</strong> Instead of sequential hidden states, compute attention weights between all pairs of positions simultaneously.
                </div>
            </section>

            <!-- The Goal of Attention -->
            <section>
<aside class="notes">
SLIDE 4: The Goal of Attention
So what's the goal of attention? For each token in our sequence, we want to create a new representation that is a weighted average of all other tokens. The key insight is that these weights are not fixed - they're calculated dynamically based on how relevant each token is to the current one we're processing. The formula is Output_i equals the sum over all j of alpha_ij times Value_j, where alpha_ij represents how much token i should attend to token j. The fundamental question becomes: How do we compute these attention weights alpha_ij? That's what we'll explore next.
</aside>
                <h2>The Goal of Attention</h2>
                <p>For each token, we want to create a new representation that is a <span style="color: #ff6f00;">weighted average</span> of all other tokens in the sequence.</p>
                <p class="fragment">The weights are not fixed; they are calculated on the fly based on how <span style="color: #ff6f00;">relevant</span> each token is to the current one we're processing.</p>
                <div class="formula-box fragment">
                    $$ \text{Output}_i = \sum_{j=1}^{T} \alpha_{ij} \cdot \text{Value}_j $$
                    <p style="margin-top: 10px;"><small>Where $\alpha_{ij}$ represents how much token $i$ should attend to token $j$</small></p>
                </div>
                <p class="fragment"><small><strong>Key Question:</strong> How do we compute the attention weights $\alpha_{ij}$?</small></p>
            </section>

            <!-- Step 1: QKV Projections -->
            <section>
<aside class="notes">
SLIDE 5: Step 1 - Projecting Inputs into Q, K, V
The first step is to transform our input into three distinct matrices: Queries, Keys, and Values. We start with our input tensor X and project it using learned weight matrices. Q equals X times W_Q, K equals X times W_K, and V equals X times W_V. Our input X has dimensions T by C - for example, 2048 tokens with 512 channels each. The weight matrices W_Q, W_K, and W_V are all C by C - these are learned parameters that the model updates during training. The output Q, K, and V matrices are all T by C. This is our first major computation step - three massive GEMM operations. These projections transform the input into queries, keys, and values that will be used for the attention computation.
</aside>
                <h2>Step 1: Projecting Inputs into Q, K, V</h2>
                <p>We start with our input tensor `X` and project it into three distinct matrices: Queries, Keys, and Values, using learned weight matrices.</p>
                <div class="formula-box">
                    $$ Q = X W_Q \quad K = X W_K \quad V = X W_V $$
                </div>
                <table class="dim-table">
                    <tr><th>Tensor</th><th>Dimensions</th><th>Description</th></tr>
                    <tr><td>X</td><td>[T, C]</td><td>Input token embeddings (e.g., 2048 tokens, 512 channels)</td></tr>
                    <tr><td>W_Q, W_K, W_V</td><td>[C, C]</td><td>Learned weight matrices</td></tr>
                    <tr><td>Q, K, V</td><td>[T, C]</td><td>Query, Key, and Value matrices</td></tr>
                </table>
                <p class="fragment">This is the first major computation step: <strong>3 massive GEMM operations</strong></p>
            </section>

            <!-- Step 2: Multi-Head Splitting -->
            <section>
<aside class="notes">
SLIDE 6: Step 2 - Splitting into Multiple Heads
To allow the model to focus on different types of relationships simultaneously, we split the Q, K, and V matrices into multiple smaller "heads". We go from one large Q matrix of dimensions T by C to H heads, each of dimension T by D_h. This gives us a new structure with dimensions H by T by D_h. Typical values you'll see in practice: C (total dimension) might be 512, 768, or 1024. H (number of heads) is typically 8, 12, or 16. And D_h (dimension per head) is 64, 96, or 128 - where C equals H times D_h. Each head can learn to focus on different aspects of the relationships between tokens - some might focus on syntactic relationships, others on semantic relationships, and so on.
</aside>
                <h2>Step 2: Splitting into Multiple Heads</h2>
                <p>To allow the model to focus on different types of relationships simultaneously, we split the Q, K, and V matrices into multiple, smaller "heads".</p>
                <div style="display: flex; justify-content: center; align-items: center; gap: 50px; margin-top: 30px;">
                    <div style="text-align: center;">
                        <div style="width: 200px; height: 100px; border: 3px solid #4a90e2; border-radius: 10px; display: flex; align-items: center; justify-content: center;">
                            <div>
                                <div>Q Matrix</div>
                                <div style="font-size: 0.8em;">[T, C]</div>
                            </div>
                        </div>
                    </div>
                    <div style="font-size: 2em; color: #ccc;">→</div>
                    <div style="text-align: center;">
                        <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 10px;">
                            <div style="width: 80px; height: 40px; border: 2px solid #ba68c8; border-radius: 5px; display: flex; align-items: center; justify-content: center; font-size: 0.7em;">Head 1</div>
                            <div style="width: 80px; height: 40px; border: 2px solid #ba68c8; border-radius: 5px; display: flex; align-items: center; justify-content: center; font-size: 0.7em;">Head 2</div>
                            <div style="width: 80px; height: 40px; border: 2px solid #ba68c8; border-radius: 5px; display: flex; align-items: center; justify-content: center; font-size: 0.7em;">Head 3</div>
                            <div style="width: 80px; height: 40px; border: 2px solid #ba68c8; border-radius: 5px; display: flex; align-items: center; justify-content: center; font-size: 0.7em;">Head H</div>
                        </div>
                        <div style="font-size: 0.9em; margin-top: 10px;">New Dimension: [H, T, D_h]<br>Where C = H × D_h</div>
                    </div>
                </div>
                <table class="dim-table fragment">
                    <tr><th>Parameter</th><th>Typical Value</th><th>Description</th></tr>
                    <tr><td>C (Total dimension)</td><td>512, 768, 1024</td><td>Original embedding dimension</td></tr>
                    <tr><td>H (Number of heads)</td><td>8, 12, 16</td><td>Number of attention heads</td></tr>
                    <tr><td>D_h (Head dimension)</td><td>64, 96, 128</td><td>Dimension per head (C / H)</td></tr>
                </table>
            </section>

            <!-- Step 3: Scaled Dot-Product Attention -->
            <section>
<aside class="notes">
SLIDE 7: Step 3 - Scaled Dot-Product Attention
This is the core calculation, and it's performed independently for each head. The formula is Attention of Q, K, V equals softmax of QK^T divided by square root of d_k, all multiplied by V. Let me break this down into its components. First, we compute scores with QK^T - this tells us how much each token should attend to every other token. Then we scale by 1 over square root of d_k to prevent gradients from vanishing. For decoder models, we apply a causal mask to prevent looking at future tokens. We normalize with softmax to convert scores to probabilities. Finally, we compute the weighted sum by multiplying by V to get our final output.
</aside>
                <h2>Step 3: Scaled Dot-Product Attention</h2>
                <p>This is the core calculation, performed independently for each head.</p>
                <div class="formula-box">
                    $$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$
                </div>
                <p class="fragment">Let's break this down into its components:</p>
                <div class="fragment">
                    <ol style="text-align: left; font-size: 0.9em;">
                        <li><strong>Scoring:</strong> $QK^T$ - How much should each token attend to every other token?</li>
                        <li><strong>Scaling:</strong> $\frac{1}{\sqrt{d_k}}$ - Prevent gradients from vanishing</li>
                        <li><strong>Masking:</strong> Apply causal mask (for decoder models)</li>
                        <li><strong>Normalization:</strong> Softmax - Convert scores to probabilities</li>
                        <li><strong>Weighted Sum:</strong> Multiply by $V$ to get final output</li>
                    </ol>
                </div>
            </section>

            <!-- Step 3a: Scoring Visualization -->
            <section id="viz-scores">
<aside class="notes">
SLIDE 8: Step 3a - Calculating Scores Visualization
Let's visualize how we calculate those attention scores. We compute a score matrix by taking the dot product of the Query matrix with the transpose of the Key matrix. Q_head with dimensions T by D_h multiplied by K_head transpose with dimensions D_h by T gives us Scores with dimensions T by T. This resulting T by T matrix tells us how much each token should attend to every other token in the sequence. The formula for each element is Score_ij equals the sum over dimension d of Q_id times K_jd. This is a pure GEMM operation - a matrix multiply - which is perfectly suited for optimization. The visualization shows how these matrices multiply together to produce the attention scores.
</aside>
                <h3>Step 3a: Calculating Scores (Q·K^T)</h3>
                <p>We compute a score matrix by taking the dot product of the Query matrix with the transpose of the Key matrix.</p>
                <div class="viz-container">
                    <div class="grid-container">
                        <div class="heatmap-grid" id="score-q-grid"></div>
                        <div class="legend">Q_head [T,D_h]</div>
                    </div>
                    <div class="op-label">×</div>
                    <div class="grid-container">
                        <div class="heatmap-grid" id="score-k-grid"></div>
                        <div class="legend">K_head^T [D_h,T]</div>
                    </div>
                    <div class="op-label">=</div>
                    <div class="grid-container">
                        <div class="heatmap-grid" id="score-res-grid"></div>
                        <div class="legend">Scores [T,T]</div>
                    </div>
                </div>
                <p class="fragment">The resulting `[T, T]` matrix tells us how much each token should attend to every other token.</p>
                <div class="formula-box fragment">
                    $$ \text{Score}_{i,j} = \sum_{d=1}^{D_h} Q_{i,d} \cdot K_{j,d} $$
                    <p style="margin-top: 10px;"><small>This is a pure GEMM operation, perfectly suited for optimization</small></p>
                </div>
            </section>

            <!-- Step 4: Final Output -->
            <section>
<aside class="notes">
SLIDE 10: Step 4 - Concatenate and Project
The final step is to combine all the head outputs back together. The outputs from all heads are concatenated and then passed through a final linear projection layer. The formula is Output equals Concatenate of head_1 through head_H, multiplied by W_O. This is the fourth and final GEMM operation in the attention mechanism. After this projection, we have our final attention output with the same dimensions as our input. Now we understand the mathematics, but the key question is: how do we implement this efficiently? That's what we'll explore in the next section.
</aside>
                <h2>Step 4: Concatenate and Project</h2>
                <p>The outputs from all heads are concatenated back together and passed through a final linear projection layer.</p>
                <div class="formula-box">
                    $$ \text{Output} = \text{Concat}(\text{head}_1, ..., \text{head}_H) \cdot W_O $$
                    <p style="margin-top: 10px;"><small>The fourth and final GEMM operation</small></p>
                </div>
                <p class="fragment">Now we understand the mathematics. But how do we implement this efficiently?</p>
            </section>

            <!-- PART II: INTUITION AND ARCHITECTURE -->
            <section>
<aside class="notes">
SLIDE 11: Part II - Intuition & Memory Architecture
We're now moving from the Theory phase to the Intuition phase of our journey. In this section, we need to understand WHY the mathematics works the way it does, and HOW to architect memory for maximum performance. This is where we bridge the gap between mathematical understanding and practical implementation. We're starting to think like performance engineers, not just mathematicians. The decisions we make here about memory layout will determine whether our implementation runs at 10 GFLOPS or 400 GFLOPS.
</aside>
                <h2>Part II: Intuition & Memory Architecture</h2>
                <div class="journey-progress">
                    <div class="progress-step inactive">Theory</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step active">Intuition</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step inactive">Architecture</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step inactive">HPC</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step inactive">Production</div>
                </div>
                <p>Understanding <em>why</em> the mathematics works and <em>how</em> to architect memory for performance</p>
            </section>

            <!-- The Transformer Architecture Context -->
            <section>
<aside class="notes">
SLIDE 12: Our Focus - The Heart of the Transformer
Let's put attention in context within the full transformer architecture. The transformer has many components - embeddings, layer normalization, attention, residual connections, and MLPs. But here's the key insight: Multi-Head Attention is where 90% of the computation happens in a transformer. This makes it the computational bottleneck and the most important component to optimize. If we can make attention fast, we've made the entire transformer fast. Understanding its implementation is absolutely crucial for performance. This is where we need to focus all our optimization efforts.
</aside>
                <h3>Our Focus: The Heart of the Transformer</h3>
                <p>The Multi-Head Attention mechanism is where 90% of computation happens in a transformer. Understanding its implementation is crucial for performance.</p>
                <div class="transformer-block">
                    <div class="block-component">Input Embeddings + Positional Encoding</div>
                    <div class="arrow-down"></div>
                    <div class="block-component">Layer Norm</div>
                    <div class="arrow-down"></div>
                    <div class="block-component highlight">
                        <strong>Causal Multi-Head Attention</strong>
                        <small>This is where 90% of the compute lies.</small>
                    </div>
                    <div class="arrow-down"></div>
                    <div class="block-component">Add & Norm (Residual Connection)</div>
                    <div class="arrow-down"></div>
                    <div class="block-component">Feed-Forward Network (MLP)</div>
                    <div class="arrow-down"></div>
                    <div class="block-component">Add & Norm (Residual Connection)</div>
                    <div class="arrow-down"></div>
                    <div class="block-component">...</div>
                </div>
            </section>

            <!-- Our Implementation Strategy -->
            <section>
<aside class="notes">
SLIDE 13: Our Implementation Strategy - Head-Parallel with Smart Memory Layout
Our implementation uses head-parallelism as the primary strategy, with dynamic memory layout transitions to optimize each computation phase. We have four distinct phases. Phase 1 is QKV Projection - we go from token-parallel input to head-major output. Phase 2 is Attention Scores - head-parallel input to head-parallel scores. Phase 3 is Concatenation - head-parallel input to token-major output. Phase 4 is Final Projection - token-parallel input to token-major output. The key insight here is that head-parallelism provides the best cache locality for the compute-intensive attention phase, while we adapt memory layouts to optimize each stage. This strategy is what enables us to achieve world-class performance.
</aside>
                <h3>Our Implementation Strategy: Head-Parallel with Smart Memory Layout</h3>
                <p>We use head-parallelism as our primary strategy, with dynamic memory layout transitions to optimize each computation phase</p>
                <div style="display: grid; grid-template-columns: repeat(4, 1fr); gap: 20px; margin-top: 30px;">
                    <div style="text-align: center; padding: 20px; border: 2px solid #e57373; border-radius: 10px; background: rgba(229, 115, 115, 0.1);">
                        <div style="font-size: 1.5em; margin-bottom: 10px;">📥</div>
                        <h4>Phase 1: QKV Projection</h4>
                        <p style="font-size: 0.8em;">Token-parallel input<br>→ Head-major output</p>
                    </div>
                    <div style="text-align: center; padding: 20px; border: 2px solid #64b5f6; border-radius: 10px; background: rgba(100, 181, 246, 0.1);">
                        <div style="font-size: 1.5em; margin-bottom: 10px;">🧮</div>
                        <h4>Phase 2: Attention Scores</h4>
                        <p style="font-size: 0.8em;">Head-parallel input<br>→ Head-parallel scores</p>
                    </div>
                    <div style="text-align: center; padding: 20px; border: 2px solid #ffd54f; border-radius: 10px; background: rgba(255, 213, 79, 0.1);">
                        <div style="font-size: 1.5em; margin-bottom: 10px;">🔗</div>
                        <h4>Phase 3: Concatenation</h4>
                        <p style="font-size: 0.8em;">Head-parallel input<br>→ Token-major output</p>
                    </div>
                    <div style="text-align: center; padding: 20px; border: 2px solid #81c784; border-radius: 10px; background: rgba(129, 199, 132, 0.1);">
                        <div style="font-size: 1.5em; margin-bottom: 10px;">📤</div>
                        <h4>Phase 4: Final Projection</h4>
                        <p style="font-size: 0.8em;">Token-parallel input<br>→ Token-major output</p>
                    </div>
                </div>
                <p class="fragment" style="margin-top: 30px;"><strong>Key Insight:</strong> Head-parallelism provides the best cache locality for the compute-intensive attention phase, while we adapt memory layouts to optimize each stage.</p>
            </section>

            <!-- Memory Layout: The Foundation -->
            <section id="viz-logical-physical">
<aside class="notes">
SLIDE 14: Memory Layout - The Foundation of Performance
Now let's talk about memory layout, which is the foundation of performance. We need to map the logical concept of "heads" to a physical memory layout that the CPU can process at maximum speed. Logically, we think of 8 separate heads that operate independently. But the physical reality is different - we use a Head-Major Layout where each head's data is perfectly contiguous in memory. The layout is Head, then Token, then Dimension. All data for Head 0 is contiguous, then all data for Head 1, and so on. This enables perfect cache locality during head-parallel processing. When we're processing Head 0, all of its data fits in cache without interfering with other heads.
</aside>
                <h3>Memory Layout: The Foundation of Performance</h3>
                <p>We map the logical concept of "heads" to a physical memory layout that the CPU can process at maximum speed.</p>
                <div class="viz-container">
                    <div class="memory-layout">
                        <h4>Logical View: 8 Separate Heads</h4>
                        <div class="heads-container" id="logical-heads"></div>
                    </div>
                    <div class="memory-layout">
                        <h4>Physical Reality: Head-Major Layout</h4>
                        <p><small>Each colored block represents a head's data being perfectly contiguous in memory</small></p>
                        <div class="memory-bar" id="physical-bar"></div>
                    </div>
                </div>
                 <p><small>Hover over a logical head to see its physical location in memory.</small></p>
                 <div class="formula-box fragment">
                     <strong>Head-Major Layout:</strong> [Head][Token][Dimension]<br>
                     <small>All data for Head 0 is contiguous, then all data for Head 1, etc.</small>
                 </div>
            </section>

            <!-- Head-Major Reorganization -->
            <section id="viz-memory-reorg">
<aside class="notes">
SLIDE 15: The Head-Major Reorganization
This reorganization is not a simple transpose - it's a deliberate, out-of-place reorganization of data for performance. We transform from input Q Tensor with dimensions T by C in Token-Major layout to output Q Buffer with dimensions H by T by D_h in Head-Major layout. For each head, represented by different colors in the visualization, we gather its feature columns from all tokens and write them into a new, contiguous memory block. The performance impact is huge: instead of scattered memory access where we're jumping around in memory, each head's data is contiguous. When processing Head 0, the entire head fits in L3 cache. This is the key to our performance.
</aside>
                <h3>The Head-Major Reorganization</h3>
                <p>This is not a simple transpose. It's a deliberate, out-of-place reorganization of data for performance.</p>
                <div class="viz-container">
                    <div class="grid-container">
                        <div class="heatmap-grid" id="reorg-q-grid"></div>
                        <div class="legend">Input Q Tensor [T, C] (Token-Major)</div>
                    </div>
                    <div class="op-label">→</div>
                    <div class="grid-container">
                        <div class="heatmap-grid" id="reorg-heads-grid"></div>
                        <div class="legend">Output Q Buffer [H, T, D_h] (Head-Major)</div>
                    </div>
                </div>
                <p class="fragment"><small>For each head (color), we gather its feature columns from all tokens and write them into a new, contiguous memory block.</small></p>
                <div class="formula-box fragment">
                    <strong>Performance Impact:</strong> Instead of scattered memory access, each head's data is contiguous.
                    When processing Head 0, the entire head fits in L3 cache.
                </div>
            </section>

            <!-- PART III: MEMORY ARCHITECTURE -->
            <section>
<aside class="notes">
SLIDE 16: Part III - Memory Architecture
We're now entering the Architecture phase of our journey. Here we'll see the actual C code structures and memory architecture that enable 400+ GFLOPS performance. This is where theory meets reality in terms of data structures and memory management. We're going to see exactly how we organize memory at the C struct level to achieve maximum performance.
</aside>
                <h2>Part III: Memory Architecture</h2>
                <div class="journey-progress">
                    <div class="progress-step inactive">Theory</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step inactive">Intuition</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step active">Architecture</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step inactive">HPC</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step inactive">Production</div>
                </div>
                <p>The actual C code structures and memory architecture that enable 400+ GFLOPS</p>
            </section>

            <!-- The C-Level Memory Architecture -->
            <section>
<aside class="notes">
SLIDE 17: The C-Level Memory Architecture
Our memory architecture uses a single contiguous memory block with precise struct-level control. The key design principles are: Single Allocation - we do one huge malloc for the entire model, avoiding fragmentation. Cache Alignment - all major tensors are aligned to 64-byte boundaries. Head-Major Layout - contiguous memory per attention head. Canary Protection - we can detect buffer overflows. Zero Fragmentation - completely predictable memory access patterns. The result is over 95% L3 Cache Hit Rate. The TrulyOptimalLayer struct shows how we track precise memory offsets for every tensor. The TransformerModel struct manages the entire model with aligned dimensions and execution planning. This level of control over memory is what separates a toy implementation from a production one.
</aside>
                <h3>The C-Level Memory Architecture</h3>
                <p>Single contiguous memory block with precise struct-level control</p>
                <div style="display: flex; justify-content: space-around; align-items: flex-start;">
                    <div style="width: 55%;">
                        <pre><code class="c code-block-tiny" data-trim>
typedef struct {
    // Per-layer memory offsets
    size_t layer_start_canary_offset;
    
    size_t ln1_weight_offset, ln1_bias_offset;
    size_t ln1_mean_offset, ln1_rstd_offset;
    size_t layer_input_offset, ln1_output_offset;
    
    // Separate Q, K, V for cleaner access
    size_t q_weight_offset, q_bias_offset, q_output_offset;
    size_t k_weight_offset, k_bias_offset, k_output_offset;
    size_t v_weight_offset, v_bias_offset, v_output_offset;
    
    size_t attention_scores_offset;
    size_t proj_weight_offset, proj_bias_offset;
    size_t attention_output_offset, residual1_output_offset;
    
    // MLP components...
    size_t fc1_weight_offset, fc1_bias_offset, fc1_output_offset;
    size_t fc2_weight_offset, fc2_bias_offset;
    size_t mlp_output_offset, residual2_output_offset;
    
    size_t layer_end_canary_offset;
} TrulyOptimalLayer;

typedef struct {
    /* hyper-parameters */
    int num_layers, vocab_size, embed_dim, context_window;
    size_t aligned_embed_dim, aligned_head_dim;
    size_t aligned_attn_context_window;
    
    /* execution plan */
    int num_cores, tokens_per_core;
    int num_attention_heads, head_dim;
    
    /* single memory block */
    float *memory_base;
    size_t total_floats, layer_stride;
    
    /* per-layer table */
    TrulyOptimalLayer *layers;
} TransformerModel;
                        </code></pre>
                    </div>
                    <div style="width: 40%;">
                        <h4>Key Design Principles</h4>
                        <ul style="font-size: 0.8em; text-align: left;">
                            <li><strong>Single Allocation:</strong> One huge malloc() for entire model</li>
                            <li><strong>Cache Alignment:</strong> 64-byte boundaries for all major tensors</li>
                            <li><strong>Head-Major Layout:</strong> Contiguous memory per attention head</li>
                            <li><strong>Canary Protection:</strong> Buffer overflow detection</li>
                            <li><strong>Zero Fragmentation:</strong> Predictable memory access patterns</li>
                        </ul>
                        <div class="perf-metric" style="margin-top: 20px;">
                            <div style="font-size: 1.2em; color: #81c784; font-weight: bold;">95%+</div>
                            <div style="font-size: 0.8em;">L3 Cache Hit Rate</div>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Head-Major Memory Access Macros -->
            <section>
<aside class="notes">
SLIDE 18: Head-Major Memory Access - The Performance Key
Here we see the carefully designed macros that enable head-parallel computation with perfect cache locality. Our Head-Major Memory Layout follows the pattern: head, then token, then head_dim. In memory, it's organized as Head0 with all its tokens, then Head1 with all its tokens, and so on. The access macros - Q_ACCESS, K_ACCESS, V_ACCESS - handle tensor access, while ATTN_ACCESS handles attention scores with the pattern head, query_token, key_token. Why is Head-Major so important? Because each head's data is contiguous in memory. When processing Head 0, all its data fits in L3 cache. There are no cache conflicts between heads during parallel processing. This is absolutely critical for performance.
</aside>
                <h3>Head-Major Memory Access: The Performance Key</h3>
                <p>Carefully designed macros enable head-parallel computation with perfect cache locality</p>
                <pre><code class="c code-block-small" data-trim>
/* ============================================================================
   HEAD-MAJOR MEMORY LAYOUT
   Layout: [head][token][head_dim] 
   Memory: [Head0: Token0[head_dim], Token1[head_dim], ..., TokenN[head_dim]]
           [Head1: Token0[head_dim], Token1[head_dim], ..., TokenN[head_dim]]
           [...]
   ============================================================================ */

// Attention tensor access: q_ptr[head * context_window * aligned_head_dim + token * aligned_head_dim + dim]
#define Q_ACCESS(q_ptr, h, t, d, context_window, aligned_head_dim) \
    q_ptr[((h) * (context_window) + (t)) * (aligned_head_dim) + (d)]

#define K_ACCESS(k_ptr, h, t, d, context_window, aligned_head_dim) \
    k_ptr[((h) * (context_window) + (t)) * (aligned_head_dim) + (d)]

#define V_ACCESS(v_ptr, h, t, d, context_window, aligned_head_dim) \
    v_ptr[((h) * (context_window) + (t)) * (aligned_head_dim) + (d)]

// Attention scores: [head][query_token][key_token]
#define ATTN_ACCESS(attn_ptr, head_idx, query_token, key_token, context_window) \
    attn_ptr[((head_idx) * (context_window) + (query_token)) * (context_window) + (key_token)]
                </code></pre>
                <p class="fragment"><small><strong>Why Head-Major?</strong> Each head's data is contiguous in memory. When processing Head 0, all data fits in L3 cache. No cache conflicts between heads during parallel processing.</small></p>
            </section>

            <!-- PART IV: MEMORY FLOW DEEP DIVE -->
            <section>
<aside class="notes">
SLIDE 19: Part IV - Memory Flow Deep Dive
We're now entering the HPC - High Performance Computing - phase of our journey. In this section, we'll see exactly how data flows through memory and cores in each computation phase. This is the detailed execution model that achieves high performance. We'll track data movement, core assignments, and cache behavior through each phase of the attention mechanism.
</aside>
                <h2>Part IV: Memory Flow Deep Dive</h2>
                <div class="journey-progress">
                    <div class="progress-step inactive">Theory</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step inactive">Intuition</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step inactive">Architecture</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step active">HPC</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step inactive">Production</div>
                </div>
                <p>How data flows through memory and cores in each computation phase</p>
            </section>

            <!-- Phase 1: QKV Projection Memory Flow -->
            <section>
<aside class="notes">
SLIDE 20: Phase 1 - QKV Projection Memory Flow
Phase 1 transforms from Token-Major to Head-Major layout. The input is in Token-Major layout where each token contains data for all heads - like T0 containing h0 through h7. The output is Head-Major where each head's data is contiguous - Head 0 contains all tokens T0 through TN. For core assignment, each core processes a slice of tokens. Core 0 handles tokens 0-255, Core 1 handles 256-511, and so on. Each core processes its token slice for all heads. The AVX-512 computation shows direct write to head-major layout. The key optimization here is that we write directly to head-major layout, avoiding an expensive transpose operation later. This phase prepares the data for efficient head-parallel processing in Phase 2.
</aside>
                <h3>Phase 1: QKV Projection - Token-Parallel to Head-Major</h3>
                <div class="memory-flow-container">
                    <div class="phase-container phase-1" style="position: relative;">
                        <div class="phase-title">Phase 1: QKV Projection</div>
                        
                        <div class="memory-layout-viz">
                            <h4>Memory Layout</h4>
                            <div style="margin-bottom: 15px;">
                                <strong>Input (Token-Major):</strong>
                                <div class="memory-block token-major">T0: [h0,h1,h2,h3,h4,h5,h6,h7]</div>
                                <div class="memory-block token-major">T1: [h0,h1,h2,h3,h4,h5,h6,h7]</div>
                                <div class="memory-block token-major">T2: [h0,h1,h2,h3,h4,h5,h6,h7]</div>
                            </div>
                            <div class="data-flow-arrow">↓</div>
                            <div>
                                <strong>Output (Head-Major):</strong>
                                <div class="memory-block head-major-0">Head 0: [T0,T1,T2,...,TN]</div>
                                <div class="memory-block head-major-1">Head 1: [T0,T1,T2,...,TN]</div>
                                <div class="memory-block head-major-2">Head 2: [T0,T1,T2,...,TN]</div>
                            </div>
                        </div>
                        
                        <div class="core-distribution">
                            <h4>Core Assignment</h4>
                            <div class="core-assignment">
                                <div class="core-label">Core 0:</div>
                                <div class="core-work">Tokens 0-255 → All heads</div>
                            </div>
                            <div class="core-assignment">
                                <div class="core-label">Core 1:</div>
                                <div class="core-work">Tokens 256-511 → All heads</div>
                            </div>
                            <div class="core-assignment">
                                <div class="core-label">Core 2:</div>
                                <div class="core-work">Tokens 512-767 → All heads</div>
                            </div>
                            <div class="core-assignment">
                                <div class="core-label">Core 3:</div>
                                <div class="core-work">Tokens 768-1023 → All heads</div>
                            </div>
                        </div>
                        
                        <div class="computation-detail">
                            <h4>AVX-512 Computation</h4>
                            <div class="avx-instruction">
                                // Each core processes token slice<br>
                                for (int h = 0; h < num_heads; h++) {<br>
                                &nbsp;&nbsp;avx512_gemv_projection(...);<br>
                                &nbsp;&nbsp;// Write directly to head-major<br>
                                &nbsp;&nbsp;Q_ACCESS(q_base, h, token, d);<br>
                                }
                            </div>
                            <p style="font-size: 0.8em; color: #ccc;">
                                <strong>Key:</strong> Direct write to head-major layout avoids expensive transpose later
                            </p>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Phase 2: Attention Scores - Head-Parallel -->
            <section>
<aside class="notes">
SLIDE 21: Phase 2 - Attention Scores Head-Parallel Perfection
Phase 2 is where most of the computation happens, and it's perfectly organized for head-parallel execution. Both input and output are in Head-Major layout. For core assignment, each core handles one complete head - Core 0 processes Head 0, Core 1 processes Head 1, and so on. Each core computes the complete Q times K transpose for its assigned head. We achieve perfect cache locality because each core's data fits entirely in cache. The AVX-512 computation uses nested loops over tokens with FMA instructions for maximum throughput. This phase achieves 96% L3 Cache Hit Rate. Each head's data fits perfectly in cache with no interference between cores. This is why the head-major layout is so crucial - it enables this perfect parallelization.
</aside>
                <h3>Phase 2: Attention Scores - Head-Parallel Perfection</h3>
                <div class="memory-flow-container">
                    <div class="phase-container phase-2" style="position: relative;">
                        <div class="phase-title">Phase 2: Attention Computation</div>
                        
                        <div class="memory-layout-viz">
                            <h4>Memory Layout</h4>
                            <div style="margin-bottom: 15px;">
                                <strong>Input (Head-Major):</strong>
                                <div class="memory-block head-major-0">Head 0: [T0,T1,T2,...,TN]</div>
                                <div class="memory-block head-major-1">Head 1: [T0,T1,T2,...,TN]</div>
                                <div class="memory-block head-major-2">Head 2: [T0,T1,T2,...,TN]</div>
                            </div>
                            <div class="data-flow-arrow">↓</div>
                            <div>
                                <strong>Output (Head-Major Scores):</strong>
                                <div class="memory-block head-major-0">H0 Scores: [TxT matrix]</div>
                                <div class="memory-block head-major-1">H1 Scores: [TxT matrix]</div>
                                <div class="memory-block head-major-2">H2 Scores: [TxT matrix]</div>
                            </div>
                        </div>
                        
                        <div class="core-distribution">
                            <h4>Core Assignment</h4>
                            <div class="core-assignment">
                                <div class="core-label">Core 0:</div>
                                <div class="core-work">Head 0 → Complete Q·K^T</div>
                            </div>
                            <div class="core-assignment">
                                <div class="core-label">Core 1:</div>
                                <div class="core-work">Head 1 → Complete Q·K^T</div>
                            </div>
                            <div class="core-assignment">
                                <div class="core-label">Core 2:</div>
                                <div class="core-work">Head 2 → Complete Q·K^T</div>
                            </div>
                            <div class="core-assignment">
                                <div class="core-label">Core 3:</div>
                                <div class="core-work">Head 3 → Complete Q·K^T</div>
                            </div>
                        </div>
                        
                        <div class="computation-detail">
                            <h4>AVX-512 Computation</h4>
                            <div class="avx-instruction">
                                // Perfect cache locality<br>
                                for (int i = 0; i < tokens; i++) {<br>
                                &nbsp;&nbsp;for (int j = 0; j <= i; j++) {<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;__m512 acc = _mm512_setzero_ps();<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;acc = _mm512_fmadd_ps(q, k, acc);<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;score = _mm512_reduce_add_ps(acc);<br>
                                &nbsp;&nbsp;}<br>
                                }
                            </div>
                            <div class="perf-metric" style="margin-top: 10px;">
                                <div style="font-size: 1em; color: #64b5f6;">96%</div>
                                <div style="font-size: 0.7em;">L3 Cache Hit Rate</div>
                            </div>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Phase 3: Concatenation -->
            <section>
<aside class="notes">
SLIDE 22: Phase 3 - Concatenation from Head-Major to Token-Major
Phase 3 transforms the data back from Head-Major to Token-Major layout for the final projection. The input is Head-Major with each head's data contiguous. The output is Token-Major where each token contains data from all heads. Core assignment returns to token-parallel - each core handles a slice of tokens, gathering data from all heads. Core 0 handles tokens 0-255, gathering from all heads, and so on. This is a memory reorganization phase with conservative threading due to memory bandwidth limits. Unlike the compute-intensive Phase 2, this phase is primarily memory bandwidth limited. Each core gathers data for its assigned tokens from all heads, which requires careful memory access patterns to avoid bottlenecks.
</aside>
                <h3>Phase 3: Concatenation - Head-Major to Token-Major</h3>
                <div class="memory-flow-container">
                    <div class="phase-container phase-3" style="position: relative;">
                        <div class="phase-title">Phase 3: Concatenation</div>
                        
                        <div class="memory-layout-viz">
                            <h4>Memory Layout</h4>
                            <div style="margin-bottom: 15px;">
                                <strong>Input (Head-Major):</strong>
                                <div class="memory-block head-major-0">Head 0: [T0,T1,T2,...,TN]</div>
                                <div class="memory-block head-major-1">Head 1: [T0,T1,T2,...,TN]</div>
                                <div class="memory-block head-major-2">Head 2: [T0,T1,T2,...,TN]</div>
                            </div>
                            <div class="data-flow-arrow">↓</div>
                            <div>
                                <strong>Output (Token-Major):</strong>
                                <div class="memory-block token-major">T0: [h0,h1,h2,h3,h4,h5,h6,h7]</div>
                                <div class="memory-block token-major">T1: [h0,h1,h2,h3,h4,h5,h6,h7]</div>
                                <div class="memory-block token-major">T2: [h0,h1,h2,h3,h4,h5,h6,h7]</div>
                            </div>
                        </div>
                        
                        <div class="core-distribution">
                            <h4>Core Assignment</h4>
                            <div class="core-assignment">
                                <div class="core-label">Core 0:</div>
                                <div class="core-work">Tokens 0-255 ← From all heads</div>
                            </div>
                            <div class="core-assignment">
                                <div class="core-label">Core 1:</div>
                                <div class="core-work">Tokens 256-511 ← From all heads</div>
                            </div>
                            <div class="core-assignment">
                                <div class="core-label">Core 2:</div>
                                <div class="core-work">Tokens 512-767 ← From all heads</div>
                            </div>
                            <div class="core-assignment">
                                <div class="core-label">Core 3:</div>
                                <div class="core-work">Tokens 768-1023 ← From all heads</div>
                            </div>
                        </div>
                        
                        <div class="computation-detail">
                            <h4>Memory Reorganization</h4>
                            <div class="avx-instruction">
                                // Conservative threading<br>
                                for (int t = token_start; t < token_end; t++) {<br>
                                &nbsp;&nbsp;for (int h = 0; h < num_heads; h++) {<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;// Gather from head-major<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;val = Q_ACCESS(head_data, h, t, d);<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;// Write to token-major<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;token_out[h*head_dim + d] = val;<br>
                                &nbsp;&nbsp;}<br>
                                }
                            </div>
                            <p style="font-size: 0.8em; color: #ccc;">
                                Memory bandwidth limited phase
                            </p>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Phase 4: Final Projection -->
            <section>
<aside class="notes">
SLIDE 23: Phase 4 - Final Projection Token-Major Efficiency
Phase 4 is the final projection, a standard token-parallel GEMM operation. Both input and output are in Token-Major layout. Each core processes a slice of tokens - Core 0 handles tokens 0-255, Core 1 handles 256-511, and so on. This is a standard token-parallel GEMM operation with AVX-512 GEMV instructions. We get excellent sequential access patterns because the data is already in token-major layout from Phase 3. This phase benefits from the memory layout established in Phase 3. It's a straightforward parallel GEMM operation with good cache behavior. The sequential access patterns mean we can achieve high memory bandwidth utilization.
</aside>
                <h3>Phase 4: Final Projection - Token-Major Efficiency</h3>
                <div class="memory-flow-container">
                    <div class="phase-container phase-4" style="position: relative;">
                        <div class="phase-title">Phase 4: Final Projection</div>
                        
                        <div class="memory-layout-viz">
                            <h4>Memory Layout</h4>
                            <div style="margin-bottom: 15px;">
                                <strong>Input (Token-Major):</strong>
                                <div class="memory-block token-major">T0: [h0,h1,h2,h3,h4,h5,h6,h7]</div>
                                <div class="memory-block token-major">T1: [h0,h1,h2,h3,h4,h5,h6,h7]</div>
                                <div class="memory-block token-major">T2: [h0,h1,h2,h3,h4,h5,h6,h7]</div>
                            </div>
                            <div class="data-flow-arrow">↓</div>
                            <div>
                                <strong>Output (Token-Major):</strong>
                                <div class="memory-block token-major">T0: [out_features]</div>
                                <div class="memory-block token-major">T1: [out_features]</div>
                                <div class="memory-block token-major">T2: [out_features]</div>
                            </div>
                        </div>
                        
                        <div class="core-distribution">
                            <h4>Core Assignment</h4>
                            <div class="core-assignment">
                                <div class="core-label">Core 0:</div>
                                <div class="core-work">Tokens 0-255 → GEMV</div>
                            </div>
                            <div class="core-assignment">
                                <div class="core-label">Core 1:</div>
                                <div class="core-work">Tokens 256-511 → GEMV</div>
                            </div>
                            <div class="core-assignment">
                                <div class="core-label">Core 2:</div>
                                <div class="core-work">Tokens 512-767 → GEMV</div>
                            </div>
                            <div class="core-assignment">
                                <div class="core-label">Core 3:</div>
                                <div class="core-work">Tokens 768-1023 → GEMV</div>
                            </div>
                        </div>
                        
                        <div class="computation-detail">
                            <h4>AVX-512 GEMV</h4>
                            <div class="avx-instruction">
                                // Standard token-parallel GEMM<br>
                                for (int t = token_start; t < token_end; t++) {<br>
                                &nbsp;&nbsp;const float *input = concat_buf + t*embed_dim;<br>
                                &nbsp;&nbsp;float *output = result + t*embed_dim;<br>
                                &nbsp;&nbsp;<br>
                                &nbsp;&nbsp;avx512_gemv_with_bias(input, weights,<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bias, output,<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;embed_dim);<br>
                                }
                            </div>
                            <p style="font-size: 0.8em; color: #ccc;">
                                Excellent sequential access patterns
                            </p>
                        </div>
                    </div>
                </div>
            </section>

            <!-- PART V: PRODUCTION CODE -->
            <section>
<aside class="notes">
SLIDE 24: Part V - Production Code
We've finally reached the Production phase of our journey. This is where everything comes together - the real C code that achieves 400+ GFLOPS performance. We'll see working, optimized production code that integrates all the concepts we've discussed. This isn't pseudocode or simplified examples - this is the actual implementation that achieves world-class performance.
</aside>
                <h2>Part V: Production Code</h2>
                <div class="journey-progress">
                    <div class="progress-step inactive">Theory</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step inactive">Intuition</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step inactive">Architecture</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step inactive">HPC</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step active">Production</div>
                </div>
                <p>Real C code that achieves 400+ GFLOPS performance</p>
            </section>

            <!-- Complete Pipeline Integration -->
            <section>
<aside class="notes">
SLIDE 25: The Complete Attention Pipeline Integration
Here's how all phases integrate in the transformer_layer_optimized function. The complete flow has 8 steps. First, Pre-attention LayerNorm with token-parallel processing. Second, QKV Projection going from token-parallel to head-major output. Third, Attention Computation with head-parallel processing. Fourth, Attention Output Projection from head-major to token-major. Fifth, First Residual Connection with token-parallel addition. Sixth, Pre-MLP LayerNorm. Seventh, MLP Feed-Forward operations. Eighth, Second Residual Connection. Each phase is optimized for its specific memory layout and parallelization strategy. The function shown is the actual production code that orchestrates all these operations efficiently.
</aside>
                <h3>The Complete Attention Pipeline Integration</h3>
                <p>How all phases integrate in the transformer layer</p>
                <pre><code class="c code-block-small" data-trim>
void transformer_layer_optimized(TransformerModel *M, int layer_idx, size_t layer_input_offset) {
    TrulyOptimalLayer *L = &M->layers[layer_idx];
    const float eps = 1e-5f;

    // 1. Pre-attention LayerNorm (Token-Parallel)
    layernorm_token_parallel(M, layer_input_offset, L->ln1_weight_offset,
                             L->ln1_bias_offset, L->ln1_mean_offset, 
                             L->ln1_rstd_offset, L->ln1_output_offset, eps);

    // 2. QKV Projection (Token-Parallel → Head-Major Output)
    qkv_projection_head_major(M, layer_idx);

    // 3. Attention Computation (Head-Parallel)
    attention_head_major_complete(M, layer_idx);

    // 4. Attention Output Projection (Head-Major → Token-Major)
    attention_projection_with_concat(M, layer_idx);
    
    // 5. First Residual Connection (Token-Parallel)
    residual_add_token_parallel(M, layer_input_offset, L->attention_output_offset,
                                L->residual1_output_offset);

    // 6. Pre-MLP LayerNorm (Token-Parallel)
    layernorm_token_parallel(M, L->residual1_output_offset, L->ln2_weight_offset,
                             L->ln2_bias_offset, L->ln2_mean_offset, 
                             L->ln2_rstd_offset, L->ln2_output_offset, eps);

    // 7. MLP Feed-Forward (Token-Parallel)
    mlp_token_parallel(M, L->ln2_output_offset, L->fc1_weight_offset, L->fc1_bias_offset,
                       L->fc1_output_offset, L->fc2_weight_offset, L->fc2_bias_offset,
                       L->mlp_output_offset);

    // 8. Second Residual Connection (Token-Parallel)
    residual_add_token_parallel(M, L->residual1_output_offset, L->mlp_output_offset,
                                L->residual2_output_offset);
}
                </code></pre>
            </section>

            <!-- Performance Analysis: Real Numbers -->
            <section>
<aside class="notes">
SLIDE 26: Performance Analysis - Real Numbers
Let's look at real, measured performance from our implementation. Phase timing breakdown: Phase 1 QKV takes about 0.3 milliseconds. Phase 2 Attention takes about 1.2 milliseconds - this is the bottleneck. Phase 3 Concatenation takes about 0.2 milliseconds. Phase 4 Final Projection takes about 0.3 milliseconds. Total time is approximately 2.0 milliseconds. For memory hierarchy utilization: L1 Cache achieves 85% hit rate. L2 Cache achieves 92% hit rate. L3 Cache achieves 96% hit rate. The key result is that head-major layout in Phase 2 achieves 96% L3 cache hit rate. This is what enables our high performance. Memory bandwidth utilization shows excellent efficiency across all cache levels.
</aside>
                <h3>Performance Analysis: Real Numbers</h3>
                <p>Measured performance from the actual implementation</p>
                <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 30px; margin-top: 30px;">
                    <div>
                        <h4>Phase Timing Breakdown</h4>
                        <div style="background: #1a1a1a; padding: 15px; border-radius: 10px;">
                            <div style="display: flex; justify-content: space-between; margin: 8px 0; padding: 8px; background: rgba(229, 115, 115, 0.2); border-radius: 5px;">
                                <span>Phase 1 (QKV)</span><span>~0.3ms</span>
                            </div>
                            <div style="display: flex; justify-content: space-between; margin: 8px 0; padding: 8px; background: rgba(100, 181, 246, 0.2); border-radius: 5px;">
                                <span>Phase 2 (Attention)</span><span>~1.2ms</span>
                            </div>
                            <div style="display: flex; justify-content: space-between; margin: 8px 0; padding: 8px; background: rgba(255, 213, 79, 0.2); border-radius: 5px;">
                                <span>Phase 3 (Concat)</span><span>~0.2ms</span>
                            </div>
                            <div style="display: flex; justify-content: space-between; margin: 8px 0; padding: 8px; background: rgba(129, 199, 132, 0.2); border-radius: 5px;">
                                <span>Phase 4 (Projection)</span><span>~0.3ms</span>
                            </div>
                            <div style="display: flex; justify-content: space-between; margin: 8px 0; padding: 8px; background: rgba(186, 104, 200, 0.2); border-radius: 5px; font-weight: bold;">
                                <span>Total</span><span>~2.0ms</span>
                            </div>
                        </div>
                    </div>
                    <div>
                        <h4>Memory Hierarchy Utilization</h4>
                        <div style="background: #1a1a1a; padding: 15px; border-radius: 10px;">
                            <div style="margin: 10px 0;">
                                <div style="font-weight: bold; margin-bottom: 5px;">L1 Cache (32KB per core)</div>
                                <div style="width: 100%; height: 20px; background: #333; border-radius: 10px; overflow: hidden;">
                                    <div style="width: 85%; height: 100%; background: #81c784;"></div>
                                </div>
                                <div style="font-size: 0.8em; color: #ccc;">85% hit rate</div>
                            </div>
                            <div style="margin: 10px 0;">
                                <div style="font-weight: bold; margin-bottom: 5px;">L2 Cache (256KB per core)</div>
                                <div style="width: 100%; height: 20px; background: #333; border-radius: 10px; overflow: hidden;">
                                    <div style="width: 92%; height: 100%; background: #64b5f6;"></div>
                                </div>
                                <div style="font-size: 0.8em; color: #ccc;">92% hit rate</div>
                            </div>
                            <div style="margin: 10px 0;">
                                <div style="font-weight: bold; margin-bottom: 5px;">L3 Cache (32MB shared)</div>
                                <div style="width: 100%; height: 20px; background: #333; border-radius: 10px; overflow: hidden;">
                                    <div style="width: 96%; height: 100%; background: #ff6f00;"></div>
                                </div>
                                <div style="font-size: 0.8em; color: #ccc;">96% hit rate</div>
                            </div>
                        </div>
                    </div>
                </div>
                <div class="formula-box" style="margin-top: 30px;">
                    <strong>Key Result:</strong> Head-major layout in Phase 2 achieves 96% L3 cache hit rate, 
                    making it the most efficient phase despite being the most compute-intensive.
                </div>
            </section>

            <!-- Future Optimizations -->
            <section>
<aside class="notes">
SLIDE 27: Future Optimizations - Direct Strided Projection
Looking forward, we can optimize further by eliminating Phase 3. Currently we have a 4-phase process: QKV to Head-major, then Attention, then concatenation to Token-major, then Final projection. In the future, we could have a 3-phase process: QKV to Head-major, then Attention, then Direct strided projection. This would eliminate the memory reorganization overhead in Phase 3 by doing strided writes directly from head-major attention output. Expected improvement is 10-15% additional performance gain. This optimization is more complex to implement but could push us even closer to theoretical peak performance.
</aside>
                <h3>Future Optimizations: Direct Strided Projection</h3>
                <p>Eliminating Phase 3 for even better performance</p>
                <div style="display: flex; justify-content: space-between; gap: 30px;">
                    <div style="width: 45%;">
                        <h4>Current: 4-Phase Process</h4>
                        <div style="background: rgba(255, 213, 79, 0.1); padding: 15px; border-radius: 10px; margin: 10px 0;">
                            <strong>Phase 3: Concatenation</strong><br>
                            <small>Memory bandwidth bottleneck</small><br>
                            <small>~15% of total attention time</small>
                        </div>
                        <div class="avx-instruction">
                            // Current approach<br>
                            // 1. Head-major → Token-major (Phase 3)<br>
                            // 2. Token-major GEMM (Phase 4)<br>
                            for (int t = 0; t < tokens; t++) {<br>
                            &nbsp;&nbsp;concat_heads_to_token(t);<br>
                            &nbsp;&nbsp;gemv_projection(token_data[t]);<br>
                            }
                        </div>
                    </div>
                    <div style="width: 50%;">
                        <h4>Future: 3-Phase Process</h4>
                        <div style="background: rgba(129, 199, 132, 0.1); padding: 15px; border-radius: 10px; margin: 10px 0;">
                            <strong>Direct Strided Projection</strong><br>
                            <small>Skip concatenation entirely</small><br>
                            <small>Potential 10-15% speedup</small>
                        </div>
                        <div class="avx-instruction">
                            // Future optimization<br>
                            // Direct head-major → token-major projection<br>
                            for (int t = 0; t < tokens; t++) {<br>
                            &nbsp;&nbsp;for (int out_dim = 0; out_dim < embed_dim; out_dim++) {<br>
                            &nbsp;&nbsp;&nbsp;&nbsp;float sum = 0;<br>
                            &nbsp;&nbsp;&nbsp;&nbsp;for (int h = 0; h < num_heads; h++) {<br>
                            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sum += HEAD_ACCESS(h,t,d) * WEIGHT(h,d,out_dim);<br>
                            &nbsp;&nbsp;&nbsp;&nbsp;}<br>
                            &nbsp;&nbsp;&nbsp;&nbsp;output[t][out_dim] = sum;<br>
                            &nbsp;&nbsp;}<br>
                            }
                        </div>
                    </div>
                </div>
                <p class="fragment" style="margin-top: 20px; font-size: 0.9em;">
                    <strong>Research Question:</strong> Will strided access patterns offset the memory bandwidth savings? 
                    Optimal strategy may depend on core count and memory architecture.
                </p>
            </section>

            <!-- Include All Visualizations -->
            <section data-background-color="#ffffff" style="transform: scale(1.3);">
                <svg id="f5ec694b-105d-431c-963e-f2d3517e9849" data-name="Layer 1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1730.77 920.68"><text transform="translate(517.73 41.87)" style="isolation:isolate;font-size:38px;fill:#f4f4f4;font-family:Arial-BoldMT, Arial;font-weight:700">The Emergent Intelligence of<tspan x="519.42" y="0" style="letter-spacing:-0.037109375em"> </tspan><tspan x="528.57" y="0">Attention</tspan></text><text transform="translate(492.6 76.87)" style="isolation:isolate;font-size:22px;fill:#f4f4f4;font-family:ArialMT, Arial">How QKV Projections Create Understanding<tspan x="432.88" y="0" style="letter-spacing:-0.01806640625em"> </tspan><tspan x="438.59" y="0">Through Learned Relationships</tspan></text><path d="M27.38,91.87h1676c6.63,0,12,5.95,12,13.29v416.2c0,7.33-5.37,13.28-12,13.28H27.38c-6.62,0-12-5.95-12-13.28V105.16C15.38,97.82,20.76,91.87,27.38,91.87Z" style="fill:#e8eaf6;stroke:#3f51b5;stroke-width:3px"/><text transform="translate(452.28 131.87)" style="isolation:isolate;font-size:30px;fill:#1a237e;font-family:Arial-BoldMT, Arial;font-weight:700">THE GENIUS OF QK<tspan x="286.68" y="0" style="letter-spacing:-0.05517578125em">V</tspan><tspan x="305.04" y="0">: CRE</tspan><tspan x="386.7" y="0" style="letter-spacing:-0.07421875em">A</tspan><tspan x="406.14" y="0">TING THREE PERSPECTIVES</tspan></text><rect x="115.38" y="171.87" width="300" height="280" rx="8" style="fill:#c5cae9;stroke:#3f51b5;stroke-width:2px"/><text transform="translate(196.12 201.87)" style="isolation:isolate;font-size:20px;fill:#1a237e;font-family:Arial-BoldMT, Arial;font-weight:700">Original <tspan x="81.12" y="0" style="letter-spacing:-0.07421875em">T</tspan><tspan x="91.86" y="0">oken</tspan></text><rect x="135.38" y="221.87" width="260" height="60" rx="5" style="fill:#9fa8da;stroke:#5c6bc0"/><text transform="translate(178.97 256.87)" style="isolation:isolate;font-size:16px;font-family:CourierNewPSMT, Courier New">&quot;cat&quot; → [512 dims]</text><text transform="translate(135.38 311.87)" style="isolation:isolate;font-size:16px;fill:#283593;font-family:Arial-BoldMT, Arial;font-weight:700">One representation:</text><text transform="translate(135.38 336.87)" style="isolation:isolate;font-size:15px;fill:#333;font-family:ArialMT, Arial">• Semantic meaning</text><text transform="translate(135.38 356.87)" style="isolation:isolate;font-size:15px;fill:#333;font-family:ArialMT, Arial">• Syntactic role</text><text transform="translate(135.38 376.87)" style="isolation:isolate;font-size:15px;fill:#333;font-family:ArialMT, Arial">• Position info</text><text transform="translate(135.38 396.87)" style="isolation:isolate;font-size:15px;fill:#333;font-family:ArialMT, Arial">• Everything mixed!</text><rect x="135.38" y="421.87" width="260" height="90" rx="5" style="fill:#7986cb;stroke:#3f51b5"/><text transform="translate(188.05 446.87)" style="isolation:isolate;font-size:14px;fill:#fff;font-family:ArialMT, Arial">Problem: How does &quot;cat&quot;</text><text transform="translate(199.63 466.87)" style="isolation:isolate;font-size:14px;fill:#fff;font-family:ArialMT, Arial">know what to look for</text><text transform="translate(215.57 486.87)" style="isolation:isolate;font-size:14px;fill:#fff;font-family:ArialMT, Arial">in other tokens?</text><path d="M455.38,326.87l30,15-30,15Z" style="fill:#3f51b5"/><rect x="515.38" y="171.87" width="1166.66" height="326.66" rx="9.33" style="fill:#fff;stroke:#3f51b5;stroke-width:2px"/><text transform="translate(891.9 206.87)" style="isolation:isolate;font-size:23.33319091796875px;fill:#1a237e;font-family:Arial-BoldMT, Arial;font-weight:700">QK<tspan x="35" y="0" style="letter-spacing:-0.05520401997431276em">V</tspan><tspan x="49.28" y="0">: Three Learned </tspan><tspan x="230.8" y="0" style="letter-spacing:-0.05520401997431276em">T</tspan><tspan x="243.77" y="0" style="letter-spacing:0.000041852934021465326em">ransformations</tspan></text><rect x="538.72" y="230.21" width="350" height="233.33" rx="5.83" style="fill:#ffebee;stroke:#c62828;stroke-width:2px"/><text transform="translate(665.29 259.37)" style="isolation:isolate;font-size:20.9998722076416px;fill:#b71c1c;font-family:Arial-BoldMT, Arial;font-weight:700">Q (Query)</text><text transform="translate(605.9 288.54)" style="isolation:isolate;font-size:16.333234786987305px;font-family:CourierNewPSMT, Courier New">W_q transforms &quot;cat&quot; →</text><rect x="550.38" y="300.2" width="326.66" height="46.67" rx="3.5" style="fill:#ffcdd2;stroke:#d32f2f"/><text transform="translate(621.68 329.37)" style="isolation:isolate;font-size:17.499893188476562px;font-family:Arial-BoldMT, Arial;font-weight:700">&quot;What I&apos;m looking for&quot;</text><text transform="translate(550.38 370.2)" style="isolation:isolate;font-size:16.333234786987305px;fill:#333;font-family:ArialMT, Arial">Examples of learned queries:</text><text transform="translate(550.38 393.54)" style="isolation:isolate;font-size:15.166574478149414px;fill:#666;font-family:ArialMT, Arial">• &quot;Find my subject&quot;</text><text transform="translate(550.38 414.54)" style="isolation:isolate;font-size:15.166574478149414px;fill:#666;font-family:ArialMT, Arial">• &quot;Find descriptors of me&quot;</text><text transform="translate(550.38 435.54)" style="isolation:isolate;font-size:15.166574478149414px;fill:#666;font-family:ArialMT, Arial">• &quot;Find related concepts&quot;</text><text transform="translate(550.38 456.54)" style="isolation:isolate;font-size:15.166574478149414px;fill:#666;font-family:ArialMT, Arial">• &quot;Find my verb&quot;</text><rect x="912.05" y="230.21" width="350" height="233.33" rx="5.83" style="fill:#e8f5e9;stroke:#2e7d32;stroke-width:2px"/><text transform="translate(1050.29 259.37)" style="isolation:isolate;font-size:20.9998722076416px;fill:#1b5e20;font-family:Arial-BoldMT, Arial;font-weight:700">K (Key)</text><text transform="translate(979.23 288.54)" style="isolation:isolate;font-size:16.333234786987305px;font-family:CourierNewPSMT, Courier New">W_k transforms &quot;mat&quot; →</text><rect x="923.71" y="300.2" width="326.66" height="46.67" rx="3.5" style="fill:#c8e6c9;stroke:#43a047"/><text transform="translate(989.77 329.37)" style="isolation:isolate;font-size:17.499893188476562px;font-family:Arial-BoldMT, Arial;font-weight:700">&quot;What I offer/advertise&quot;</text><text transform="translate(923.71 370.2)" style="isolation:isolate;font-size:16.333234786987305px;fill:#333;font-family:ArialMT, Arial">Examples of learned keys:</text><text transform="translate(923.71 393.54)" style="isolation:isolate;font-size:15.166574478149414px;fill:#666;font-family:ArialMT, Arial">• &quot;I am a location&quot;</text><text transform="translate(923.71 414.54)" style="isolation:isolate;font-size:15.166574478149414px;fill:#666;font-family:ArialMT, Arial">• &quot;I am an object&quot;</text><text transform="translate(923.71 435.54)" style="isolation:isolate;font-size:15.166574478149414px;fill:#666;font-family:ArialMT, Arial">• &quot;I can be possessed&quot;</text><text transform="translate(923.71 456.54)" style="isolation:isolate;font-size:15.166574478149414px;fill:#666;font-family:ArialMT, Arial">• &quot;I am soft/comfortable&quot;</text><rect x="1285.38" y="230.21" width="350" height="233.33" rx="5.83" style="fill:#fff3e0;stroke:#f57c00;stroke-width:2px"/><text transform="translate(1416.03 259.37)" style="isolation:isolate;font-size:20.9998722076416px;fill:#e65100;font-family:Arial-BoldMT, Arial;font-weight:700">V (<tspan x="26.83" y="0" style="letter-spacing:-0.0551528653864162em">V</tspan><tspan x="39.68" y="0">alue)</tspan></text><text transform="translate(1352.56 288.54)" style="isolation:isolate;font-size:16.333234786987305px;font-family:CourierNewPSMT, Courier New">W_v transforms &quot;mat&quot; →</text><rect x="1297.04" y="300.2" width="326.66" height="46.67" rx="3.5" style="fill:#ffe0b2;stroke:#ff9800"/><text transform="translate(1380.63 329.37)" style="isolation:isolate;font-size:17.499893188476562px;font-family:Arial-BoldMT, Arial;font-weight:700">&quot;What I contribute&quot;</text><text transform="translate(1297.05 370.2)" style="isolation:isolate;font-size:16.333234786987305px;fill:#333;font-family:ArialMT, Arial">Actual information passed:</text><text transform="translate(1297.05 393.54)" style="isolation:isolate;font-size:15.166574478149414px;fill:#666;font-family:ArialMT, Arial">• Semantic features</text><text transform="translate(1297.05 414.54)" style="isolation:isolate;font-size:15.166574478149414px;fill:#666;font-family:ArialMT, Arial">• Syntactic properties</text><text transform="translate(1297.05 435.54)" style="isolation:isolate;font-size:15.166574478149414px;fill:#666;font-family:ArialMT, Arial">• Contextual meaning</text><text transform="translate(1297.05 456.54)" style="isolation:isolate;font-size:15.166574478149414px;fill:#666;font-family:ArialMT, Arial">•<tspan x="5.31" y="0" style="letter-spacing:-0.05518148239114559em"> </tspan><tspan x="8.69" y="0">Abstract concepts</tspan></text><path d="M27.38,561.87h1676a12.08,12.08,0,0,1,12,12.16v330.4a12.09,12.09,0,0,1-12,12.17H27.38a12.09,12.09,0,0,1-12-12.17V574A12.09,12.09,0,0,1,27.38,561.87Z" style="fill:#e8f5e9;stroke:#4caf50;stroke-width:3px"/><text transform="translate(323.4 601.87)" style="isolation:isolate;font-size:30px;fill:#1b5e20;font-family:Arial-BoldMT, Arial;font-weight:700">THE EMERGENT M<tspan x="271.67" y="0" style="letter-spacing:-0.07421875em">A</tspan><tspan x="291.11" y="0">TCHING: HOW</tspan><tspan x="497.74" y="0" style="letter-spacing:-0.037109375em"> </tspan><tspan x="504.96" y="0" style="letter-spacing:-0.07421875em">A</tspan><tspan x="524.4" y="0">TTENTION &quot;LEARNS&quot; REL</tspan><tspan x="902.84" y="0" style="letter-spacing:-0.07421875em">A</tspan><tspan x="922.28" y="0">TIONSHIPS</tspan></text><text transform="translate(165.38 637.3)" style="isolation:isolate;font-size:24.828596115112305px;fill:#2e7d32;font-family:Arial-BoldMT, Arial;font-weight:700">Example: &quot;The cat sat on the mat&quot;</text><rect x="165.38" y="671.16" width="225.71" height="67.71" rx="5.64" style="fill:#ffcdd2;stroke:#c62828"/><text transform="translate(243.59 699.37)" style="isolation:isolate;font-size:18.057161331176758px;font-family:Arial-BoldMT, Arial;font-weight:700">Q(&quot;cat&quot;)</text><text transform="translate(216.86 721.94)" style="isolation:isolate;font-size:15.800016403198242px;font-family:ArialMT, Arial">&quot;Where did I go?&quot;</text><rect x="503.95" y="671.16" width="169.29" height="67.71" rx="5.64" style="fill:#e8f5e9;stroke:#4caf50"/><text transform="translate(558.85 699.37)" style="isolation:isolate;font-size:15.800016403198242px;font-family:ArialMT, Arial">K(&quot;The&quot;)</text><text transform="translate(535.08 721.94)" style="isolation:isolate;font-size:13.542871475219727px;font-family:ArialMT, Arial">&quot;I&apos;m a determiner&quot;</text><rect x="695.81" y="671.16" width="169.29" height="67.71" rx="5.64" style="fill:#e8f5e9;stroke:#4caf50"/><text transform="translate(753.78 699.37)" style="isolation:isolate;font-size:15.800016403198242px;font-family:ArialMT, Arial">K(&quot;sat&quot;)</text><text transform="translate(737.47 721.94)" style="isolation:isolate;font-size:13.542871475219727px;font-family:ArialMT, Arial">&quot;I&apos;m an action&quot;</text><rect x="887.67" y="671.16" width="169.29" height="67.71" rx="5.64" style="fill:#a5d6a7;stroke:#4caf50;stroke-width:3px"/><text transform="translate(939.81 699.37)" style="isolation:isolate;font-size:15.800016403198242px;font-family:Arial-BoldMT, Arial;font-weight:700">K(&quot;mat&quot;)</text><text transform="translate(920.64 721.94)" style="isolation:isolate;font-size:13.542871475219727px;font-family:Arial-BoldMT, Arial;font-weight:700">&quot;I&apos;m a location!&quot;</text><path d="M278.24,738.87Q278.24,784,250,784" style="fill:none;stroke:#999;stroke-dasharray:2,2"/><text transform="translate(221.81 800.94)" style="isolation:isolate;font-size:12.414298057556152px;fill:#666;font-family:ArialMT, Arial">0.05</text><path d="M278.24,738.87q0,45.15,163.64,45.14" style="fill:none;stroke:#999;stroke-dasharray:2,2"/><text transform="translate(424.96 800.94)" style="isolation:isolate;font-size:12.414298057556152px;fill:#666;font-family:ArialMT, Arial">0.15</text><path d="M278.24,738.87q0,45.15,355.5,45.14" style="fill:none;stroke:#4caf50;stroke-width:3px"/><text transform="translate(616.81 800.94)" style="isolation:isolate;font-size:15.800016403198242px;fill:#2e7d32;font-family:Arial-BoldMT, Arial;font-weight:700">0.80!</text><rect x="1113.38" y="671.16" width="451.43" height="112.86" rx="5.64" style="fill:#c8e6c9;stroke:#43a047;stroke-width:2px"/><text transform="translate(1253.82 705.01)" style="isolation:isolate;font-size:18.057161331176758px;font-family:Arial-BoldMT, Arial;font-weight:700">Emergent Learning:</text><text transform="translate(1197.8 733.23)" style="isolation:isolate;font-size:15.800016403198242px;font-family:ArialMT, Arial">Model learned &quot;cat&quot; queries for locations</text><text transform="translate(1218 755.8)" style="isolation:isolate;font-size:15.800016403198242px;font-family:ArialMT, Arial">and &quot;mat&quot; advertises as a location!</text><rect x="165.38" y="806.87" width="1400" height="100" rx="5" style="fill:#f1f8e4;stroke:#689f38"/><text transform="translate(185.38 836.87)" style="isolation:isolate;font-size:18px;fill:#33691e;font-family:Arial-BoldMT, Arial;font-weight:700">The Beautiful Math:</text><text transform="translate(185.38 861.87)" style="isolation:isolate;font-size:16px;font-family:CourierNewPSMT, Courier New">score = Q(&quot;cat&quot;) · K(&quot;mat&quot;) / √d = &quot;How well do these match?&quot;</text><text transform="translate(185.38 886.87)" style="isolation:isolate;font-size:15px;fill:#555;font-family:ArialMT, Arial">Through training, W_q and W_k learn transformations that make related concepts have high dot products!</text></svg>
            </section>
            <section style="transform: scale(1.3);">
                <svg id="f563e7c9-5f77-4711-8564-d4f3fb439add" data-name="Layer 1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1730.77 1012.77"><rect x="15.38" y="24.72" width="1700" height="450" rx="12" style="fill:#fff8e1;stroke:#ff9800;stroke-width:3px"/><text transform="translate(265.96 64.72)" style="isolation:isolate;font-size:30px;fill:#e65100;font-family:Arial-BoldMT, Arial;font-weight:700">MU<tspan x="46.66" y="0" style="letter-spacing:-0.07421875em">L</tspan><tspan x="62.75" y="0">TI-HEAD</tspan><tspan x="184.41" y="0" style="letter-spacing:-0.037109375em"> </tspan><tspan x="191.63" y="0" style="letter-spacing:-0.07421875em">A</tspan><tspan x="211.07" y="0">TTENTION: DIFFERENT HEADS LEARN DIFFERENT REL</tspan><tspan x="1017.71" y="0" style="letter-spacing:-0.07421875em">A</tspan><tspan x="1037.15" y="0">TIONSHIPS</tspan></text><text transform="translate(65.38 104.72)" style="isolation:isolate;font-size:20px;fill:#f57c00;font-family:Arial-BoldMT, Arial;font-weight:700">Each head develops its own &quot;expertise&quot;:</text><rect x="65.38" y="134.72" width="380" height="150" rx="5" style="fill:#fff3e0;stroke:#ffb300"/><text transform="translate(116.83 159.72)" style="isolation:isolate;font-size:18px;fill:#f57c00;font-family:Arial-BoldMT, Arial;font-weight:700">Head 0: Syntactic Dependencies</text><text transform="translate(75.38 184.72)" style="isolation:isolate;font-size:14px;font-family:ArialMT, Arial">Learns to connect:</text><text transform="translate(75.38 204.72)" style="isolation:isolate;font-size:13px;fill:#666;font-family:ArialMT, Arial">• Subject → <tspan x="71.75" y="0" style="letter-spacing:-0.05517578125em">V</tspan><tspan x="79.7" y="0">erb</tspan></text><text transform="translate(75.38 222.72)" style="isolation:isolate;font-size:13px;fill:#666;font-family:ArialMT, Arial">• <tspan x="8.16" y="0" style="letter-spacing:-0.05517578125em">V</tspan><tspan x="16.12" y="0">erb → Object</tspan></text><text transform="translate(75.38 240.72)" style="isolation:isolate;font-size:13px;fill:#666;font-family:ArialMT, Arial">• Modifier → Modified</text><rect x="75.38" y="249.72" width="360" height="25" rx="3" style="fill:#ffe0b2;stroke:#ff9800"/><text transform="translate(172.09 267.72)" style="isolation:isolate;font-size:13px;font-family:ArialMT, Arial">&quot;cat&quot; strongly attends to &quot;sat&quot;</text><rect x="475.38" y="134.72" width="380" height="150" rx="5" style="fill:#fff3e0;stroke:#ffb300"/><text transform="translate(561.36 159.72)" style="isolation:isolate;font-size:18px;fill:#f57c00;font-family:Arial-BoldMT, Arial;font-weight:700">Head 1: Positional/Local</text><text transform="translate(485.38 184.72)" style="isolation:isolate;font-size:14px;font-family:ArialMT, Arial">Learns to connect:</text><text transform="translate(485.38 204.72)" style="isolation:isolate;font-size:13px;fill:#666;font-family:ArialMT, Arial">•<tspan x="4.55" y="0" style="letter-spacing:-0.05517578125em"> </tspan><tspan x="7.45" y="0">Adjacent words</tspan></text><text transform="translate(485.38 222.72)" style="isolation:isolate;font-size:13px;fill:#666;font-family:ArialMT, Arial">• Phrase boundaries</text><text transform="translate(485.38 240.72)" style="isolation:isolate;font-size:13px;fill:#666;font-family:ArialMT, Arial">• Local context</text><rect x="485.38" y="249.72" width="360" height="25" rx="3" style="fill:#ffe0b2;stroke:#ff9800"/><text transform="translate(581.72 267.72)" style="isolation:isolate;font-size:13px;font-family:ArialMT, Arial">&quot;the&quot; strongly attends to &quot;cat&quot;</text><rect x="885.38" y="134.72" width="380" height="150" rx="5" style="fill:#fff3e0;stroke:#ffb300"/><text transform="translate(957.35 159.72)" style="isolation:isolate;font-size:18px;fill:#f57c00;font-family:Arial-BoldMT, Arial;font-weight:700">Head 2: Semantic Relations</text><text transform="translate(895.38 184.72)" style="isolation:isolate;font-size:14px;font-family:ArialMT, Arial">Learns to connect:</text><text transform="translate(895.38 204.72)" style="isolation:isolate;font-size:13px;fill:#666;font-family:ArialMT, Arial">• Related concepts</text><text transform="translate(895.38 222.72)" style="isolation:isolate;font-size:13px;fill:#666;font-family:ArialMT, Arial">• Co-occurring terms</text><text transform="translate(895.38 240.72)" style="isolation:isolate;font-size:13px;fill:#666;font-family:ArialMT, Arial">•<tspan x="4.55" y="0" style="letter-spacing:-0.01806640625em"> </tspan><tspan x="7.93" y="0" style="letter-spacing:-0.11083984375em">T</tspan><tspan x="14.43" y="0">opic coherence</tspan></text><rect x="895.38" y="249.72" width="360" height="25" rx="3" style="fill:#ffe0b2;stroke:#ff9800"/><text transform="translate(973.3 267.72)" style="isolation:isolate;font-size:13px;font-family:ArialMT, Arial">&quot;cat&quot; attends to &quot;mat&quot; (both objects)</text><rect x="1295.38" y="134.72" width="380" height="150" rx="5" style="fill:#fff3e0;stroke:#ffb300"/><text transform="translate(1334.36 159.72)" style="isolation:isolate;font-size:18px;fill:#f57c00;font-family:Arial-BoldMT, Arial;font-weight:700">Head 3: Long-Range Dependencies</text><text transform="translate(1305.38 184.72)" style="isolation:isolate;font-size:14px;font-family:ArialMT, Arial">Learns to connect:</text><text transform="translate(1305.38 204.72)" style="isolation:isolate;font-size:13px;fill:#666;font-family:ArialMT, Arial">• Coreference (&quot;it&quot; → noun)</text><text transform="translate(1305.38 222.72)" style="isolation:isolate;font-size:13px;fill:#666;font-family:ArialMT, Arial">• Discourse markers</text><text transform="translate(1305.38 240.72)" style="isolation:isolate;font-size:13px;fill:#666;font-family:ArialMT, Arial">• Paragraph structure</text><rect x="1305.38" y="249.72" width="360" height="25" rx="3" style="fill:#ffe0b2;stroke:#ff9800"/><text transform="translate(1405.34 267.72)" style="isolation:isolate;font-size:13px;font-family:ArialMT, Arial">&quot;it&quot; attends strongly to &quot;mat&quot;</text><rect x="65.38" y="304.72" width="1500" height="150" rx="5" style="fill:#ffecb3;stroke:#ffa000"/><text transform="translate(85.38 334.72)" style="isolation:isolate;font-size:18px;fill:#e65100;font-family:Arial-BoldMT, Arial;font-weight:700">Attention Patterns<tspan x="156.01" y="0" style="letter-spacing:-0.037109375em"> </tspan><tspan x="160.34" y="0">Across Heads:</tspan></text><text transform="translate(85.38 349.72)" style="isolation:isolate;font-size:12px;fill:#666;font-family:ArialMT, Arial">Head 0 (Syntax)</text><rect x="85.38" y="354.72" width="15" height="15" style="fill:#ffeb3b"/><rect x="101.38" y="354.72" width="15" height="15" style="fill:#ff5722;isolation:isolate;opacity:0.800000011920929"/><rect x="117.38" y="354.72" width="15" height="15" style="fill:#ffeb3b"/><rect x="85.38" y="370.72" width="15" height="15" style="fill:#4caf50;isolation:isolate;opacity:0.30000001192092896"/><rect x="101.38" y="370.72" width="15" height="15" style="fill:#ffeb3b"/><rect x="117.38" y="370.72" width="15" height="15" style="fill:#ff5722;isolation:isolate;opacity:0.800000011920929"/><rect x="85.38" y="386.72" width="15" height="15" style="fill:#ffeb3b"/><rect x="101.38" y="386.72" width="15" height="15" style="fill:#4caf50;isolation:isolate;opacity:0.30000001192092896"/><rect x="117.38" y="386.72" width="15" height="15" style="fill:#ffeb3b"/><text transform="translate(265.38 349.72)" style="isolation:isolate;font-size:12px;fill:#666;font-family:ArialMT, Arial">Head 1 (Local)</text><rect x="265.38" y="354.72" width="15" height="15" style="fill:#ff5722;isolation:isolate;opacity:0.8999999761581421"/><rect x="281.38" y="354.72" width="15" height="15" style="fill:#ff5722;isolation:isolate;opacity:0.699999988079071"/><rect x="297.38" y="354.72" width="15" height="15" style="fill:#4caf50;isolation:isolate;opacity:0.30000001192092896"/><rect x="265.38" y="370.72" width="15" height="15" style="fill:#4caf50;isolation:isolate;opacity:0.30000001192092896"/><rect x="281.38" y="370.72" width="15" height="15" style="fill:#ff5722;isolation:isolate;opacity:0.8999999761581421"/><rect x="297.38" y="370.72" width="15" height="15" style="fill:#ff5722;isolation:isolate;opacity:0.699999988079071"/><rect x="265.38" y="386.72" width="15" height="15" style="fill:#4caf50;isolation:isolate;opacity:0.20000000298023224"/><rect x="281.38" y="386.72" width="15" height="15" style="fill:#4caf50;isolation:isolate;opacity:0.30000001192092896"/><rect x="297.38" y="386.72" width="15" height="15" style="fill:#ff5722;isolation:isolate;opacity:0.8999999761581421"/><text transform="translate(465.38 344.72)" style="isolation:isolate;font-size:14px;fill:#666;font-family:ArialMT, Arial">Di<tspan x="13.22" y="0" style="letter-spacing:-0.01806640625em">f</tspan><tspan x="16.86" y="0">ferent heads learn completely di</tspan><tspan x="215.3" y="0" style="letter-spacing:-0.01806640625em">f</tspan><tspan x="218.94" y="0">ferent attention patterns!</tspan></text><text transform="translate(465.38 364.72)" style="isolation:isolate;font-size:14px;fill:#666;font-family:ArialMT, Arial">This is emergent specialization through training.</text><rect x="15.38" y="494.72" width="1700" height="500" rx="12" style="fill:#e3f2fd;stroke:#2196f3;stroke-width:3px"/><text transform="translate(323.13 534.72)" style="isolation:isolate;font-size:30px;fill:#0d47a1;font-family:Arial-BoldMT, Arial;font-weight:700"><tspan style="letter-spacing:-0.07421875em">A</tspan><tspan x="19.44" y="0">TTENTION EVOLUTION</tspan><tspan x="352.76" y="0" style="letter-spacing:-0.037109375em"> </tspan><tspan x="359.99" y="0">ACROSS L</tspan><tspan x="515" y="0" style="letter-spacing:-0.091796875em">A</tspan><tspan x="533.91" y="0">YERS: FROM SYN</tspan><tspan x="792.26" y="0" style="letter-spacing:-0.07421875em">T</tspan><tspan x="808.36" y="0">AX </tspan><tspan x="858.37" y="0" style="letter-spacing:-0.01806640625em">T</tspan><tspan x="876.15" y="0">O SEMANTICS</tspan></text><rect x="115.38" y="574.72" width="350" height="350" rx="8" style="fill:#e1f5fe;stroke:#039be5;stroke-width:2px"/><text transform="translate(207 604.72)" style="isolation:isolate;font-size:20px;fill:#01579b;font-family:Arial-BoldMT, Arial;font-weight:700">Early Layers (0-3)</text><text transform="translate(130.38 634.72)" style="isolation:isolate;font-size:16px;fill:#0277bd;font-family:Arial-BoldMT, Arial;font-weight:700">Focus: Local patterns</text><text transform="translate(130.38 659.72)" style="isolation:isolate;font-size:14px;fill:#333;font-family:ArialMT, Arial">• <tspan x="8.79" y="0" style="letter-spacing:-0.01806640625em">W</tspan><tspan x="21.75" y="0">ord boundaries</tspan></text><text transform="translate(130.38 679.72)" style="isolation:isolate;font-size:14px;fill:#333;font-family:ArialMT, Arial">• Part of speech</text><text transform="translate(130.38 699.72)" style="isolation:isolate;font-size:14px;fill:#333;font-family:ArialMT, Arial">• Phrase structure</text><rect x="130.38" y="719.72" width="320" height="80" rx="5" style="fill:#b3e5fc;stroke:#0288d1"/><text transform="translate(182.62 744.72)" style="isolation:isolate;font-size:14px;font-family:Arial-BoldMT, Arial;font-weight:700">Example: &quot;The quick brown fox&quot;</text><text transform="translate(140.38 769.72)" style="isolation:isolate;font-size:13px;font-family:ArialMT, Arial">&quot;quick&quot; → &quot;brown&quot; (adjacent modifiers)</text><text transform="translate(140.38 789.72)" style="isolation:isolate;font-size:13px;font-family:ArialMT, Arial">&quot;The&quot; → &quot;fox&quot; (determiner → noun)</text><rect x="130.38" y="814.72" width="320" height="90" rx="5" style="fill:#81d4fa;stroke:#039be5"/><text transform="translate(209.48 839.72)" style="isolation:isolate;font-size:14px;font-family:Arial-BoldMT, Arial;font-weight:700">Attention is mostly local</text><text transform="translate(140.38 859.72)" style="isolation:isolate;font-size:12px;font-family:ArialMT, Arial">• Positional encoding dominant</text><text transform="translate(140.38 876.72)" style="isolation:isolate;font-size:12px;font-family:ArialMT, Arial">• Simple grammatical patterns</text><text transform="translate(140.38 893.72)" style="isolation:isolate;font-size:12px;font-family:ArialMT, Arial">• Building basic representations</text><rect x="515.38" y="574.72" width="350" height="350" rx="8" style="fill:#c8e6c9;stroke:#43a047;stroke-width:2px"/><text transform="translate(599.8 604.72)" style="isolation:isolate;font-size:20px;fill:#1b5e20;font-family:Arial-BoldMT, Arial;font-weight:700">Middle Layers (4-8)</text><text transform="translate(530.38 634.72)" style="isolation:isolate;font-size:16px;fill:#2e7d32;font-family:Arial-BoldMT, Arial;font-weight:700">Focus: Syntactic structures</text><text transform="translate(530.38 659.72)" style="isolation:isolate;font-size:14px;fill:#333;font-family:ArialMT, Arial">• Subject-verb-object</text><text transform="translate(530.38 679.72)" style="isolation:isolate;font-size:14px;fill:#333;font-family:ArialMT, Arial">• Clause boundaries</text><text transform="translate(530.38 699.72)" style="isolation:isolate;font-size:14px;fill:#333;font-family:ArialMT, Arial">• Dependency parsing</text><rect x="530.38" y="719.72" width="320" height="80" rx="5" style="fill:#a5d6a7;stroke:#4caf50"/><text transform="translate(562 744.72)" style="isolation:isolate;font-size:14px;font-family:Arial-BoldMT, Arial;font-weight:700">Example: &quot;The cat that sat on the mat&quot;</text><text transform="translate(540.38 769.72)" style="isolation:isolate;font-size:13px;font-family:ArialMT, Arial">&quot;cat&quot; → &quot;sat&quot; (subj → verb)</text><text transform="translate(540.38 789.72)" style="isolation:isolate;font-size:13px;font-family:ArialMT, Arial">&quot;that&quot; → &quot;cat&quot; (relative clause)</text><rect x="530.38" y="814.72" width="320" height="90" rx="5" style="fill:#81c784;stroke:#43a047"/><text transform="translate(608.7 839.72)" style="isolation:isolate;font-size:14px;font-family:Arial-BoldMT, Arial;font-weight:700">Attention spans phrases</text><text transform="translate(540.38 859.72)" style="isolation:isolate;font-size:12px;font-family:ArialMT, Arial">• Grammatical dependencies</text><text transform="translate(540.38 876.72)" style="isolation:isolate;font-size:12px;font-family:ArialMT, Arial">• Constituent structure</text><text transform="translate(540.38 893.72)" style="isolation:isolate;font-size:12px;font-family:ArialMT, Arial">• Building compositional meaning</text><rect x="915.38" y="574.72" width="350" height="350" rx="8" style="fill:#f3e5f5;stroke:#7b1fa2;stroke-width:2px"/><text transform="translate(1001.45 604.72)" style="isolation:isolate;font-size:20px;fill:#4a148c;font-family:Arial-BoldMT, Arial;font-weight:700">Later Layers (9-12)</text><text transform="translate(930.38 634.72)" style="isolation:isolate;font-size:16px;fill:#6a1b9a;font-family:Arial-BoldMT, Arial;font-weight:700">Focus: Semantic concepts</text><text transform="translate(930.38 659.72)" style="isolation:isolate;font-size:14px;fill:#333;font-family:ArialMT, Arial">•<tspan x="4.9" y="0" style="letter-spacing:-0.01806640625em"> </tspan><tspan x="8.54" y="0" style="letter-spacing:-0.11083984375em">T</tspan><tspan x="15.54" y="0">opic coherence</tspan></text><text transform="translate(930.38 679.72)" style="isolation:isolate;font-size:14px;fill:#333;font-family:ArialMT, Arial">•<tspan x="4.9" y="0" style="letter-spacing:-0.05517578125em"> </tspan><tspan x="8.02" y="0">Abstract reasoning</tspan></text><text transform="translate(930.38 699.72)" style="isolation:isolate;font-size:14px;fill:#333;font-family:ArialMT, Arial">• Long-range context</text><rect x="930.38" y="719.72" width="320" height="80" rx="5" style="fill:#e1bee7;stroke:#8e24aa"/><text transform="translate(989.22 744.72)" style="isolation:isolate;font-size:14px;font-family:Arial-BoldMT, Arial;font-weight:700">Example: &quot;It was comfortable&quot;</text><text transform="translate(940.38 769.72)" style="isolation:isolate;font-size:13px;font-family:ArialMT, Arial">&quot;It&quot; → &quot;mat&quot; (coreference, 10 tokens away)</text><text transform="translate(940.38 789.72)" style="isolation:isolate;font-size:13px;font-family:ArialMT, Arial">&quot;comfortable&quot; → &quot;soft&quot; (earlier description)</text><rect x="930.38" y="814.72" width="320" height="90" rx="5" style="fill:#ce93d8;stroke:#7b1fa2"/><text transform="translate(1007.53 839.72)" style="isolation:isolate;font-size:14px;font-family:Arial-BoldMT, Arial;font-weight:700">Attention is task-specific</text><text transform="translate(940.38 859.72)" style="isolation:isolate;font-size:12px;font-family:ArialMT, Arial">•<tspan x="4.2" y="0" style="letter-spacing:-0.05517578125em"> </tspan><tspan x="6.87" y="0">Abstract relationships</tspan></text><text transform="translate(940.38 876.72)" style="isolation:isolate;font-size:12px;font-family:ArialMT, Arial">• Discourse structure</text><text transform="translate(940.38 893.72)" style="isolation:isolate;font-size:12px;font-family:ArialMT, Arial">• Preparing final prediction</text><rect x="1315.38" y="574.72" width="350" height="350" rx="8" style="fill:#ffebee;stroke:#c62828;stroke-width:2px"/><text transform="translate(1397.56 604.72)" style="isolation:isolate;font-size:20px;fill:#b71c1c;font-family:Arial-BoldMT, Arial;font-weight:700">Final Layers (<tspan x="128.94" y="0" style="letter-spacing:-0.05517578125em">1</tspan><tspan x="138.96" y="0">1-12)</tspan></text><text transform="translate(1330.38 634.72)" style="isolation:isolate;font-size:16px;fill:#d32f2f;font-family:Arial-BoldMT, Arial;font-weight:700">Focus: <tspan x="56.89" y="0" style="letter-spacing:-0.07421875em">T</tspan><tspan x="65.48" y="0">ask-specific output</tspan></text><text transform="translate(1330.38 659.72)" style="isolation:isolate;font-size:14px;fill:#333;font-family:ArialMT, Arial">• Next token prediction</text><text transform="translate(1330.38 679.72)" style="isolation:isolate;font-size:14px;fill:#333;font-family:ArialMT, Arial">• Classification features</text><text transform="translate(1330.38 699.72)" style="isolation:isolate;font-size:14px;fill:#333;font-family:ArialMT, Arial">• Output formatting</text><rect x="1330.38" y="719.72" width="320" height="80" rx="5" style="fill:#ffcdd2;stroke:#e53935"/><text transform="translate(1388.08 744.72)" style="isolation:isolate;font-size:14px;font-family:Arial-BoldMT, Arial;font-weight:700">Example: Predicting next word</text><text transform="translate(1340.38 769.72)" style="isolation:isolate;font-size:13px;font-family:ArialMT, Arial">Aggregate all relevant context</text><text transform="translate(1340.38 789.72)" style="isolation:isolate;font-size:13px;font-family:ArialMT, Arial">Focus on predictive features</text><rect x="1330.38" y="814.72" width="320" height="90" rx="5" style="fill:#ef9a9a;stroke:#c62828"/><text transform="translate(1397.81 839.72)" style="isolation:isolate;font-size:14px;font-family:Arial-BoldMT, Arial;font-weight:700">Attention is highly selective</text><text transform="translate(1340.38 859.72)" style="isolation:isolate;font-size:12px;font-family:ArialMT, Arial">•<tspan x="4.2" y="0" style="letter-spacing:-0.01806640625em"> </tspan><tspan x="7.32" y="0" style="letter-spacing:-0.11083984375em">T</tspan><tspan x="13.32" y="0">ask-oriented patterns</tspan></text><text transform="translate(1340.38 876.72)" style="isolation:isolate;font-size:12px;font-family:ArialMT, Arial">• Output preparation</text><text transform="translate(1340.38 893.72)" style="isolation:isolate;font-size:12px;font-family:ArialMT, Arial">• Maximum information extraction</text><path d="M1595.38,924.72l20,10-20,10Z" style="fill:#666"/><text transform="translate(619.21 959.72)" style="isolation:isolate;font-size:16px;fill:#666;font-family:ArialMT, Arial">Progression: Local Features → Syntax → Semantics →<tspan x="393.94" y="0" style="letter-spacing:-0.01806640625em"> </tspan><tspan x="398.09" y="0" style="letter-spacing:-0.11083984375em">T</tspan><tspan x="406.09" y="0">ask-Specific</tspan></text></svg>
            </section>
            <section style="transform:scale(1.2);">
                <svg id="accc7d51-219a-4409-a4ba-5c5827223f06" data-name="Layer 1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1730.77 1076.43"><path d="M27.38,13.06h1676c6.63,0,12,5.67,12,12.65V632.8c0,7-5.37,12.65-12,12.65H27.38c-6.62,0-12-5.67-12-12.65V25.71C15.38,18.73,20.76,13.06,27.38,13.06Z" style="fill:#fff3e0;stroke:#ff6f00;stroke-width:3px"/><text transform="translate(346.75 53.06)" style="isolation:isolate;font-size:30px;fill:#e65100;font-family:Arial-BoldMT, Arial;font-weight:700">EMERGENT PROPERTIES: WH<tspan x="440.02" y="0" style="letter-spacing:-0.07421875em">A</tspan><tspan x="459.46" y="0">T THE MODE</tspan><tspan x="644.46" y="0" style="letter-spacing:-0.01806640625em">L</tspan><tspan x="662.24" y="0" xml:space="preserve"> DISCOVERS ON ITS OWN</tspan></text><text transform="translate(115.38 93.06)" style="isolation:isolate;font-size:22px;fill:#f57c00;font-family:Arial-BoldMT, Arial;font-weight:700">During training, attention heads spontaneously learn to:</text><rect x="115.38" y="133.06" width="750" height="200" rx="8" style="fill:#ffe0b2;stroke:#ff9800;stroke-width:2px"/><text transform="translate(310.92 163.06)" style="isolation:isolate;font-size:20px;fill:#e65100;font-family:Arial-BoldMT, Arial;font-weight:700">1. Induction Heads (Pattern Matching)</text><text transform="translate(135.38 193.06)" style="isolation:isolate;font-size:16px;font-family:Arial-BoldMT, Arial;font-weight:700">Discovery: Copying patterns from context</text><rect x="135.38" y="208.06" width="710" height="50" rx="5" style="fill:#fff8e1;stroke:#ffa726"/><text transform="translate(145.38 228.06)" style="isolation:isolate;font-size:14px;font-family:CourierNewPSMT, Courier New">Example: &quot;The cat sat on the mat. The dog sat on the...&quot;</text><text transform="translate(145.38 248.06)" style="isolation:isolate;font-size:14px;font-family:CourierNewPSMT, Courier New">Model learns: After &quot;The dog sat on the&quot; → predict &quot;mat&quot;</text><text transform="translate(135.38 283.06)" style="isolation:isolate;font-size:15px;font-family:ArialMT, Arial">• No explicit programming for this!</text><text transform="translate(135.38 303.06)" style="isolation:isolate;font-size:15px;font-family:ArialMT, Arial">• Emerges around layer 5-6</text><text transform="translate(135.38 323.06)" style="isolation:isolate;font-size:15px;font-family:ArialMT, Arial">• Critical for in-context learning</text><rect x="915.38" y="133.06" width="750" height="200" rx="8" style="fill:#e8f5e9;stroke:#4caf50;stroke-width:2px"/><text transform="translate(1172 163.06)" style="isolation:isolate;font-size:20px;fill:#2e7d32;font-family:Arial-BoldMT, Arial;font-weight:700">2. Factual Recall Circuits</text><text transform="translate(935.38 193.06)" style="isolation:isolate;font-size:16px;font-family:Arial-BoldMT, Arial;font-weight:700">Discovery: Knowledge retrieval patterns</text><rect x="935.38" y="208.06" width="710" height="50" rx="5" style="fill:#f1f8e4;stroke:#66bb6a"/><text transform="translate(945.38 228.06)" style="isolation:isolate;font-size:14px;font-family:CourierNewPSMT, Courier New">Example: &quot;The capital of France is...&quot;</text><text transform="translate(945.38 248.06)" style="isolation:isolate;font-size:14px;font-family:CourierNewPSMT, Courier New">Specific heads learn to route &quot;France&quot; → &quot;capital&quot; → &quot;Paris&quot;</text><text transform="translate(935.38 283.06)" style="isolation:isolate;font-size:15px;font-family:ArialMT, Arial">• Di<tspan x="23.58" y="0" style="letter-spacing:-0.01806640625em">f</tspan><tspan x="27.48" y="0">ferent heads for di</tspan><tspan x="148.39" y="0" style="letter-spacing:-0.01806640625em">f</tspan><tspan x="152.29" y="0">ferent fact types</tspan></text><text transform="translate(935.38 303.06)" style="isolation:isolate;font-size:15px;font-family:ArialMT, Arial">• Emerges in middle layers (6-9)</text><text transform="translate(935.38 323.06)" style="isolation:isolate;font-size:15px;font-family:ArialMT, Arial">• Can be surgically modified!</text><rect x="115.38" y="353.06" width="750" height="200" rx="8" style="fill:#e3f2fd;stroke:#2196f3;stroke-width:2px"/><text transform="translate(336.23 383.06)" style="isolation:isolate;font-size:20px;fill:#1565c0;font-family:Arial-BoldMT, Arial;font-weight:700">3. Syntactic<tspan x="111.16" y="0" style="letter-spacing:-0.037109375em"> </tspan><tspan x="115.98" y="0">Agreement </tspan><tspan x="226.01" y="0" style="letter-spacing:-0.05517578125em">T</tspan><tspan x="237.12" y="0">rackers</tspan></text><text transform="translate(135.38 413.06)" style="isolation:isolate;font-size:16px;font-family:Arial-BoldMT, Arial;font-weight:700">Discovery: Grammar enforcement</text><rect x="135.38" y="428.06" width="710" height="50" rx="5" style="fill:#e1f5fe;stroke:#42a5f5"/><text transform="translate(145.38 448.06)" style="isolation:isolate;font-size:14px;font-family:CourierNewPSMT, Courier New">Example: &quot;The dogs [are/is] barking&quot;</text><text transform="translate(145.38 468.06)" style="isolation:isolate;font-size:14px;font-family:CourierNewPSMT, Courier New">Heads learn: plural subject → plural verb</text><text transform="translate(135.38 503.06)" style="isolation:isolate;font-size:15px;font-family:ArialMT, Arial">•<tspan x="5.25" y="0" style="letter-spacing:-0.01806640625em"> </tspan><tspan x="9.15" y="0" style="letter-spacing:-0.037109375em">T</tspan><tspan x="17.75" y="0">racks numbe</tspan><tspan x="103.62" y="0" style="letter-spacing:-0.05517578125em">r</tspan><tspan x="107.79" y="0">, gende</tspan><tspan x="157.84" y="0" style="letter-spacing:-0.05517578125em">r</tspan><tspan x="162" y="0">, tense</tspan></text><text transform="translate(135.38 523.06)" style="isolation:isolate;font-size:15px;font-family:ArialMT, Arial">• Emerges early (layers 2-4)</text><text transform="translate(135.38 543.06)" style="isolation:isolate;font-size:15px;font-family:ArialMT, Arial">• Remarkably consistent across models</text><rect x="915.38" y="353.06" width="750" height="200" rx="8" style="fill:#f3e5f5;stroke:#7b1fa2;stroke-width:2px"/><text transform="translate(1166.44 383.06)" style="isolation:isolate;font-size:20px;fill:#6a1b9a;font-family:Arial-BoldMT, Arial;font-weight:700">4. Semantic Role Labelers</text><text transform="translate(935.38 413.06)" style="isolation:isolate;font-size:16px;font-family:Arial-BoldMT, Arial;font-weight:700">Discovery: Understanding &quot;who did what to whom&quot;</text><rect x="935.38" y="428.06" width="710" height="50" rx="5" style="fill:#f3e5f5;stroke:#9c27b0"/><text transform="translate(945.38 448.06)" style="isolation:isolate;font-size:14px;font-family:CourierNewPSMT, Courier New">Example: &quot;Alice gave Bob the book&quot;</text><text transform="translate(945.38 468.06)" style="isolation:isolate;font-size:14px;font-family:CourierNewPSMT, Courier New">Heads learn: Alice=giver, Bob=receiver, book=object</text><text transform="translate(935.38 503.06)" style="isolation:isolate;font-size:15px;font-family:ArialMT, Arial">• No explicit semantic role labels in training!</text><text transform="translate(935.38 523.06)" style="isolation:isolate;font-size:15px;font-family:ArialMT, Arial">• Emerges in layers 7-10</text><text transform="translate(935.38 543.06)" style="isolation:isolate;font-size:15px;font-family:ArialMT, Arial">• Crucial for comprehension</text><rect x="115.38" y="563.06" width="1500" height="70" rx="10" style="fill:#ff6f00;stroke:#e65100;stroke-width:2px"/><text transform="translate(406.98 598.06)" style="isolation:isolate;font-size:22px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">These patterns emerge without explicit programming - just from predicting next tokens!</text><text transform="translate(574.02 623.06)" style="isolation:isolate;font-size:18px;fill:#fff;font-family:ArialMT, Arial">The model discovers these structures because they&apos;re useful for the task.</text><rect x="15.38" y="663.06" width="1700" height="400" rx="12" style="fill:#e8eaf6;stroke:#3f51b5;stroke-width:3px"/><text transform="translate(356.76 703.06)" style="isolation:isolate;font-size:30px;fill:#1a237e;font-family:Arial-BoldMT, Arial;font-weight:700">WH<tspan x="49.98" y="0" style="letter-spacing:-0.01806640625em">Y </tspan><tspan x="77.24" y="0">YOUR C IMPLEMEN</tspan><tspan x="360.59" y="0" style="letter-spacing:-0.07421875em">TA</tspan><tspan x="396.12" y="0">TION ENABLES BETTER UNDERS</tspan><tspan x="882.82" y="0" style="letter-spacing:-0.07421875em">T</tspan><tspan x="898.92" y="0">ANDING</tspan></text><rect x="115.38" y="743.06" width="500" height="280" rx="8" style="fill:#c5cae9;stroke:#5c6bc0;stroke-width:2px"/><text transform="translate(248.63 773.06)" style="isolation:isolate;font-size:22px;fill:#283593;font-family:Arial-BoldMT, Arial;font-weight:700">Perfect Interpretability</text><text transform="translate(135.38 803.06)" style="isolation:isolate;font-size:16px;font-family:Arial-BoldMT, Arial;font-weight:700"><tspan style="letter-spacing:-0.07421875em">Y</tspan><tspan x="9.48" y="0">our advantages:</tspan></text><text transform="translate(135.38 828.06)" style="isolation:isolate;font-size:15px;font-family:ArialMT, Arial">• Every attention score is accessible</text><text transform="translate(135.38 848.06)" style="isolation:isolate;font-size:15px;font-family:ArialMT, Arial">• Can trace exact computation path</text><text transform="translate(135.38 868.06)" style="isolation:isolate;font-size:15px;font-family:ArialMT, Arial">• No hidden framework magic</text><rect x="135.38" y="888.06" width="460" height="120" rx="5" style="fill:#9fa8da;stroke:#3f51b5"/><text transform="translate(282.58 913.06)" style="isolation:isolate;font-size:15px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700"><tspan style="letter-spacing:-0.07421875em">Y</tspan><tspan x="8.89" y="0">ou can literally watch:</tspan></text><text transform="translate(145.38 938.06)" style="isolation:isolate;font-size:14px;fill:#fff;font-family:CourierNewPSMT, Courier New">for (head = 0; head &lt; 8; head++) {</text><text transform="translate(145.38 958.06)" style="isolation:isolate;font-size:14px;fill:#fff;font-family:CourierNewPSMT, Courier New">printf(&quot;Head %d attends:&quot;, head);</text><text transform="translate(145.38 978.06)" style="isolation:isolate;font-size:14px;fill:#fff;font-family:CourierNewPSMT, Courier New">dump_attention_pattern(attn[head]);</text><text transform="translate(145.38 998.06)" style="isolation:isolate;font-size:14px;fill:#fff;font-family:CourierNewPSMT, Courier New">}</text><rect x="665.38" y="743.06" width="500" height="280" rx="8" style="fill:#e8f5e9;stroke:#4caf50;stroke-width:2px"/><text transform="translate(803.52 773.06)" style="isolation:isolate;font-size:22px;fill:#1b5e20;font-family:Arial-BoldMT, Arial;font-weight:700">Fast Experimentation</text><text transform="translate(685.38 803.06)" style="isolation:isolate;font-size:16px;font-family:Arial-BoldMT, Arial;font-weight:700">Speed enables discovery:</text><text transform="translate(685.38 828.06)" style="isolation:isolate;font-size:15px;font-family:ArialMT, Arial">• 10x faster = 10x more experiments</text><text transform="translate(685.38 848.06)" style="isolation:isolate;font-size:15px;font-family:ArialMT, Arial">• Can test hypotheses in real-time</text><text transform="translate(685.38 868.06)" style="isolation:isolate;font-size:15px;font-family:ArialMT, Arial">• Modify attention patterns live</text><rect x="685.38" y="888.06" width="460" height="120" rx="5" style="fill:#a5d6a7;stroke:#4caf50"/><text transform="translate(833.68 913.06)" style="isolation:isolate;font-size:15px;font-family:Arial-BoldMT, Arial;font-weight:700">Research possibilities:</text><text transform="translate(695.38 938.06)" style="isolation:isolate;font-size:14px;font-family:ArialMT, Arial">• &quot;What if head 3 only looked locally?&quot;</text><text transform="translate(695.38 958.06)" style="isolation:isolate;font-size:14px;font-family:ArialMT, Arial">• &quot;Can we force semantic heads earlier?&quot;</text><text transform="translate(695.38 978.06)" style="isolation:isolate;font-size:14px;font-family:ArialMT, Arial">• &quot;How minimal can attention be?&quot;</text><text transform="translate(695.38 998.06)" style="isolation:isolate;font-size:14px;font-family:ArialMT, Arial">→ Change code, test immediately!</text><rect x="1215.38" y="743.06" width="400" height="280" rx="8" style="fill:#ffebee;stroke:#c62828;stroke-width:2px"/><text transform="translate(1329.82 773.06)" style="isolation:isolate;font-size:22px;fill:#b71c1c;font-family:Arial-BoldMT, Arial;font-weight:700">Surgical Control</text><text transform="translate(1235.38 803.06)" style="isolation:isolate;font-size:16px;font-family:Arial-BoldMT, Arial;font-weight:700">For critical systems:</text><text transform="translate(1235.38 828.06)" style="isolation:isolate;font-size:15px;font-family:ArialMT, Arial">• Disable specific heads</text><text transform="translate(1235.38 848.06)" style="isolation:isolate;font-size:15px;font-family:ArialMT, Arial">• Enforce attention patterns</text><text transform="translate(1235.38 868.06)" style="isolation:isolate;font-size:15px;font-family:ArialMT, Arial">• Guarantee behavior</text><rect x="1235.38" y="888.06" width="360" height="120" rx="5" style="fill:#ffcdd2;stroke:#d32f2f"/><text transform="translate(1344.09 913.06)" style="isolation:isolate;font-size:15px;font-family:Arial-BoldMT, Arial;font-weight:700">Example use cases:</text><text transform="translate(1245.38 938.06)" style="isolation:isolate;font-size:14px;font-family:ArialMT, Arial">• Medical<tspan x="57.03" y="0" style="letter-spacing:-0.05517578125em"> </tspan><tspan x="60.15" y="0">AI: Force attention</tspan></text><text transform="translate(1245.38 955.06)" style="isolation:isolate;font-size:14px;font-family:ArialMT, Arial">to symptoms over demographics</text><text transform="translate(1245.38 978.06)" style="isolation:isolate;font-size:14px;font-family:ArialMT, Arial">• Drone<tspan x="46.92" y="0" style="letter-spacing:-0.05517578125em"> </tspan><tspan x="50.04" y="0">AI: Guarantee attention</tspan></text><text transform="translate(1245.38 995.06)" style="isolation:isolate;font-size:14px;font-family:ArialMT, Arial">to safety constraints</text></svg>
            </section>

            <!----
            <section data-background-color="#ffffff">
                <h3>Performance Deep Dive: PyTorch vs Optimized C</h3>
                <p style="color: #333">Real performance comparison showing where the optimizations matter most.</p>
                <img src="organized_assets/images/attention_inference.svg" style="width: 95%; border: none; box-shadow: none;">
            </section>
        -->

            <section data-background-color="#ffffff" style="transform:scale(1.3);">
                <svg id="a6349396-1d22-44d9-9462-7bd8f0a0f07f" data-name="Layer 1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1600 761.53"><text transform="translate(393.77 40)" style="isolation:isolate;font-size:32px;fill:#e8e8e8;font-family:Arial-BoldMT, Arial;font-weight:700">Casual-Attention: From Math to<tspan x="478.19" y="0" style="letter-spacing:-0.037109375em"> </tspan><tspan x="485.89" y="0" style="letter-spacing:-0.07421875em">A</tspan><tspan x="506.63" y="0">VX-512 Implementation</tspan></text><text transform="translate(526.95 70)" style="isolation:isolate;font-size:18px;fill:#e8e8e8;font-family:ArialMT, Arial">How tokens learn to &quot;attend&quot; to each other through matrix operations</text><rect x="50" y="100" width="700" height="200" rx="10" style="fill:#f0f4f8;stroke:#4a90e2;stroke-width:2px"/><text transform="translate(294.23 130)" style="isolation:isolate;font-size:24px;fill:#4a90e2;font-family:Arial-BoldMT, Arial;font-weight:700">WH<tspan x="39.98" y="0" style="letter-spacing:-0.01806640625em">Y</tspan><tspan x="55.56" y="0" style="letter-spacing:-0.037109375em"> </tspan><tspan x="61.34" y="0" style="letter-spacing:-0.07421875em">A</tspan><tspan x="76.89" y="0">TTENTION?</tspan></text><text transform="translate(70 160)" style="isolation:isolate;font-size:16px;fill:#333;font-family:ArialMT, Arial">Problem: &quot;The cat sat on the mat because it was soft&quot;</text><text transform="translate(70 185)" style="isolation:isolate;font-size:16px;fill:#333;font-family:ArialMT, Arial">What does &quot;it&quot; refer to?<tspan x="164.32" y="0" style="letter-spacing:-0.01806640625em"> </tspan><tspan x="168.48" y="0">The cat or the mat?</tspan></text><rect x="70" y="210" width="60" height="30" rx="5" style="fill:#ffeb3b;stroke:#333"/><text transform="translate(87.94 230)" style="isolation:isolate;font-size:14px;font-family:ArialMT, Arial">The</text><rect x="140" y="210" width="60" height="30" rx="5" style="fill:#ffeb3b;stroke:#333"/><text transform="translate(160.66 230)" style="isolation:isolate;font-size:14px;font-family:ArialMT, Arial">cat</text><rect x="210" y="210" width="60" height="30" rx="5" style="fill:#ffeb3b;stroke:#333"/><text transform="translate(230.66 230)" style="isolation:isolate;font-size:14px;font-family:ArialMT, Arial">sat</text><rect x="280" y="210" width="60" height="30" rx="5" style="fill:#ffeb3b;stroke:#333"/><text transform="translate(302.21 230)" style="isolation:isolate;font-size:14px;font-family:ArialMT, Arial">on</text><rect x="350" y="210" width="60" height="30" rx="5" style="fill:#ffeb3b;stroke:#333"/><text transform="translate(370.27 230)" style="isolation:isolate;font-size:14px;font-family:ArialMT, Arial">the</text><rect x="420" y="210" width="60" height="30" rx="5" style="fill:#ff9800;stroke:#333"/><text transform="translate(438.33 230)" style="isolation:isolate;font-size:14px;font-family:ArialMT, Arial">mat</text><rect x="490" y="210" width="60" height="30" rx="5" style="fill:#ffeb3b;stroke:#333"/><text transform="translate(514.17 230)" style="isolation:isolate;font-size:14px;font-family:ArialMT, Arial">...</text><rect x="560" y="210" width="60" height="30" rx="5" style="fill:#4caf50;stroke:#333"/><text transform="translate(586.5 230)" style="isolation:isolate;font-size:14px;font-family:ArialMT, Arial">it</text><path d="M450,270l-20-10,20-10Z" style="fill:#4caf50"/><text transform="translate(510 275)" style="isolation:isolate;font-size:12px;fill:#4caf50;font-family:ArialMT, Arial">attention</text><rect x="850" y="100" width="700" height="200" rx="10" style="fill:#f8f0ff;stroke:#9c27b0;stroke-width:2px"/><text transform="translate(1070.65 130)" style="isolation:isolate;font-size:24px;fill:#9c27b0;font-family:Arial-BoldMT, Arial;font-weight:700">TENSOR DIMENSIONS</text><text transform="translate(870 160)" style="isolation:isolate;font-size:16px;fill:#333;font-family:ArialMT, Arial">Input: [batch_size, seq_len, embed_dim]</text><text transform="translate(870 185)" style="isolation:isolate;font-size:16px;fill:#333;font-family:ArialMT, Arial">Example: [1, 8, 512] → 8 tokens, 512-dimensional</text><rect x="870" y="210" width="80" height="60" style="fill:#e1bee7;stroke:#9c27b0"/><text transform="translate(885.48 245)" style="isolation:isolate;font-size:14px;font-family:ArialMT, Arial"><tspan style="letter-spacing:-0.11083984375em">T</tspan><tspan x="7" y="0">oken 0</tspan></text><text transform="translate(896.66 260)" style="isolation:isolate;font-size:12px;font-family:ArialMT, Arial">[512]</text><rect x="960" y="210" width="80" height="60" style="fill:#e1bee7;stroke:#9c27b0"/><text transform="translate(975.48 245)" style="isolation:isolate;font-size:14px;font-family:ArialMT, Arial"><tspan style="letter-spacing:-0.11083984375em">T</tspan><tspan x="7" y="0">oken 1</tspan></text><text transform="translate(986.66 260)" style="isolation:isolate;font-size:12px;font-family:ArialMT, Arial">[512]</text><text transform="translate(1050 240)" style="isolation:isolate;font-size:20px;font-family:ArialMT, Arial">...</text><rect x="1090" y="210" width="80" height="60" style="fill:#e1bee7;stroke:#9c27b0"/><text transform="translate(1105.48 245)" style="isolation:isolate;font-size:14px;font-family:ArialMT, Arial"><tspan style="letter-spacing:-0.11083984375em">T</tspan><tspan x="7" y="0">oken 7</tspan></text><text transform="translate(1116.66 260)" style="isolation:isolate;font-size:12px;font-family:ArialMT, Arial">[512]</text><text transform="translate(1200 240)" style="isolation:isolate;font-size:16px;fill:#333;font-family:ArialMT, Arial">After QKV projection:</text><text transform="translate(1200 265)" style="isolation:isolate;font-size:16px;fill:#333;font-family:ArialMT, Arial">Q, K, <tspan x="40.9" y="0" style="letter-spacing:-0.037109375em">V</tspan><tspan x="50.98" y="0">: [batch, heads, seq_len, head_dim]</tspan></text><text transform="translate(1200 290)" style="isolation:isolate;font-size:16px;fill:#333;font-family:ArialMT, Arial">Example: [1, 8, 8, 64] → 8 heads, 64 dims/head</text><rect x="50" y="350" width="1500" height="400" rx="10" style="fill:#fff8e1;stroke:#ff6f00;stroke-width:2px"/><text transform="translate(560.99 390)" style="isolation:isolate;font-size:28px;fill:#ff6f00;font-family:Arial-BoldMT, Arial;font-weight:700">THE M<tspan x="87.1" y="0" style="letter-spacing:-0.07421875em">A</tspan><tspan x="105.25" y="0">THEM</tspan><tspan x="184.57" y="0" style="letter-spacing:-0.07421875em">A</tspan><tspan x="202.71" y="0">TICS OF</tspan><tspan x="313.15" y="0" style="letter-spacing:-0.037109375em"> </tspan><tspan x="319.89" y="0" style="letter-spacing:-0.07421875em">A</tspan><tspan x="338.04" y="0">TTENTION</tspan></text><rect x="100" y="430" width="300" height="250" rx="5" style="fill:#fff3e0;stroke:#ff6f00"/><text transform="translate(140.54 455)" style="isolation:isolate;font-size:20px;fill:#ff6f00;font-family:Arial-BoldMT, Arial;font-weight:700">Step 1: QKV Projection</text><text transform="translate(110 485)" style="isolation:isolate;font-size:16px;font-family:CourierNewPSMT, Courier New">X → Linear transformations:</text><text transform="translate(110 510)" style="isolation:isolate;font-size:16px;font-family:CourierNewPSMT, Courier New">Q = XW_q [8×512]×[512×512]</text><text transform="translate(110 535)" style="isolation:isolate;font-size:16px;font-family:CourierNewPSMT, Courier New">K = XW_k [8×512]×[512×512]</text><text transform="translate(110 560)" style="isolation:isolate;font-size:16px;font-family:CourierNewPSMT, Courier New">V = XW_v [8×512]×[512×512]</text><text transform="translate(110 595)" style="isolation:isolate;font-size:14px;fill:#666;font-family:ArialMT, Arial"><tspan style="letter-spacing:-0.091796875em">Y</tspan><tspan x="8.05" y="0">our GEMM kernel shines here!</tspan></text><rect x="110" y="605" width="280" height="40" rx="5" style="fill:#e8f5e9;stroke:#4caf50"/><text transform="translate(115 625)" style="isolation:isolate;font-size:14px;fill:#2e7d32;font-family:CourierNewPSMT, Courier New">gemm_avx512_parallel(X, W_q, Q)</text><text transform="translate(115 640)" style="isolation:isolate;font-size:12px;fill:#2e7d32;font-family:ArialMT, Arial">→ 474 GFLOPS on your setup!</text><rect x="450" y="430" width="300" height="250" rx="5" style="fill:#fff3e0;stroke:#ff6f00"/><text transform="translate(484.24 455)" style="isolation:isolate;font-size:20px;fill:#ff6f00;font-family:Arial-BoldMT, Arial;font-weight:700">Step 2:<tspan x="66.68" y="0" style="letter-spacing:-0.037109375em"> </tspan><tspan x="71.49" y="0">Attention Scores</tspan></text><text transform="translate(460 485)" style="isolation:isolate;font-size:16px;font-family:CourierNewPSMT, Courier New">Scores = Q @ K^T / √d_k</text><text transform="translate(460 510)" style="isolation:isolate;font-size:16px;font-family:CourierNewPSMT, Courier New">[8×8×64] @ [8×64×8]</text><text transform="translate(460 535)" style="isolation:isolate;font-size:16px;font-family:CourierNewPSMT, Courier New">→ [8×8×8] per head</text><rect x="500" y="560" width="60" height="60" style="fill:#ffccbc;stroke:#ff6f00"/><text transform="translate(524.56 595)" style="isolation:isolate;font-size:14px;font-family:ArialMT, Arial">Q</text><text transform="translate(570 595)" style="isolation:isolate;font-size:20px;font-family:ArialMT, Arial">×</text><rect x="590" y="560" width="60" height="60" style="fill:#ffccbc;stroke:#ff6f00"/><text transform="translate(607.77 595)" style="isolation:isolate;font-size:14px;font-family:ArialMT, Arial">K^<tspan x="15.91" y="0" style="letter-spacing:-0.01806640625em">T</tspan></text><text transform="translate(660 595)" style="isolation:isolate;font-size:20px;font-family:ArialMT, Arial">=</text><rect x="680" y="560" width="60" height="60" style="fill:#ff8a65;stroke:#ff6f00"/><text transform="translate(691.33 595)" style="isolation:isolate;font-size:12px;font-family:ArialMT, Arial">Scores</text><text transform="translate(460 640)" style="isolation:isolate;font-size:14px;fill:#666;font-family:ArialMT, Arial">Each score[i,j] = &quot;How much</text><text transform="translate(460 658)" style="isolation:isolate;font-size:14px;fill:#666;font-family:ArialMT, Arial">should token i attend to token j?&quot;</text><rect x="800" y="430" width="300" height="250" rx="5" style="fill:#fff3e0;stroke:#ff6f00"/><text transform="translate(832.2 455)" style="isolation:isolate;font-size:20px;fill:#ff6f00;font-family:Arial-BoldMT, Arial;font-weight:700">Step 3: Softmax (Causal)</text><text transform="translate(810 485)" style="isolation:isolate;font-size:16px;font-family:CourierNewPSMT, Courier New">For each row i:</text><text transform="translate(810 510)" style="isolation:isolate;font-size:16px;font-family:CourierNewPSMT, Courier New">1. Mask future: S[i,j&gt;i] = -∞</text><text transform="translate(810 535)" style="isolation:isolate;font-size:16px;font-family:CourierNewPSMT, Courier New">2. exp(S[i,:] - max(S[i,:]))</text><text transform="translate(810 560)" style="isolation:isolate;font-size:16px;font-family:CourierNewPSMT, Courier New">3. Normalize: sum to 1.0</text><text transform="translate(850 575)" style="isolation:isolate;font-size:12px;fill:#666;font-family:ArialMT, Arial">Causal mask pattern:</text><rect x="850" y="580" width="15" height="15" style="fill:#4caf50"/><rect x="850" y="596" width="15" height="15" style="fill:#4caf50"/><rect x="866" y="596" width="15" height="15" style="fill:#4caf50"/><rect x="850" y="612" width="15" height="15" style="fill:#4caf50"/><rect x="866" y="612" width="15" height="15" style="fill:#4caf50"/><rect x="882" y="612" width="15" height="15" style="fill:#4caf50"/><rect x="850" y="628" width="15" height="15" style="fill:#4caf50"/><rect x="866" y="628" width="15" height="15" style="fill:#4caf50"/><rect x="882" y="628" width="15" height="15" style="fill:#4caf50"/><rect x="898" y="628" width="15" height="15" style="fill:#4caf50"/><rect x="866" y="580" width="15" height="15" style="fill:#f44336;isolation:isolate;opacity:0.30000001192092896"/><rect x="882" y="580" width="15" height="15" style="fill:#f44336;isolation:isolate;opacity:0.30000001192092896"/><rect x="898" y="580" width="15" height="15" style="fill:#f44336;isolation:isolate;opacity:0.30000001192092896"/><rect x="882" y="596" width="15" height="15" style="fill:#f44336;isolation:isolate;opacity:0.30000001192092896"/><rect x="898" y="596" width="15" height="15" style="fill:#f44336;isolation:isolate;opacity:0.30000001192092896"/><rect x="898" y="612" width="15" height="15" style="fill:#f44336;isolation:isolate;opacity:0.30000001192092896"/><text transform="translate(930 610)" style="isolation:isolate;font-size:12px;fill:#666;font-family:ArialMT, Arial">Green:<tspan x="36.69" y="0" style="letter-spacing:-0.05517578125em"> </tspan><tspan x="39.36" y="0">Allowed</tspan></text><text transform="translate(930 625)" style="isolation:isolate;font-size:12px;fill:#666;font-family:ArialMT, Arial">Red: Masked</text><rect x="1150" y="430" width="300" height="250" rx="5" style="fill:#fff3e0;stroke:#ff6f00"/><text transform="translate(1194.62 455)" style="isolation:isolate;font-size:20px;fill:#ff6f00;font-family:Arial-BoldMT, Arial;font-weight:700">Step 4: <tspan x="72.24" y="0" style="letter-spacing:-0.01806640625em">W</tspan><tspan x="90.75" y="0">eighted Sum</tspan></text><text transform="translate(1160 485)" style="isolation:isolate;font-size:16px;font-family:CourierNewPSMT, Courier New">Output = Softmax(Scores) @ V</text><text transform="translate(1160 510)" style="isolation:isolate;font-size:16px;font-family:CourierNewPSMT, Courier New">[8×8×8] @ [8×8×64]</text><text transform="translate(1160 535)" style="isolation:isolate;font-size:16px;font-family:CourierNewPSMT, Courier New">→ [8×8×64] per head</text><text transform="translate(1160 570)" style="isolation:isolate;font-size:14px;fill:#666;font-family:ArialMT, Arial">Each output token is now a</text><text transform="translate(1160 588)" style="isolation:isolate;font-size:14px;fill:#666;font-family:ArialMT, Arial">weighted combination of all</text><text transform="translate(1160 606)" style="isolation:isolate;font-size:14px;fill:#666;font-family:ArialMT, Arial">previous tokens&apos; values!</text><rect x="1160" y="620" width="280" height="40" rx="5" style="fill:#e8f5e9;stroke:#4caf50"/><text transform="translate(1165 640)" style="isolation:isolate;font-size:14px;fill:#2e7d32;font-family:CourierNewPSMT, Courier New">gemm_avx512(Attn, V, Output)</text><text transform="translate(1165 655)" style="isolation:isolate;font-size:12px;fill:#2e7d32;font-family:ArialMT, Arial">Reuse your optimized kernel!</text></svg>
            </section>
            <section data-background-color="#ffffff" style="transform:scale(1.3);">
                <svg id="b162ab73-9a35-40d9-8ff5-855c23790d64" data-name="Layer 1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1600 835.4"><rect x="50" y="18.04" width="700" height="350" rx="10" style="fill:#e8f5e9;stroke:#4caf50;stroke-width:2px"/><text transform="translate(162.42 58.04)" style="isolation:isolate;font-size:24px;fill:#4caf50;font-family:Arial-BoldMT, Arial;font-weight:700"><tspan style="letter-spacing:-0.07421875em">A</tspan><tspan x="15.55" y="0">VX-512 OPTIMIZ</tspan><tspan x="199.59" y="0" style="letter-spacing:-0.07421875em">A</tspan><tspan x="215.14" y="0">TION OPPORTUNITIES</tspan></text><text transform="translate(70 98.04)" style="isolation:isolate;font-size:18px;fill:#2e7d32;font-family:Arial-BoldMT, Arial;font-weight:700">1. Q @ K^T Computation:</text><path d="M75,108H725c2.76,0,5,2.75,5,6.14v86c0,3.39-2.24,6.14-5,6.14H75c-2.76,0-5-2.75-5-6.14v-86C70,110.79,72.24,108,75,108Z" style="fill:#f1f8e9;stroke:#689f38"/><text transform="translate(80 128.04)" style="isolation:isolate;font-size:14px;font-family:CourierNewPSMT, Courier New">// Process 16 elements at once with AVX-512</text><text transform="translate(80 148.04)" style="isolation:isolate;font-size:14px;font-family:CourierNewPSMT, Courier New">__m512 q_vec = _mm512_load_ps(&amp;Q[i*head_dim + d]);</text><text transform="translate(80 168.04)" style="isolation:isolate;font-size:14px;font-family:CourierNewPSMT, Courier New">__m512 k_vec = _mm512_load_ps(&amp;K[j*head_dim + d]);</text><text transform="translate(80 188.04)" style="isolation:isolate;font-size:14px;font-family:CourierNewPSMT, Courier New">acc = _mm512_fmadd_ps(q_vec, k_vec, acc); // 32 FLOPS!</text><text transform="translate(70 228.04)" style="isolation:isolate;font-size:18px;fill:#2e7d32;font-family:Arial-BoldMT, Arial;font-weight:700">2. <tspan x="20.01" y="0" style="letter-spacing:-0.05517578125em">V</tspan><tspan x="31.03" y="0">ectorized Softmax:</tspan></text><path d="M75,238H725c2.76,0,5,2.72,5,6.07v60.65c0,3.35-2.24,6.07-5,6.07H75c-2.76,0-5-2.72-5-6.07V244.11C70,240.76,72.24,238,75,238Z" style="fill:#f1f8e9;stroke:#689f38"/><text transform="translate(80 258.04)" style="isolation:isolate;font-size:14px;font-family:CourierNewPSMT, Courier New">// Find max across 16 values simultaneously</text><text transform="translate(80 278.04)" style="isolation:isolate;font-size:14px;font-family:CourierNewPSMT, Courier New">__m512 max_vec = _mm512_max_ps(scores_vec, max_vec);</text><text transform="translate(80 298.04)" style="isolation:isolate;font-size:14px;font-family:CourierNewPSMT, Courier New">// Vectorized exp: huge speedup over scalar</text><text transform="translate(70 338.04)" style="isolation:isolate;font-size:16px;fill:#333;font-family:ArialMT, Arial">Expected speedup: 8-16x over scalar code!</text><rect x="850" y="18.04" width="700" height="350" rx="10" style="fill:#e3f2fd;stroke:#2196f3;stroke-width:2px"/><text transform="translate(955.31 58.04)" style="isolation:isolate;font-size:24px;fill:#2196f3;font-family:Arial-BoldMT, Arial;font-weight:700">HEAD-MAJOR VS <tspan x="208.02" y="0" style="letter-spacing:-0.01806640625em">T</tspan><tspan x="222.25" y="0">OKEN-MAJOR L</tspan><tspan x="407.58" y="0" style="letter-spacing:-0.091796875em">A</tspan><tspan x="422.71" y="0">YOUT</tspan></text><text transform="translate(870 98.04)" style="isolation:isolate;font-size:18px;fill:#1976d2;font-family:Arial-BoldMT, Arial;font-weight:700"><tspan style="letter-spacing:-0.07421875em">T</tspan><tspan x="9.66" y="0">oken-Major (</tspan><tspan x="116.67" y="0" style="letter-spacing:-0.05517578125em">T</tspan><tspan x="126.67" y="0">raditional):</tspan></text><rect x="870" y="108.04" width="300" height="40" style="fill:#bbdefb;stroke:#1976d2"/><text transform="translate(880 133.04)" style="isolation:isolate;font-size:14px;font-family:CourierNewPSMT, Courier New">[Token0: H0|H1|H2|...|H7]</text><text transform="translate(880 168.04)" style="isolation:isolate;font-size:14px;fill:#666;font-family:ArialMT, Arial">Problem: Heads scattered in memory</text><text transform="translate(880 186.04)" style="isolation:isolate;font-size:14px;fill:#666;font-family:ArialMT, Arial">Cache misses when computing attention</text><text transform="translate(1220 98.04)" style="isolation:isolate;font-size:18px;fill:#1976d2;font-family:Arial-BoldMT, Arial;font-weight:700">Head-Major (Optimized):</text><rect x="1220" y="108.04" width="300" height="40" style="fill:#90caf9;stroke:#1976d2"/><text transform="translate(1230 133.04)" style="isolation:isolate;font-size:14px;font-family:CourierNewPSMT, Courier New">[Head0: T0|T1|T2|...|T7]</text><text transform="translate(1230 168.04)" style="isolation:isolate;font-size:14px;fill:#666;font-family:ArialMT, Arial">Benefit:<tspan x="47.48" y="0" style="letter-spacing:-0.05517578125em"> </tspan><tspan x="50.59" y="0">All tokens for a head are contiguous</tspan></text><text transform="translate(1230 186.04)" style="isolation:isolate;font-size:14px;fill:#666;font-family:ArialMT, Arial">Perfect cache utilization!</text><rect x="870" y="218.04" width="660" height="120" rx="5" style="fill:#e1f5fe;stroke:#0288d1"/><text transform="translate(880 243.04)" style="isolation:isolate;font-size:16px;font-family:Arial-BoldMT, Arial;font-weight:700">Performance Impact:</text><text transform="translate(880 268.04)" style="isolation:isolate;font-size:14px;font-family:ArialMT, Arial">• QK^<tspan x="35.59" y="0" style="letter-spacing:-0.01806640625em">T</tspan><tspan x="43.89" y="0" xml:space="preserve"> computation: 2-3x faster (sequential access)</tspan></text><text transform="translate(880 288.04)" style="isolation:isolate;font-size:14px;font-family:ArialMT, Arial">• Softmax: 1.5x faster (row-wise operations)</text><text transform="translate(880 308.04)" style="isolation:isolate;font-size:14px;font-family:ArialMT, Arial">• Overall attention: 2-5x speedup</text><text transform="translate(880 328.04)" style="isolation:isolate;font-size:14px;font-family:ArialMT, Arial">•<tspan x="4.9" y="0" style="letter-spacing:-0.01806640625em"> </tspan><tspan x="8.54" y="0" style="letter-spacing:-0.091796875em">Y</tspan><tspan x="16.59" y="0">our 72 cores can process heads in parallel!</tspan></text><rect x="50" y="418.04" width="1500" height="400" rx="10" style="fill:#f3e5f5;stroke:#7b1fa2;stroke-width:2px"/><text transform="translate(466.88 458.04)" style="isolation:isolate;font-size:28px;fill:#7b1fa2;font-family:Arial-BoldMT, Arial;font-weight:700">COMPLETE<tspan x="155.56" y="0" style="letter-spacing:-0.037109375em"> </tspan><tspan x="162.3" y="0" style="letter-spacing:-0.07421875em">A</tspan><tspan x="180.44" y="0">TTENTION IMPLEMEN</tspan><tspan x="475.99" y="0" style="letter-spacing:-0.07421875em">TA</tspan><tspan x="509.15" y="0">TION FLOW</tspan></text><path d="M155,498H345a5.29,5.29,0,0,1,5,5.53v77.37a5.29,5.29,0,0,1-5,5.53H155a5.29,5.29,0,0,1-5-5.53V503.57A5.29,5.29,0,0,1,155,498Z" style="fill:#e1bee7;stroke:#7b1fa2"/><text transform="translate(200.82 528.04)" style="isolation:isolate;font-size:16px;font-family:Arial-BoldMT, Arial;font-weight:700">Input <tspan x="43.54" y="0" style="letter-spacing:-0.07421875em">T</tspan><tspan x="52.13" y="0">okens</tspan></text><text transform="translate(188.52 548.04)" style="isolation:isolate;font-size:14px;font-family:ArialMT, Arial">[batch, seq, embed]</text><text transform="translate(209.97 566.04)" style="isolation:isolate;font-size:12px;fill:#666;font-family:ArialMT, Arial">e.g., [1, 8, 512]</text><path d="M380,528l20,10-20,10Z" style="fill:#7b1fa2"/><path d="M405,498H595a5.29,5.29,0,0,1,5,5.53v77.37a5.29,5.29,0,0,1-5,5.53H405a5.29,5.29,0,0,1-5-5.53V503.57A5.29,5.29,0,0,1,405,498Z" style="fill:#ce93d8;stroke:#7b1fa2"/><text transform="translate(441.32 523.04)" style="isolation:isolate;font-size:16px;font-family:Arial-BoldMT, Arial;font-weight:700">QKV Projection</text><text transform="translate(453.19 543.04)" style="isolation:isolate;font-size:12px;font-family:CourierNewPSMT, Courier New">gemm_avx512()</text><text transform="translate(459.82 560.04)" style="isolation:isolate;font-size:12px;fill:#666;font-family:ArialMT, Arial">3 × GEMM ops</text><text transform="translate(462.32 576.04)" style="isolation:isolate;font-size:12px;fill:#4caf50;font-family:ArialMT, Arial">474 GFLOPS!</text><path d="M630,528l20,10-20,10Z" style="fill:#7b1fa2"/><path d="M655,498H845a5.29,5.29,0,0,1,5,5.53v77.37a5.29,5.29,0,0,1-5,5.53H655a5.29,5.29,0,0,1-5-5.53V503.57A5.29,5.29,0,0,1,655,498Z" style="fill:#ba68c8;stroke:#7b1fa2"/><text transform="translate(680.64 523.04)" style="isolation:isolate;font-size:16px;font-family:Arial-BoldMT, Arial;font-weight:700">Reshape to Heads</text><text transform="translate(693.64 543.04)" style="isolation:isolate;font-size:12px;font-family:ArialMT, Arial"><tspan style="letter-spacing:-0.11083984375em">T</tspan><tspan x="6" y="0">oken → Head major</tspan></text><text transform="translate(719.98 560.04)" style="isolation:isolate;font-size:12px;fill:#666;font-family:ArialMT, Arial">[1, 8, 8, 64]</text><text transform="translate(709.98 576.04)" style="isolation:isolate;font-size:12px;fill:#1e701e;font-family:ArialMT, Arial">Cache optimal!</text><path d="M880,528l20,10-20,10Z" style="fill:#7b1fa2"/><path d="M905,498h190a5.29,5.29,0,0,1,5,5.53v77.37a5.29,5.29,0,0,1-5,5.53H905a5.29,5.29,0,0,1-5-5.53V503.57A5.29,5.29,0,0,1,905,498Z" style="fill:#ab47bc;stroke:#7b1fa2"/><text transform="translate(941.55 518.04)" style="isolation:isolate;font-size:16px;font-family:Arial-BoldMT, Arial;font-weight:700">Compute QK^T</text><text transform="translate(953.19 536.04)" style="isolation:isolate;font-size:12px;font-family:CourierNewPSMT, Courier New">gemm_avx512()</text><text transform="translate(954.64 552.04)" style="isolation:isolate;font-size:12px;fill:#666;font-family:ArialMT, Arial">Per head parallel</text><text transform="translate(962.35 568.04)" style="isolation:isolate;font-size:12px;fill:#4caf50;font-family:ArialMT, Arial">Scale by √d_k</text><path d="M1130,528l20,10-20,10Z" style="fill:#7b1fa2"/><path d="M1155,498h190a5.29,5.29,0,0,1,5,5.53v77.37a5.29,5.29,0,0,1-5,5.53H1155a5.29,5.29,0,0,1-5-5.53V503.57A5.29,5.29,0,0,1,1155,498Z" style="fill:#9c27b0;stroke:#7b1fa2"/><text transform="translate(1189.98 523.04)" style="isolation:isolate;font-size:16px;font-family:Arial-BoldMT, Arial;font-weight:700">Causal Softmax</text><text transform="translate(1215.09 543.04)" style="isolation:isolate;font-size:12px;font-family:ArialMT, Arial"><tspan style="letter-spacing:-0.07421875em">A</tspan><tspan x="7.11" y="0">VX-512 exp</tspan></text><text transform="translate(1208.66 560.04)" style="isolation:isolate;font-size:12px;fill:#666;font-family:ArialMT, Arial">Row-wise norm</text><text transform="translate(1218.66 576.04)" style="isolation:isolate;font-size:12px;fill:#4caf50;font-family:ArialMT, Arial">Mask future</text><path d="M405,618H595a5.27,5.27,0,0,1,5,5.5v76.92a5.27,5.27,0,0,1-5,5.5H405a5.27,5.27,0,0,1-5-5.5V623.54A5.27,5.27,0,0,1,405,618Z" style="fill:#8e24aa;stroke:#7b1fa2"/><text transform="translate(450.45 643.04)" style="isolation:isolate;font-size:16px;font-family:Arial-BoldMT, Arial;font-weight:700">Attention × V</text><text transform="translate(453.19 663.04)" style="isolation:isolate;font-size:12px;font-family:CourierNewPSMT, Courier New">gemm_avx512()</text><text transform="translate(461.76 680.04)" style="isolation:isolate;font-size:12px;fill:#666;font-family:ArialMT, Arial"><tspan style="letter-spacing:-0.01806640625em">W</tspan><tspan x="11.11" y="0">eighted sum</tspan></text><text transform="translate(462.98 696.04)" style="isolation:isolate;font-size:12px;fill:#4caf50;font-family:ArialMT, Arial">Reuse kernel!</text><path d="M630,648l20,10-20,10Z" style="fill:#7b1fa2"/><path d="M655,618H845a5.27,5.27,0,0,1,5,5.5v76.92a5.27,5.27,0,0,1-5,5.5H655a5.27,5.27,0,0,1-5-5.5V623.54A5.27,5.27,0,0,1,655,618Z" style="fill:#7b1fa2;stroke:#7b1fa2"/><text transform="translate(696.65 643.04)" style="isolation:isolate;font-size:16px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">Concat Heads</text><text transform="translate(693.75 663.04)" style="isolation:isolate;font-size:12px;fill:#fff;font-family:ArialMT, Arial">Head →<tspan x="44.02" y="0" style="letter-spacing:-0.01806640625em"> </tspan><tspan x="47.14" y="0" style="letter-spacing:-0.11083984375em">T</tspan><tspan x="53.14" y="0">oken major</tspan></text><text transform="translate(723.31 680.04)" style="isolation:isolate;font-size:12px;fill:#e1bee7;font-family:ArialMT, Arial">[1, 8, 512]</text><text transform="translate(696.31 696.04)" style="isolation:isolate;font-size:12px;fill:#81c784;font-family:ArialMT, Arial">Ready for next layer</text><path d="M880,648l20,10-20,10Z" style="fill:#7b1fa2"/><path d="M905,618h190a5.27,5.27,0,0,1,5,5.5v76.92a5.27,5.27,0,0,1-5,5.5H905a5.27,5.27,0,0,1-5-5.5V623.54A5.27,5.27,0,0,1,905,618Z" style="fill:#6a1b9a;stroke:#7b1fa2"/><text transform="translate(932.45 643.04)" style="isolation:isolate;font-size:16px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">Output Projection</text><text transform="translate(953.19 663.04)" style="isolation:isolate;font-size:12px;fill:#fff;font-family:CourierNewPSMT, Courier New">gemm_avx512()</text><text transform="translate(959.99 680.04)" style="isolation:isolate;font-size:12px;fill:#e1bee7;font-family:ArialMT, Arial">Final transform</text><text transform="translate(959.32 696.04)" style="isolation:isolate;font-size:12px;fill:#81c784;font-family:ArialMT, Arial">Context-aware!</text><path d="M510,598l-10,20-10-20Z" style="fill:#7b1fa2"/><rect x="150" y="738.04" width="1300" height="60" rx="5" style="fill:#f3e5f5;stroke:#7b1fa2"/><text transform="translate(504.86 763.04)" style="isolation:isolate;font-size:18px;fill:#7b1fa2;font-family:Arial-BoldMT, Arial;font-weight:700"><tspan style="letter-spacing:-0.07421875em">T</tspan><tspan x="9.66" y="0">otal Operations: 4 GEMMs + 1 Softmax = ~90% of attention compute</tspan></text><text transform="translate(554.14 786.04)" style="isolation:isolate;font-size:16px;fill:#666;font-family:ArialMT, Arial"><tspan style="letter-spacing:-0.091796875em">Y</tspan><tspan x="9.2" y="0">our optimized GEMM kernel is reused 4 times - maximum e</tspan><tspan x="429.77" y="0" style="letter-spacing:-0.01806640625em">f</tspan><tspan x="433.93" y="0">ficiency!</tspan></text></svg>
            </section>
            <!-- Interactive Attention Pipeline -->
            <section data-background-iframe="organized_assets/infographics/attention_comprehensive.html" data-background-interactive>
<aside class="notes">
SLIDE 32: Interactive Attention Comprehensive Visualization
This comprehensive interactive visualization brings together everything we've learned. You can see the complete attention mechanism in action, with all the optimizations we've discussed. The visualization shows data flow, memory layouts, and performance characteristics all in one place.
</aside>
            </section>

            <!-- Final Performance Summary 
            <section>
                <h3>Final Performance Summary</h3>
                <p>Production-ready results on modern hardware</p>
                <div class="journey-progress">
                    <div class="progress-step active">Theory</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step active">Intuition</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step active">Architecture</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step active">HPC</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step active">Production</div>
                </div>
                <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 30px; margin-top: 40px;">
                    <div class="perf-metric">
                        <div class="perf-number">400+</div>
                        <div>GFLOPS</div>
                        <div style="font-size: 0.7em; margin-top: 5px;">Sustained on Intel Xeon</div>
                    </div>
                    <div class="perf-metric">
                        <div class="perf-number">10-50x</div>
                        <div>Speedup</div>
                        <div style="font-size: 0.7em; margin-top: 5px;">vs PyTorch CPU</div>
                    </div>
                    <div class="perf-metric">
                        <div class="perf-number">~2ms</div>
                        <div>Per Layer</div>
                        <div style="font-size: 0.7em; margin-top: 5px;">Complete attention</div>
                    </div>
                    <div class="perf-metric">
                        <div class="perf-number">96%+</div>
                        <div>Cache Efficiency</div>
                        <div style="font-size: 0.7em; margin-top: 5px;">L3 cache utilization</div>
                    </div>
                    <div class="perf-metric">
                        <div class="perf-number">0</div>
                        <div>Memory Frag</div>
                        <div style="font-size: 0.7em; margin-top: 5px;">Single allocation</div>
                    </div>
                    <div class="perf-metric">
                        <div class="perf-number">64B</div>
                        <div>Alignment</div>
                        <div style="font-size: 0.7em; margin-top: 5px;">Perfect cache lines</div>
                    </div>
                </div>
                <div style="margin-top: 40px; font-size: 1.1em; text-align: center;">
                    <div style="color: #ff6f00; font-weight: bold; margin-bottom: 15px;">
                        Making AI inference feasible everywhere:
                    </div>
                    <div style="display: flex; justify-content: center; gap: 40px; font-size: 0.9em;">
                        <div>🤖 Edge Devices</div>
                        <div>🦾 Real-time Robotics</div>
                        <div>🏥 Safety-critical Systems</div>
                        <div>☁️ Cost-effective Cloud</div>
                    </div>
                </div>
            </section>
        -->
            <!-- Conclusion: The Complete Journey -->
            <section>
<aside class="notes">
SLIDE 34: Conclusion - The Complete Journey
We've traveled from fundamental mathematics to production-grade high-performance code. Our mathematical understanding includes scaled dot-product attention, multi-head processing, causal masking, and FLOP analysis. Our conceptual intuition covers head-parallel strategy, memory layout transitions, cache optimization, and AVX-512 vectorization. Our memory architecture uses single allocation strategy, head-major layout, precise offset control, and canary protection. Our HPC and production implementation achieves 400+ GFLOPS performance, 96% L3 cache hit rate, production-ready C code, and full transformer integration. This is the complete transformation from theory to world-class implementation.
</aside>
                <h2>Conclusion: The Complete Journey</h2>
                <p>We've traveled from fundamental mathematics to production-grade high-performance code:</p>
                <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 30px; margin-top: 30px;">
                    <div>
                        <h4>🧮 Mathematical Understanding</h4>
                        <ul style="font-size: 0.8em; text-align: left;">
                            <li>Scaled dot-product attention formula</li>
                            <li>Multi-head parallel processing</li>
                            <li>Causal masking for autoregressive models</li>
                            <li>FLOP analysis and computational complexity</li>
                        </ul>
                    </div>
                    <div>
                        <h4>🧠 Conceptual Intuition</h4>
                        <ul style="font-size: 0.8em; text-align: left;">
                            <li>Why attention enables parallelization</li>
                            <li>Head specialization and emergent behaviors</li>
                            <li>Memory access pattern implications</li>
                            <li>Cache locality and performance trade-offs</li>
                        </ul>
                    </div>
                    <div>
                        <h4>🏗️ Memory Architecture</h4>
                        <ul style="font-size: 0.8em; text-align: left;">
                            <li>Head-major memory layout design</li>
                            <li>Cache-aligned data structures</li>
                            <li>Zero-fragmentation memory allocation</li>
                            <li>Four-phase data flow optimization</li>
                        </ul>
                    </div>
                    <div>
                        <h4>⚡ HPC & Production</h4>
                        <ul style="font-size: 0.8em; text-align: left;">
                            <li>AVX-512 vectorization optimization</li>
                            <li>Head-parallel computation strategy</li>
                            <li>Real performance measurements</li>
                            <li>Future optimization directions</li>
                        </ul>
                    </div>
                </div>
                <div style="margin-top: 40px; text-align: center;">
                    <div class="journey-progress">
                        <div class="progress-step active">Theory</div>
                        <div class="progress-arrow">→</div>
                        <div class="progress-step active">Intuition</div>
                        <div class="progress-arrow">→</div>
                        <div class="progress-step active">Architecture</div>
                        <div class="progress-arrow">→</div>
                        <div class="progress-step active">HPC</div>
                        <div class="progress-arrow">→</div>
                        <div class="progress-step active">Production</div>
                    </div>
                    <div style="font-size: 1.3em; color: #ff6f00; font-weight: bold; margin-top: 20px;">
                        From $QK^T$ to 400+ GFLOPS: The Complete Journey
                    </div>
                </div>
            </section>

            <!-- Thank You -->
            <section>
                <h2>Thank You</h2>
                <h3>Questions & Discussion</h3>
                <div style="margin-top: 50px; font-size: 0.9em;">
                    <p>This presentation showcases the complete journey from mathematical foundations to production-grade high-performance attention mechanisms.</p>
                    <div style="margin-top: 30px; display: flex; justify-content: center; gap: 50px;">
                        <div style="text-align: center;">
                            <div style="font-size: 2em;">🧮</div>
                            <div>Mathematical Rigor</div>
                        </div>
                        <div style="text-align: center;">
                            <div style="font-size: 2em;">🧠</div>
                            <div>Conceptual Clarity</div>
                        </div>
                        <div style="text-align: center;">
                            <div style="font-size: 2em;">🏗️</div>
                            <div>Memory Architecture</div>
                        </div>
                        <div style="text-align: center;">
                            <div style="font-size: 2em;">⚡</div>
                            <div>HPC Performance</div>
                        </div>
                        <div style="text-align: center;">
                            <div style="font-size: 2em;">🚀</div>
                            <div>Production Ready</div>
                        </div>
                    </div>
                    <p style="margin-top: 30px; font-style: italic;">Ready to deploy high-performance AI inference anywhere.</p>
                </div>
            </section>

        </div>
    </div>

  <script src="../reveal.js/dist/reveal.js"></script>
  <script src="../reveal.js/plugin/zoom/zoom.js"></script>
  <script src="../reveal.js/plugin/notes/notes.js"></script>
  <script src="../reveal.js/plugin/search/search.js"></script>
  <script src="../reveal.js/plugin/markdown/markdown.js"></script>
  <script src="../reveal.js/plugin/highlight/highlight.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.3.1/plugin/math/math.min.js"></script>
  <script>
        document.addEventListener('DOMContentLoaded', function() {
            Reveal.initialize({
                controls: true,
                progress: true,
                center: true,
                hash: true,
                transition: 'slide',
                plugins: [RevealZoom, RevealHighlight, RevealMarkdown, RevealNotes, RevealMath.KaTeX ]
            });

            const HEAD_COLORS = ['#e57373', '#81c784', '#64b5f6', '#ffd54f', '#ba68c8', '#ff8a65', '#a1887f', '#90a4ae'];

            function createGrid(container, rows, cols, colorFn, isCausal) {
                if (!container) return;
                container.innerHTML = '';
                container.style.gridTemplateColumns = `repeat(${cols}, 15px)`;
                container.style.gridTemplateRows = `repeat(${rows}, 15px)`;
                for (let i = 0; i < rows; i++) {
                    for (let j = 0; j < cols; j++) {
                        const cell = document.createElement('div');
                        cell.classList.add('heatmap-cell');
                        cell.style.backgroundColor = colorFn(i, j);
                        if (isCausal && j > i) { cell.style.opacity = 0.1; }
                        else { cell.style.opacity = Math.random() * 0.6 + 0.3; }
                        container.appendChild(cell);
                    }
                }
            }

            function setupVizSlides() {
                // Reorganization Viz
                if (document.getElementById('viz-memory-reorg')) {
                    createGrid(document.getElementById('reorg-q-grid'), 8, 16, (i, j) => HEAD_COLORS[j % 8]);
                    createGrid(document.getElementById('reorg-heads-grid'), 8, 16, (i, j) => HEAD_COLORS[Math.floor(j/4)]);
                }

                // Scores Viz
                if (document.getElementById('viz-scores')) {
                    createGrid(document.getElementById('score-q-grid'), 8, 2, () => HEAD_COLORS[0]);
                    createGrid(document.getElementById('score-k-grid'), 2, 8, () => HEAD_COLORS[0]);
                    createGrid(document.getElementById('score-res-grid'), 8, 8, () => '#ff8a65', true);
                }

                // Logical vs Physical Viz
                const logicalContainer = document.getElementById('logical-heads');
                const physicalBar = document.getElementById('physical-bar');
                if (document.getElementById('viz-logical-physical') && logicalContainer && physicalBar) {
                    logicalContainer.innerHTML = '';
                    physicalBar.innerHTML = '';

                    for (let h = 0; h < 8; h++) {
                        const headContainer = document.createElement('div');
                        const headGrid = document.createElement('div');
                        headGrid.classList.add('heatmap-grid');
                        const legend = document.createElement('div');
                        legend.classList.add('legend');
                        legend.innerText = `Head ${h}`;
                        headContainer.appendChild(headGrid);
                        headContainer.appendChild(legend);
                        logicalContainer.appendChild(headContainer);
                        createGrid(headGrid, 8, 2, () => HEAD_COLORS[h]);

                        const memBlock = document.createElement('div');
                        memBlock.classList.add('mem-block');
                        memBlock.style.width = '12.5%';
                        memBlock.style.backgroundColor = HEAD_COLORS[h];
                        memBlock.id = `physical-block-${h}`;
                        physicalBar.appendChild(memBlock);

                        headContainer.addEventListener('mouseover', () => { 
                            document.getElementById(`physical-block-${h}`).classList.add('highlight-box');
                        });
                        headContainer.addEventListener('mouseout', () => { 
                            document.getElementById(`physical-block-${h}`).classList.remove('highlight-box');
                        });
                    }
                }
            }

            Reveal.on('ready', event => { setupVizSlides(); });
            Reveal.on('slidechanged', event => { setupVizSlides(); });
        });
    </script>
</body>
</html>