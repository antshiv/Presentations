<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title>Attention Is All You Need - Draft 4</title>

    <link rel="stylesheet" href="../reveal.js/dist/reset.css">
    <link rel="stylesheet" href="../reveal.js/dist/reveal.css">
    <link rel="stylesheet" href="../reveal.js/dist/theme/black.css" id="theme">

    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="../reveal.js/plugin/highlight/monokai.css">
    <link rel="stylesheet" href="../css/styles.css">
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/3.9.1/gsap.min.js"></script>

    <style>
        :root { --r-main-font-size: 24px; }
        .reveal .slides section { font-size: 0.9em; }
        .reveal h1, .reveal h2, .reveal h3, .reveal h4 { text-transform: none; }
        .formula-box { background-color: rgba(45, 51, 59, 0.8); border-radius: 15px; padding: 15px; margin-top: 15px; border: 1px solid #484f58; }
        .dim-table { margin: 15px auto; font-size: 0.75em; border-collapse: collapse; }
        .dim-table th, .dim-table td { border: 1px solid #484f58; padding: 8px 15px; }
        .transformer-block { display: flex; flex-direction: column; align-items: center; gap: 10px; }
        .block-component { border: 2px solid #484f58; border-radius: 10px; padding: 10px 20px; width: 350px; text-align: center; background-color: #2d333b; }
        .block-component.highlight { border-color: #ff6f00; background-color: #4d3c20; box-shadow: 0 0 15px #ff6f00; }
        .arrow-down { width: 0; height: 0; border-left: 15px solid transparent; border-right: 15px solid transparent; border-top: 20px solid #484f58; }

        /* Visualization & Memory Styles */
        .viz-container { display: flex; justify-content: center; align-items: center; gap: 20px; position: relative; min-height: 350px;}
        .grid-container { position: relative; }
        .heatmap-grid { display: grid; gap: 1px; border: 1px solid #666;}
        .heatmap-cell { background-color: #4a90e2; }
        .legend { position: absolute; color: #ccc; font-size: 0.6em; }
        .legend-y { writing-mode: vertical-rl; transform: rotate(180deg); left: -30px; top: 50%; transform-origin: center; }
        .legend-x { top: -25px; left: 50%; transform: translateX(-50%); }
        .op-label { font-size: 1.8em; color: #ccc; }
        .heads-container { display: grid; grid-template-columns: repeat(4, 1fr); gap: 10px; }
        .memory-viz { display: flex; justify-content: space-around; width: 100%; }
        .memory-layout { width: 45%; background-color: #1e1e1e; padding: 15px; border-radius: 10px; border: 1px solid #444;}
        .memory-bar { display: flex; flex-wrap: wrap; border: 2px solid #888; background: #111; padding: 2px; border-radius: 5px;}
        .mem-block { width: 12.5%; height: 20px; box-sizing: border-box; border: 1px solid #333;}
        .head-color-0 { background-color: #e57373; } .head-color-1 { background-color: #81c784; } .head-color-2 { background-color: #64b5f6; } .head-color-3 { background-color: #ffd54f; }
        .head-color-4 { background-color: #ba68c8; } .head-color-5 { background-color: #ff8a65; } .head-color-6 { background-color: #a1887f; } .head-color-7 { background-color: #90a4ae; }
        .code-block-small { font-size: 0.6em !important; }
    </style>
</head>
<body>
    <div class="reveal">
        <div class="slides">
            <section>
                <h2>Attention Is All You Need</h2>
                <h3>From Theory to AVX-512</h3>
                <p>A Deep Dive into High-Performance Transformer Implementation</p>
                <p><small>Created by bashbash</small></p>
            </section>

            <section>
                <h3>Our Focus: The HPC of Attention</h3>
                <p>We are focusing on the **Multi-Head Attention** mechanism, not as a theoretical concept, but as a high-performance computing challenge.</p>
                <div class="transformer-block">
                    <div class="block-component">Input (from LayerNorm)</div>
                    <div class="arrow-down"></div>
                    <div class="block-component highlight">
                        <strong>Causal Multi-Head Attention</strong>
                        <small>This is where 90% of the compute lies.</small>
                    </div>
                    <div class="arrow-down"></div>
                    <div class="block-component">Add & Norm</div>
                    <div class="arrow-down"></div>
                    <div class="block-component">Feed-Forward Network</div>
                    <div class="arrow-down"></div>
                    <div class="block-component">...</div>
                </div>
            </section>

            <section id="viz-memory-struct">
                <h3>The C-Level Memory Architecture</h3>
                <p>Performance comes from design. We use a single, huge-page-aligned memory block, planned with C structs for perfect layout control.</p>
                <div style="display: flex; justify-content: space-around; align-items: flex-start;">
                    <div style="width: 45%;">
                        <pre><code class="c code-block-small" data-trim>
// A single contiguous block for the whole model
struct TransformerModel {
    // ... hyper-parameters ...
    float *memory_base;
    size_t total_floats;
    TrulyOptimalLayer *layers;
    // ... offsets ...
};

// Defines the memory map for one layer
struct TrulyOptimalLayer {
    size_t q_weight_offset, q_bias_offset;
    size_t k_weight_offset, k_bias_offset;
    size_t v_weight_offset, v_bias_offset;

    // Buffers for this layer's activations
    size_t q_output_offset;
    size_t k_output_offset;
    size_t v_output_offset;
    size_t attention_scores_offset;
    // ... other offsets ...
};
                        </code></pre>
                    </div>
                    <div style="width: 50%;">
                        <h4>Visual Memory Map</h4>
                        <div style="font-family: monospace; font-size: 0.8em; background: #222; padding: 10px; border-radius: 5px;">
                            [Token & Pos Embeddings]<br>
                            [Layer 0]<br>
                            &nbsp;&nbsp;[LN1 Weights]<br>
                            &nbsp;&nbsp;[Attn Weights (Q,K,V,Proj)]<br>
                            &nbsp;&nbsp;<strong style="color:#ff6f00">[QKV Buffers (Head-Major)]</strong><br>
                            &nbsp;&nbsp;<strong style="color:#ff6f00">[Attn Scores Buffer (Head-Major)]</strong><br>
                            &nbsp;&nbsp;[MLP Weights...]<br>
                            [Layer 1]<br>
                            &nbsp;&nbsp;...<br>
                            [Final LayerNorm]
                        </div>
                        <p class="fragment"><small>This layout, defined in C, gives us absolute control to eliminate fragmentation and optimize for the cache.</small></p>
                    </div>
                </div>
            </section>

            <section id="viz-memory-layout">
                <h3>Head-Major vs. Token-Major: The Memory View</h3>
                <p>This is why the C struct layout is critical. It lets us enforce a cache-optimal **Head-Major** memory layout.</p>
                <div class="memory-viz">
                    <div class="memory-layout">
                        <h4>Token-Major (The Slow Way)</h4>
                        <p><small>Head data is scattered. To get all of Head 0, a core must jump across memory, causing cache misses.</small></p>
                        <div class="memory-bar" id="token-major-bar"></div>
                        <pre><code class="c code-block-small">// Accessing requires complex strides
// [T0_H0, T0_H1, ..., T1_H0, T1_H1, ...]</code></pre>
                    </div>
                    <div class="memory-layout">
                        <h4>Head-Major (Your Optimized Way)</h4>
                        <p><small>All data for a head is contiguous. A core reads its assigned head in one sequential, cache-friendly pass.</small></p>
                        <div class="memory-bar" id="head-major-bar"></div>
                        <pre><code class="c code-block-small" data-trim>
// Access is a simple, flattened index
#define ATTN_ACCESS(ptr, h, q, k, T) \
    ptr[((h)*(T)+(q))*(T)+(k)]</code></pre>
                    </div>
                </div>
            </section>

            <section id="viz-parallelism">
                <h3>The Parallelism Dance: Token → Head → Token</h3>
                <p>Our strategy shifts to match the computation, enabled by our memory control.</p>
                <div class="viz-container">
                    <div class="fragment" data-fragment-index="1">
                        <h4>1. Input (Token-Parallel)</h4>
                        <p>Cores work on slices of tokens for LayerNorm & QKV projection.</p>
                        <!-- Visual: Cores taking horizontal slices of X -->
                    </div>
                    <div class="fragment" data-fragment-index="2">
                        <h4>2. Attention (Head-Parallel)</h4>
                        <p>Cores switch to processing entire heads, which are now contiguous in memory. <strong>This is the key HPC win.</strong></p>
                        <!-- Visual: Cores taking vertical, colored blocks of Head-Major memory -->
                    </div>
                    <div class="fragment" data-fragment-index="3">
                        <h4>3. Output (Token-Parallel)</h4>
                        <p>After concatenating head outputs, cores switch back to working on token slices for the MLP.</p>
                        <!-- Visual: Cores taking horizontal slices of the output tensor -->
                    </div>
                </div>
            </section>

            <section data-background-color="#ffffff">
                <h3>The Emergence of Intelligence</h3>
                <img src="assets/emergent.svg" style="width: 95%; border: none; box-shadow: none;">
            </section>

            <section data-background-color="#ffffff">
                <h3>Performance Deep Dive: The Bottlenecks</h3>
                <img src="assets/attention_inference.svg" style="width: 95%; border: none; box-shadow: none;">
            </section>

            <section>
                <h3>Performance</h3>
                <p>By leveraging C, AVX-512, and a cache-optimal memory layout, we achieve performance orders of magnitude faster than a standard Python implementation.</p>
                <p><strong>Result: 400+ GFLOPS on a modern CPU.</strong></p>
                <p class="fragment">This is not just about theory. This is about engineering a solution that makes powerful AI feasible where it wasn't before.</p>
            </section>

        </div>
    </div>

  <script src="../reveal.js/dist/reveal.js"></script>
  <script src="../reveal.js/plugin/zoom/zoom.js"></script>
  <script src="../reveal.js/plugin/notes/notes.js"></script>
  <script src="../reveal.js/plugin/search/search.js"></script>
  <script src="../reveal.js/plugin/markdown/markdown.js"></script>
  <script src="../reveal.js/plugin/highlight/highlight.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.3.1/plugin/math/math.min.js"></script>
  <script>
        document.addEventListener('DOMContentLoaded', function() {
            Reveal.initialize({
                controls: true,
                progress: true,
                center: true,
                hash: true,
                transition: 'slide',
                plugins: [RevealZoom, RevealHighlight, RevealMarkdown, RevealNotes, RevealMath.KaTeX ]
            });

            const HEAD_COLORS = ['#e57373', '#81c784', '#64b5f6', '#ffd54f', '#ba68c8', '#ff8a65', '#a1887f', '#90a4ae'];

            function createMemoryLayouts() {
                const tokenMajorBar = document.getElementById('token-major-bar');
                const headMajorBar = document.getElementById('head-major-bar');
                if (!tokenMajorBar || !headMajorBar) return;
                tokenMajorBar.innerHTML = '';
                headMajorBar.innerHTML = '';

                // Token-Major
                for (let i = 0; i < 64; i++) {
                    const block = document.createElement('div');
                    block.classList.add('mem-block', `head-color-${i % 8}`);
                    tokenMajorBar.appendChild(block);
                }

                // Head-Major
                for (let i = 0; i < 8; i++) {
                    for (let j = 0; j < 8; j++) {
                         const block = document.createElement('div');
                        block.classList.add('mem-block', `head-color-${i}`);
                        headMajorBar.appendChild(block);
                    }
                }
            }

            function setupVizSlides() {
                if (document.getElementById('viz-memory-layout')) {
                    createMemoryLayouts();
                }
            }

            Reveal.on('ready', event => {
                setupVizSlides();
            });
            Reveal.on('slidechanged', event => {
                setupVizSlides();
            });
        });
    </script>
</body>
</html>