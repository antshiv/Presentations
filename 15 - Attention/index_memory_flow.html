<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title>Attention Memory Flow - The Complete Data Journey</title>

    <link rel="stylesheet" href="../reveal.js/dist/reset.css">
    <link rel="stylesheet" href="../reveal.js/dist/reveal.css">
    <link rel="stylesheet" href="../reveal.js/dist/theme/black.css" id="theme">

    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="../reveal.js/plugin/highlight/monokai.css">
    <link rel="stylesheet" href="../css/styles.css">
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/3.9.1/gsap.min.js"></script>

    <style>
        :root { --r-main-font-size: 24px; }
        .reveal .slides section { font-size: 0.9em; text-align: left;}
        .reveal h1, .reveal h2, .reveal h3, .reveal h4 { text-transform: none; text-align: center;}
        .reveal p { text-align: center; }
        
        /* Memory Flow Visualization Styles */
        .memory-flow-container {
            display: flex;
            flex-direction: column;
            gap: 30px;
            margin: 20px 0;
        }
        
        .phase-container {
            display: flex;
            justify-content: space-between;
            align-items: flex-start;
            gap: 20px;
            padding: 20px;
            border: 2px solid #444;
            border-radius: 15px;
            background: rgba(68, 68, 68, 0.1);
        }
        
        .memory-layout-viz {
            width: 30%;
            background: #1a1a1a;
            border-radius: 10px;
            padding: 15px;
            border: 1px solid #555;
        }
        
        .core-distribution {
            width: 35%;
            background: #1a1a1a;
            border-radius: 10px;
            padding: 15px;
            border: 1px solid #555;
        }
        
        .computation-detail {
            width: 30%;
            background: #1a1a1a;
            border-radius: 10px;
            padding: 15px;
            border: 1px solid #555;
        }
        
        .memory-block {
            height: 25px;
            margin: 2px 0;
            border-radius: 3px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 0.7em;
            font-weight: bold;
            color: white;
        }
        
        .token-major { background: linear-gradient(90deg, #e57373, #81c784, #64b5f6, #ffd54f, #ba68c8, #ff8a65, #a1887f, #90a4ae); }
        .head-major-0 { background: #e57373; }
        .head-major-1 { background: #81c784; }
        .head-major-2 { background: #64b5f6; }
        .head-major-3 { background: #ffd54f; color: #000; }
        .head-major-4 { background: #ba68c8; }
        .head-major-5 { background: #ff8a65; }
        .head-major-6 { background: #a1887f; }
        .head-major-7 { background: #90a4ae; }
        
        .core-assignment {
            display: flex;
            align-items: center;
            margin: 8px 0;
            padding: 8px;
            border-radius: 5px;
            background: rgba(100, 181, 246, 0.1);
            border-left: 4px solid #64b5f6;
        }
        
        .core-label {
            width: 60px;
            font-weight: bold;
            color: #64b5f6;
            font-size: 0.8em;
        }
        
        .core-work {
            flex: 1;
            font-size: 0.7em;
            color: #ccc;
        }
        
        .avx-instruction {
            background: rgba(255, 111, 0, 0.1);
            border: 1px solid #ff6f00;
            border-radius: 5px;
            padding: 8px;
            margin: 5px 0;
            font-family: monospace;
            font-size: 0.6em;
            color: #ff6f00;
        }
        
        .phase-title {
            position: absolute;
            top: -15px;
            left: 20px;
            background: #000;
            padding: 5px 15px;
            border-radius: 15px;
            font-size: 0.9em;
            font-weight: bold;
        }
        
        .phase-1 { border-color: #e57373; }
        .phase-1 .phase-title { color: #e57373; border: 2px solid #e57373; }
        
        .phase-2 { border-color: #64b5f6; }
        .phase-2 .phase-title { color: #64b5f6; border: 2px solid #64b5f6; }
        
        .phase-3 { border-color: #ffd54f; }
        .phase-3 .phase-title { color: #ffd54f; border: 2px solid #ffd54f; }
        
        .phase-4 { border-color: #81c784; }
        .phase-4 .phase-title { color: #81c784; border: 2px solid #81c784; }
        
        .data-flow-arrow {
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 2em;
            color: #666;
            height: 50px;
        }
        
        .formula-box { 
            background-color: rgba(45, 51, 59, 0.8); 
            border-radius: 15px; 
            padding: 15px; 
            margin-top: 15px; 
            border: 1px solid #484f58; 
        }
        
        .code-block-tiny { font-size: 0.6em !important; line-height: 1.2; }
    </style>
</head>
<body>
    <div class="reveal">
        <div class="slides">
            <!-- Title Slide -->
            <section>
                <h2>Attention Memory Flow</h2>
                <h3>The Complete Data Journey</h3>
                <h4>From Token-Major Input to Head-Parallel Computation to Token-Major Output</h4>
                <p>Understanding how data flows through memory and cores in our high-performance attention implementation</p>
            </section>

            <!-- Overview: The Four-Phase Memory Dance -->
            <section>
                <h3>The Four-Phase Memory Dance</h3>
                <p>Our attention implementation orchestrates a sophisticated data flow through memory layouts and core assignments</p>
                <div style="display: grid; grid-template-columns: repeat(4, 1fr); gap: 20px; margin-top: 30px;">
                    <div style="text-align: center; padding: 20px; border: 2px solid #e57373; border-radius: 10px; background: rgba(229, 115, 115, 0.1);">
                        <div style="font-size: 1.5em; margin-bottom: 10px;">📥</div>
                        <h4>Phase 1: QKV Projection</h4>
                        <p style="font-size: 0.8em;">Token-parallel input<br>→ Head-major output</p>
                    </div>
                    <div style="text-align: center; padding: 20px; border: 2px solid #64b5f6; border-radius: 10px; background: rgba(100, 181, 246, 0.1);">
                        <div style="font-size: 1.5em; margin-bottom: 10px;">🧮</div>
                        <h4>Phase 2: Attention Scores</h4>
                        <p style="font-size: 0.8em;">Head-parallel input<br>→ Head-parallel scores</p>
                    </div>
                    <div style="text-align: center; padding: 20px; border: 2px solid #ffd54f; border-radius: 10px; background: rgba(255, 213, 79, 0.1);">
                        <div style="font-size: 1.5em; margin-bottom: 10px;">🔗</div>
                        <h4>Phase 3: Concatenation</h4>
                        <p style="font-size: 0.8em;">Head-parallel input<br>→ Token-major output</p>
                    </div>
                    <div style="text-align: center; padding: 20px; border: 2px solid #81c784; border-radius: 10px; background: rgba(129, 199, 132, 0.1);">
                        <div style="font-size: 1.5em; margin-bottom: 10px;">📤</div>
                        <h4>Phase 4: Final Projection</h4>
                        <p style="font-size: 0.8em;">Token-parallel input<br>→ Token-parallel output</p>
                    </div>
                </div>
                <p class="fragment" style="margin-top: 30px;"><strong>Key Insight:</strong> We use head-parallelism as our primary strategy, but adapt the memory layout to optimize each computation phase.</p>
            </section>

            <!-- Phase 1: QKV Projection Memory Flow -->
            <section>
                <h3>Phase 1: QKV Projection - Token-Parallel to Head-Major</h3>
                <div class="memory-flow-container">
                    <div class="phase-container phase-1" style="position: relative;">
                        <div class="phase-title">Phase 1: QKV Projection</div>
                        
                        <div class="memory-layout-viz">
                            <h4>Memory Layout</h4>
                            <div style="margin-bottom: 15px;">
                                <strong>Input (Token-Major):</strong>
                                <div class="memory-block token-major">T0: [h0,h1,h2,h3,h4,h5,h6,h7]</div>
                                <div class="memory-block token-major">T1: [h0,h1,h2,h3,h4,h5,h6,h7]</div>
                                <div class="memory-block token-major">T2: [h0,h1,h2,h3,h4,h5,h6,h7]</div>
                                <div style="text-align: center; color: #666; font-size: 0.7em;">...</div>
                            </div>
                            <div class="data-flow-arrow">↓</div>
                            <div>
                                <strong>Output (Head-Major):</strong>
                                <div class="memory-block head-major-0">Head 0: [T0,T1,T2,...,TN]</div>
                                <div class="memory-block head-major-1">Head 1: [T0,T1,T2,...,TN]</div>
                                <div class="memory-block head-major-2">Head 2: [T0,T1,T2,...,TN]</div>
                                <div style="text-align: center; color: #666; font-size: 0.7em;">...</div>
                            </div>
                        </div>
                        
                        <div class="core-distribution">
                            <h4>Core Assignment</h4>
                            <div class="core-assignment">
                                <div class="core-label">Core 0:</div>
                                <div class="core-work">Tokens 0-255 → All heads</div>
                            </div>
                            <div class="core-assignment">
                                <div class="core-label">Core 1:</div>
                                <div class="core-work">Tokens 256-511 → All heads</div>
                            </div>
                            <div class="core-assignment">
                                <div class="core-label">Core 2:</div>
                                <div class="core-work">Tokens 512-767 → All heads</div>
                            </div>
                            <div class="core-assignment">
                                <div class="core-label">Core 3:</div>
                                <div class="core-work">Tokens 768-1023 → All heads</div>
                            </div>
                            <p style="font-size: 0.8em; color: #ccc; margin-top: 15px;">
                                <strong>Strategy:</strong> Each core processes a slice of tokens but writes to all heads in head-major format
                            </p>
                        </div>
                        
                        <div class="computation-detail">
                            <h4>AVX-512 Computation</h4>
                            <div class="avx-instruction">
                                // For each token in core's slice<br>
                                for (int h = 0; h < num_heads; h++) {<br>
                                &nbsp;&nbsp;avx512_gemv_projection(...);<br>
                                &nbsp;&nbsp;// Write to head-major location<br>
                                &nbsp;&nbsp;Q_ACCESS(q_base, h, token, d);<br>
                                }
                            </div>
                            <p style="font-size: 0.8em; color: #ccc;">
                                <strong>Key:</strong> Direct write to head-major layout avoids expensive transpose later
                            </p>
                            <div style="margin-top: 15px;">
                                <strong>Memory Access Pattern:</strong>
                                <div style="font-size: 0.7em; color: #999; margin-top: 5px;">
                                    • Read: Sequential token data<br>
                                    • Write: Scattered to head locations<br>
                                    • Cache: Good for reads, writes distributed
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Phase 2: Attention Scores - Head-Parallel -->
            <section>
                <h3>Phase 2: Attention Scores - Head-Parallel Perfection</h3>
                <div class="memory-flow-container">
                    <div class="phase-container phase-2" style="position: relative;">
                        <div class="phase-title">Phase 2: Attention Computation</div>
                        
                        <div class="memory-layout-viz">
                            <h4>Memory Layout</h4>
                            <div style="margin-bottom: 15px;">
                                <strong>Input (Head-Major):</strong>
                                <div class="memory-block head-major-0">Head 0: [T0,T1,T2,...,TN]</div>
                                <div class="memory-block head-major-1">Head 1: [T0,T1,T2,...,TN]</div>
                                <div class="memory-block head-major-2">Head 2: [T0,T1,T2,...,TN]</div>
                                <div style="text-align: center; color: #666; font-size: 0.7em;">...</div>
                            </div>
                            <div class="data-flow-arrow">↓</div>
                            <div>
                                <strong>Output (Head-Major Scores):</strong>
                                <div class="memory-block head-major-0">H0 Scores: [TxT matrix]</div>
                                <div class="memory-block head-major-1">H1 Scores: [TxT matrix]</div>
                                <div class="memory-block head-major-2">H2 Scores: [TxT matrix]</div>
                                <div style="text-align: center; color: #666; font-size: 0.7em;">...</div>
                            </div>
                        </div>
                        
                        <div class="core-distribution">
                            <h4>Core Assignment</h4>
                            <div class="core-assignment">
                                <div class="core-label">Core 0:</div>
                                <div class="core-work">Head 0 → Complete Q·K^T</div>
                            </div>
                            <div class="core-assignment">
                                <div class="core-label">Core 1:</div>
                                <div class="core-work">Head 1 → Complete Q·K^T</div>
                            </div>
                            <div class="core-assignment">
                                <div class="core-label">Core 2:</div>
                                <div class="core-work">Head 2 → Complete Q·K^T</div>
                            </div>
                            <div class="core-assignment">
                                <div class="core-label">Core 3:</div>
                                <div class="core-work">Head 3 → Complete Q·K^T</div>
                            </div>
                            <p style="font-size: 0.8em; color: #ccc; margin-top: 15px;">
                                <strong>Strategy:</strong> Perfect cache locality - each core owns entire head data
                            </p>
                        </div>
                        
                        <div class="computation-detail">
                            <h4>AVX-512 Computation</h4>
                            <div class="avx-instruction">
                                // Each core processes one head<br>
                                for (int i = 0; i < tokens; i++) {<br>
                                &nbsp;&nbsp;for (int j = 0; j <= i; j++) {<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;__m512 acc = _mm512_setzero_ps();<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;// Vectorized dot product<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;acc = _mm512_fmadd_ps(q, k, acc);<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;score = _mm512_reduce_add_ps(acc);<br>
                                &nbsp;&nbsp;}<br>
                                }
                            </div>
                            <div style="margin-top: 15px;">
                                <strong>Memory Access Pattern:</strong>
                                <div style="font-size: 0.7em; color: #999; margin-top: 5px;">
                                    • Read: Sequential within head<br>
                                    • Write: Sequential score matrix<br>
                                    • Cache: Perfect locality, 95%+ hit rate
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Phase 3: Concatenation - Head-Major to Token-Major -->
            <section>
                <h3>Phase 3: Concatenation - Head-Major to Token-Major</h3>
                <div class="memory-flow-container">
                    <div class="phase-container phase-3" style="position: relative;">
                        <div class="phase-title">Phase 3: Concatenation</div>
                        
                        <div class="memory-layout-viz">
                            <h4>Memory Layout</h4>
                            <div style="margin-bottom: 15px;">
                                <strong>Input (Head-Major):</strong>
                                <div class="memory-block head-major-0">Head 0: [T0,T1,T2,...,TN]</div>
                                <div class="memory-block head-major-1">Head 1: [T0,T1,T2,...,TN]</div>
                                <div class="memory-block head-major-2">Head 2: [T0,T1,T2,...,TN]</div>
                                <div style="text-align: center; color: #666; font-size: 0.7em;">...</div>
                            </div>
                            <div class="data-flow-arrow">↓</div>
                            <div>
                                <strong>Output (Token-Major):</strong>
                                <div class="memory-block token-major">T0: [h0,h1,h2,h3,h4,h5,h6,h7]</div>
                                <div class="memory-block token-major">T1: [h0,h1,h2,h3,h4,h5,h6,h7]</div>
                                <div class="memory-block token-major">T2: [h0,h1,h2,h3,h4,h5,h6,h7]</div>
                                <div style="text-align: center; color: #666; font-size: 0.7em;">...</div>
                            </div>
                        </div>
                        
                        <div class="core-distribution">
                            <h4>Core Assignment</h4>
                            <div class="core-assignment">
                                <div class="core-label">Core 0:</div>
                                <div class="core-work">Tokens 0-255 ← From all heads</div>
                            </div>
                            <div class="core-assignment">
                                <div class="core-label">Core 1:</div>
                                <div class="core-work">Tokens 256-511 ← From all heads</div>
                            </div>
                            <div class="core-assignment">
                                <div class="core-label">Core 2:</div>
                                <div class="core-work">Tokens 512-767 ← From all heads</div>
                            </div>
                            <div class="core-assignment">
                                <div class="core-label">Core 3:</div>
                                <div class="core-work">Tokens 768-1023 ← From all heads</div>
                            </div>
                            <p style="font-size: 0.8em; color: #ccc; margin-top: 15px;">
                                <strong>Strategy:</strong> Conservative threading to avoid memory bandwidth saturation
                            </p>
                        </div>
                        
                        <div class="computation-detail">
                            <h4>Memory Reorganization</h4>
                            <div class="avx-instruction">
                                // Each core handles token slice<br>
                                for (int t = token_start; t < token_end; t++) {<br>
                                &nbsp;&nbsp;for (int h = 0; h < num_heads; h++) {<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;for (int d = 0; d < head_dim; d++) {<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;// Gather from head-major<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;val = Q_ACCESS(head_data, h, t, d);<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;// Write to token-major<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;token_out[h*head_dim + d] = val;<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;}<br>
                                &nbsp;&nbsp;}<br>
                                }
                            </div>
                            <div style="margin-top: 15px;">
                                <strong>Memory Access Pattern:</strong>
                                <div style="font-size: 0.7em; color: #999; margin-top: 5px;">
                                    • Read: Scattered across heads<br>
                                    • Write: Sequential token data<br>
                                    • Cache: Memory bandwidth limited
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Phase 4: Final Projection - Token-Major -->
            <section>
                <h3>Phase 4: Final Projection - Token-Major Efficiency</h3>
                <div class="memory-flow-container">
                    <div class="phase-container phase-4" style="position: relative;">
                        <div class="phase-title">Phase 4: Final Projection</div>
                        
                        <div class="memory-layout-viz">
                            <h4>Memory Layout</h4>
                            <div style="margin-bottom: 15px;">
                                <strong>Input (Token-Major):</strong>
                                <div class="memory-block token-major">T0: [h0,h1,h2,h3,h4,h5,h6,h7]</div>
                                <div class="memory-block token-major">T1: [h0,h1,h2,h3,h4,h5,h6,h7]</div>
                                <div class="memory-block token-major">T2: [h0,h1,h2,h3,h4,h5,h6,h7]</div>
                                <div style="text-align: center; color: #666; font-size: 0.7em;">...</div>
                            </div>
                            <div class="data-flow-arrow">↓</div>
                            <div>
                                <strong>Output (Token-Major):</strong>
                                <div class="memory-block token-major">T0: [out_features]</div>
                                <div class="memory-block token-major">T1: [out_features]</div>
                                <div class="memory-block token-major">T2: [out_features]</div>
                                <div style="text-align: center; color: #666; font-size: 0.7em;">...</div>
                            </div>
                        </div>
                        
                        <div class="core-distribution">
                            <h4>Core Assignment</h4>
                            <div class="core-assignment">
                                <div class="core-label">Core 0:</div>
                                <div class="core-work">Tokens 0-255 → GEMV</div>
                            </div>
                            <div class="core-assignment">
                                <div class="core-label">Core 1:</div>
                                <div class="core-work">Tokens 256-511 → GEMV</div>
                            </div>
                            <div class="core-assignment">
                                <div class="core-label">Core 2:</div>
                                <div class="core-work">Tokens 512-767 → GEMV</div>
                            </div>
                            <div class="core-assignment">
                                <div class="core-label">Core 3:</div>
                                <div class="core-work">Tokens 768-1023 → GEMV</div>
                            </div>
                            <p style="font-size: 0.8em; color: #ccc; margin-top: 15px;">
                                <strong>Strategy:</strong> Standard token-parallel GEMM, optimal for this layout
                            </p>
                        </div>
                        
                        <div class="computation-detail">
                            <h4>AVX-512 GEMV</h4>
                            <div class="avx-instruction">
                                // Each core: token slice × weights<br>
                                for (int t = token_start; t < token_end; t++) {<br>
                                &nbsp;&nbsp;const float *input = concat_buf + t*embed_dim;<br>
                                &nbsp;&nbsp;float *output = result + t*embed_dim;<br>
                                &nbsp;&nbsp;<br>
                                &nbsp;&nbsp;// AVX-512 optimized GEMV<br>
                                &nbsp;&nbsp;avx512_gemv_with_bias(input, weights,<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bias, output,<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;embed_dim);<br>
                                }
                            </div>
                            <div style="margin-top: 15px;">
                                <strong>Memory Access Pattern:</strong>
                                <div style="font-size: 0.7em; color: #999; margin-top: 5px;">
                                    • Read: Sequential token data<br>
                                    • Write: Sequential output<br>
                                    • Cache: Excellent locality
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Memory Access Patterns Summary -->
            <section>
                <h3>Memory Access Patterns: The Complete Picture</h3>
                <p>Understanding how each phase optimizes for different memory access patterns</p>
                <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 20px; margin-top: 30px;">
                    <div style="background: rgba(229, 115, 115, 0.1); padding: 20px; border-radius: 10px; border: 1px solid #e57373;">
                        <h4>Cache-Friendly Phases</h4>
                        <div style="font-size: 0.8em;">
                            <div style="margin: 10px 0; padding: 10px; background: rgba(100, 181, 246, 0.1); border-radius: 5px;">
                                <strong>Phase 2: Attention Scores</strong><br>
                                95%+ L3 cache hit rate<br>
                                Perfect head-major locality
                            </div>
                            <div style="margin: 10px 0; padding: 10px; background: rgba(129, 199, 132, 0.1); border-radius: 5px;">
                                <strong>Phase 4: Final Projection</strong><br>
                                Sequential token access<br>
                                Standard GEMM efficiency
                            </div>
                        </div>
                    </div>
                    <div style="background: rgba(255, 213, 79, 0.1); padding: 20px; border-radius: 10px; border: 1px solid #ffd54f;">
                        <h4>Memory-Bandwidth Limited</h4>
                        <div style="font-size: 0.8em;">
                            <div style="margin: 10px 0; padding: 10px; background: rgba(229, 115, 115, 0.1); border-radius: 5px;">
                                <strong>Phase 1: QKV Projection</strong><br>
                                Scattered writes to heads<br>
                                Token-parallel helps distribute
                            </div>
                            <div style="margin: 10px 0; padding: 10px; background: rgba(255, 213, 79, 0.1); border-radius: 5px;">
                                <strong>Phase 3: Concatenation</strong><br>
                                Scattered reads from heads<br>
                                Conservative threading needed
                            </div>
                        </div>
                    </div>
                </div>
                <div class="formula-box fragment" style="margin-top: 30px;">
                    <strong>Key Insight:</strong> We prioritize head-parallelism for the compute-intensive Phase 2, where cache locality has the biggest performance impact. 
                    Phases 1, 3, and 4 are optimized for memory bandwidth efficiency.
                </div>
            </section>

            <!-- The Head-Major Memory Access Formula -->
            <section>
                <h3>Head-Major Memory Access: The Mathematical Reality</h3>
                <p>How our memory access macros translate to actual hardware addresses</p>
                <div style="display: flex; justify-content: space-between; gap: 30px;">
                    <div style="width: 45%;">
                        <h4>Memory Layout Formula</h4>
                        <div class="formula-box">
                            <strong>Head-Major Access:</strong><br>
                            <code>Q_ACCESS(q_ptr, h, t, d, context_window, aligned_head_dim)</code>
                            <br><br>
                            <strong>Address Calculation:</strong><br>
                            <code>q_ptr[((h * context_window) + t) * aligned_head_dim + d]</code>
                        </div>
                        <div style="margin-top: 20px; font-size: 0.8em;">
                            <strong>Example:</strong> Head 2, Token 5, Dimension 10<br>
                            <code>= q_ptr[((2 * 1024) + 5) * 64 + 10]</code><br>
                            <code>= q_ptr[2053 * 64 + 10]</code><br>
                            <code>= q_ptr[131402]</code>
                        </div>
                    </div>
                    <div style="width: 50%;">
                        <h4>Cache Line Optimization</h4>
                        <div style="background: #1a1a1a; padding: 15px; border-radius: 10px; font-family: monospace; font-size: 0.7em;">
                            <div style="color: #81c784;">Cache Line 0: Head0[T0][0-15]   ← 64 bytes</div>
                            <div style="color: #81c784;">Cache Line 1: Head0[T0][16-31]  ← 64 bytes</div>
                            <div style="color: #81c784;">Cache Line 2: Head0[T0][32-47]  ← 64 bytes</div>
                            <div style="color: #81c784;">Cache Line 3: Head0[T0][48-63]  ← 64 bytes</div>
                            <div style="color: #64b5f6;">Cache Line 4: Head0[T1][0-15]   ← 64 bytes</div>
                            <div style="color: #64b5f6;">Cache Line 5: Head0[T1][16-31]  ← 64 bytes</div>
                            <div style="color: #ccc;">...</div>
                        </div>
                        <p style="font-size: 0.8em; margin-top: 15px;">
                            <strong>Benefit:</strong> Processing Head 0 sequentially loads exactly what we need into cache.
                            No cache conflicts between different heads.
                        </p>
                    </div>
                </div>
            </section>

            <!-- Future Optimization: Direct Strided Projection -->
            <section>
                <h3>Future Optimization: Eliminating Phase 3</h3>
                <p>Direct strided projection to skip concatenation entirely</p>
                <div style="display: flex; justify-content: space-between; gap: 30px;">
                    <div style="width: 45%;">
                        <h4>Current: 4-Phase Process</h4>
                        <div style="background: rgba(255, 213, 79, 0.1); padding: 15px; border-radius: 10px; margin: 10px 0;">
                            <strong>Phase 3: Concatenation</strong><br>
                            <small>Memory bandwidth bottleneck</small><br>
                            <small>~15% of total attention time</small>
                        </div>
                        <div class="avx-instruction">
                            // Current approach<br>
                            // 1. Head-major → Token-major (Phase 3)<br>
                            // 2. Token-major GEMM (Phase 4)<br>
                            for (int t = 0; t < tokens; t++) {<br>
                            &nbsp;&nbsp;concat_heads_to_token(t);<br>
                            &nbsp;&nbsp;gemv_projection(token_data[t]);<br>
                            }
                        </div>
                    </div>
                    <div style="width: 50%;">
                        <h4>Future: 3-Phase Process</h4>
                        <div style="background: rgba(129, 199, 132, 0.1); padding: 15px; border-radius: 10px; margin: 10px 0;">
                            <strong>Direct Strided Projection</strong><br>
                            <small>Skip concatenation entirely</small><br>
                            <small>Potential 10-15% speedup</small>
                        </div>
                        <div class="avx-instruction">
                            // Future optimization<br>
                            // Direct head-major → token-major projection<br>
                            for (int t = 0; t < tokens; t++) {<br>
                            &nbsp;&nbsp;for (int out_dim = 0; out_dim < embed_dim; out_dim++) {<br>
                            &nbsp;&nbsp;&nbsp;&nbsp;float sum = 0;<br>
                            &nbsp;&nbsp;&nbsp;&nbsp;for (int h = 0; h < num_heads; h++) {<br>
                            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sum += HEAD_ACCESS(h,t,d) * WEIGHT(h,d,out_dim);<br>
                            &nbsp;&nbsp;&nbsp;&nbsp;}<br>
                            &nbsp;&nbsp;&nbsp;&nbsp;output[t][out_dim] = sum;<br>
                            &nbsp;&nbsp;}<br>
                            }
                        </div>
                    </div>
                </div>
                <p class="fragment" style="margin-top: 20px; font-size: 0.9em;">
                    <strong>Research Question:</strong> Will strided access patterns offset the memory bandwidth savings? 
                    Optimal strategy may depend on core count and memory architecture.
                </p>
            </section>

            <!-- Performance Impact Summary -->
            <section>
                <h3>Performance Impact: Where Every Microsecond Counts</h3>
                <p>Real measurements from our 400+ GFLOPS implementation</p>
                <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 30px; margin-top: 30px;">
                    <div>
                        <h4>Phase Timing Breakdown</h4>
                        <div style="background: #1a1a1a; padding: 15px; border-radius: 10px;">
                            <div style="display: flex; justify-content: space-between; margin: 8px 0; padding: 8px; background: rgba(229, 115, 115, 0.2); border-radius: 5px;">
                                <span>Phase 1 (QKV)</span><span>~0.3ms</span>
                            </div>
                            <div style="display: flex; justify-content: space-between; margin: 8px 0; padding: 8px; background: rgba(100, 181, 246, 0.2); border-radius: 5px;">
                                <span>Phase 2 (Attention)</span><span>~1.2ms</span>
                            </div>
                            <div style="display: flex; justify-content: space-between; margin: 8px 0; padding: 8px; background: rgba(255, 213, 79, 0.2); border-radius: 5px;">
                                <span>Phase 3 (Concat)</span><span>~0.2ms</span>
                            </div>
                            <div style="display: flex; justify-content: space-between; margin: 8px 0; padding: 8px; background: rgba(129, 199, 132, 0.2); border-radius: 5px;">
                                <span>Phase 4 (Projection)</span><span>~0.3ms</span>
                            </div>
                            <div style="display: flex; justify-content: space-between; margin: 8px 0; padding: 8px; background: rgba(186, 104, 200, 0.2); border-radius: 5px; font-weight: bold;">
                                <span>Total</span><span>~2.0ms</span>
                            </div>
                        </div>
                    </div>
                    <div>
                        <h4>Memory Hierarchy Utilization</h4>
                        <div style="background: #1a1a1a; padding: 15px; border-radius: 10px;">
                            <div style="margin: 10px 0;">
                                <div style="font-weight: bold; margin-bottom: 5px;">L1 Cache (32KB per core)</div>
                                <div style="width: 100%; height: 20px; background: #333; border-radius: 10px; overflow: hidden;">
                                    <div style="width: 85%; height: 100%; background: #81c784;"></div>
                                </div>
                                <div style="font-size: 0.8em; color: #ccc;">85% hit rate</div>
                            </div>
                            <div style="margin: 10px 0;">
                                <div style="font-weight: bold; margin-bottom: 5px;">L2 Cache (256KB per core)</div>
                                <div style="width: 100%; height: 20px; background: #333; border-radius: 10px; overflow: hidden;">
                                    <div style="width: 92%; height: 100%; background: #64b5f6;"></div>
                                </div>
                                <div style="font-size: 0.8em; color: #ccc;">92% hit rate</div>
                            </div>
                            <div style="margin: 10px 0;">
                                <div style="font-weight: bold; margin-bottom: 5px;">L3 Cache (32MB shared)</div>
                                <div style="width: 100%; height: 20px; background: #333; border-radius: 10px; overflow: hidden;">
                                    <div style="width: 96%; height: 100%; background: #ff6f00;"></div>
                                </div>
                                <div style="font-size: 0.8em; color: #ccc;">96% hit rate</div>
                            </div>
                        </div>
                    </div>
                </div>
                <div class="formula-box" style="margin-top: 30px;">
                    <strong>Key Result:</strong> Head-major layout in Phase 2 achieves 96% L3 cache hit rate, 
                    making it the most efficient phase despite being the most compute-intensive.
                </div>
            </section>

            <!-- Conclusion -->
            <section>
                <h3>Conclusion: Memory Flow Mastery</h3>
                <p>Our sophisticated data flow achieves world-class performance through careful memory orchestration</p>
                <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 30px; margin-top: 30px;">
                    <div>
                        <h4>🎯 Key Innovations</h4>
                        <ul style="font-size: 0.8em; text-align: left;">
                            <li><strong>Head-Major Priority:</strong> Optimize cache for compute-heavy Phase 2</li>
                            <li><strong>Direct Write Strategy:</strong> QKV projection writes head-major immediately</li>
                            <li><strong>Conservative Concatenation:</strong> Limited threading prevents bandwidth saturation</li>
                            <li><strong>Phase-Adaptive Parallelism:</strong> Match strategy to memory access patterns</li>
                        </ul>
                    </div>
                    <div>
                        <h4>📊 Performance Results</h4>
                        <ul style="font-size: 0.8em; text-align: left;">
                            <li><strong>400+ GFLOPS:</strong> Sustained performance on Intel Xeon</li>
                            <li><strong>96% Cache Hit Rate:</strong> L3 cache utilization in Phase 2</li>
                            <li><strong>~2ms Total:</strong> Complete attention layer timing</li>
                            <li><strong>10-50x Speedup:</strong> vs PyTorch CPU implementation</li>
                        </ul>
                    </div>
                </div>
                <div style="margin-top: 40px; text-align: center; font-size: 1.2em; color: #ff6f00; font-weight: bold;">
                    From scattered memory access to perfect cache locality:<br>
                    The art of high-performance attention implementation
                </div>
            </section>

        </div>
    </div>

  <script src="../reveal.js/dist/reveal.js"></script>
  <script src="../reveal.js/plugin/zoom/zoom.js"></script>
  <script src="../reveal.js/plugin/notes/notes.js"></script>
  <script src="../reveal.js/plugin/search/search.js"></script>
  <script src="../reveal.js/plugin/markdown/markdown.js"></script>
  <script src="../reveal.js/plugin/highlight/highlight.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.3.1/plugin/math/math.min.js"></script>
  <script>
        document.addEventListener('DOMContentLoaded', function() {
            Reveal.initialize({
                controls: true,
                progress: true,
                center: true,
                hash: true,
                transition: 'slide',
                plugins: [RevealZoom, RevealHighlight, RevealMarkdown, RevealNotes, RevealMath.KaTeX ]
            });
        });
    </script>
</body>
</html>