<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title>Attention Is All You Need - The Complete Journey</title>

    <link rel="stylesheet" href="../reveal.js/dist/reset.css">
    <link rel="stylesheet" href="../reveal.js/dist/reveal.css">
    <link rel="stylesheet" href="../reveal.js/dist/theme/black.css" id="theme">

    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="../reveal.js/plugin/highlight/monokai.css">
    <link rel="stylesheet" href="../css/styles.css">
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/3.9.1/gsap.min.js"></script>

    <style>
        :root { --r-main-font-size: 24px; }
        .reveal .slides section { font-size: 0.9em; text-align: left;}
        .reveal h1, .reveal h2, .reveal h3, .reveal h4 { text-transform: none; text-align: center;}
        .reveal p { text-align: center; }
        .formula-box { background-color: rgba(45, 51, 59, 0.8); border-radius: 15px; padding: 15px; margin-top: 15px; border: 1px solid #484f58; }
        .dim-table { margin: 15px auto; font-size: 0.75em; border-collapse: collapse; }
        .dim-table th, .dim-table td { border: 1px solid #484f58; padding: 8px 15px; }
        .dim-table th { background-color: #37474f; }
        .transformer-block { display: flex; flex-direction: column; align-items: center; gap: 10px; margin-top: 20px;}
        .block-component { border: 2px solid #484f58; border-radius: 10px; padding: 10px 20px; width: 400px; text-align: center; background-color: #2d333b; }
        .block-component.highlight { border-color: #ff6f00; background-color: #4d3c20; box-shadow: 0 0 15px #ff6f00; }
        .arrow-down { width: 0; height: 0; border-left: 15px solid transparent; border-right: 15px solid transparent; border-top: 20px solid #484f58; }

        /* Visualization & Memory Styles */
        .viz-container { display: flex; justify-content: space-around; align-items: center; gap: 20px; width: 100%; margin-top: 20px;}
        .grid-container { position: relative; text-align: center;}
        .heatmap-grid { display: grid; gap: 1px; border: 1px solid #666; margin: 0 auto;}
        .heatmap-cell { background-color: #4a90e2; }
        .legend { color: #ccc; font-size: 0.7em; margin-top: 5px;}
        .legend-y { writing-mode: vertical-rl; transform: rotate(180deg); position: absolute; left: -40px; top: 50%; transform-origin: center; }
        .legend-x { position: absolute; top: -30px; left: 50%; transform: translateX(-50%); }
        .op-label { font-size: 2.5em; color: #ccc; align-self: center;}
        .heads-container { display: grid; grid-template-columns: repeat(4, 1fr); gap: 15px; }
        .memory-layout { width: 48%; background-color: #1e1e1e; padding: 15px; border-radius: 10px; border: 1px solid #444;}
        .memory-bar { display: flex; flex-wrap: wrap; border: 2px solid #888; background: #111; padding: 2px; border-radius: 5px; min-height: 40px;}
        .mem-block { height: 20px; box-sizing: border-box; border: 1px solid #333; transition: all 0.3s;}
        .head-color-0 { background-color: #e57373; } .head-color-1 { background-color: #81c784; } .head-color-2 { background-color: #64b5f6; } .head-color-3 { background-color: #ffd54f; }
        .head-color-4 { background-color: #ba68c8; } .head-color-5 { background-color: #ff8a65; } .head-color-6 { background-color: #a1887f; } .head-color-7 { background-color: #90a4ae; }
        .code-block-small { font-size: 0.7em !important; }
        .code-block-tiny { font-size: 0.6em !important; line-height: 1.2; }
        .highlight-box { border: 3px solid #ff6f00 !important; box-shadow: 0 0 10px #ff6f00; }
        
        /* Memory comparison styles */
        .memory-viz { display: flex; justify-content: space-around; width: 100%; }
        .explanation { text-align: left; margin-top: 20px; font-size: 0.9em; }
        .explanation li { margin-bottom: 15px; }
        .svg-text { font-family: var(--r-main-font); fill: white; text-anchor: middle; font-size: 12px; }
        
        /* Performance indicators */
        .perf-metric { 
            background: rgba(100, 181, 246, 0.2); 
            border: 1px solid #64b5f6; 
            border-radius: 10px; 
            padding: 15px; 
            margin: 10px 0; 
            text-align: center; 
        }
        .perf-number { 
            font-size: 2em; 
            color: #64b5f6; 
            font-weight: bold; 
        }
        
        /* Flow diagrams */
        .flow-stage { 
            border: 2px solid #81c784; 
            border-radius: 10px; 
            padding: 15px; 
            margin: 10px; 
            background-color: rgba(129, 199, 132, 0.1); 
        }
        .flow-stage.token-parallel { border-color: #e57373; background-color: rgba(229, 115, 115, 0.1); }
        .flow-stage.head-parallel { border-color: #64b5f6; background-color: rgba(100, 181, 246, 0.1); }
        .flow-stage.mixed { border-color: #ffd54f; background-color: rgba(255, 213, 79, 0.1); }
        
        .phase-indicator { 
            display: inline-block; 
            background: #ff6f00; 
            color: #000; 
            padding: 5px 10px; 
            border-radius: 15px; 
            font-weight: bold; 
            margin-right: 10px; 
        }
        
        /* Journey progress indicator */
        .journey-progress {
            display: flex;
            justify-content: center;
            align-items: center;
            margin: 20px 0;
            gap: 10px;
        }
        .progress-step {
            padding: 8px 16px;
            border-radius: 20px;
            font-size: 0.8em;
            font-weight: bold;
        }
        .progress-step.active { background: #ff6f00; color: #000; }
        .progress-step.inactive { background: #444; color: #ccc; }
        .progress-arrow { color: #666; font-size: 1.2em; }
    </style>
</head>
<body>
    <div class="reveal">
        <div class="slides">
            <!-- Title Slide -->
            <section>
                <h2>Attention Is All You Need</h2>
                <h3>The Complete Journey</h3>
                <h4>From Mathematical Theory to Production-Grade AVX-512 Implementation</h4>
                <p>Math → Intuition → Implementation → HPC → 400+ GFLOPS</p>
                <div class="journey-progress">
                    <div class="progress-step active">Theory</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step inactive">Intuition</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step inactive">Implementation</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step inactive">HPC</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step inactive">Production</div>
                </div>
                <p><small>A comprehensive walk-through from fundamental mathematics to real-world performance</small></p>
            </section>

            <!-- PART I: MATHEMATICAL FOUNDATIONS -->
            <section>
                <h2>Part I: Mathematical Foundations</h2>
                <div class="journey-progress">
                    <div class="progress-step active">Theory</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step inactive">Intuition</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step inactive">Implementation</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step inactive">HPC</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step inactive">Production</div>
                </div>
                <p>Understanding the mathematical building blocks that make attention work</p>
            </section>

            <!-- Why Attention Matters -->
            <section>
                <h3>Why Attention Changed Everything</h3>
                <p>Before attention, sequence models were fundamentally limited by sequential processing. Attention introduced parallelization and global context understanding.</p>
                <div style="display: flex; justify-content: space-around; margin-top: 30px;">
                    <div style="width: 45%;">
                        <h4>Traditional RNNs</h4>
                        <ul style="font-size: 0.8em;">
                            <li>Sequential processing: O(T) time complexity</li>
                            <li>Information bottleneck through hidden states</li>
                            <li>Gradient vanishing over long sequences</li>
                            <li>No parallelization possible</li>
                        </ul>
                    </div>
                    <div style="width: 45%;">
                        <h4>Self-Attention</h4>
                        <ul style="font-size: 0.8em;">
                            <li>Parallel processing: O(1) with sufficient cores</li>
                            <li>Direct connections between all positions</li>
                            <li>Constant path length for information flow</li>
                            <li>Highly parallelizable computation</li>
                        </ul>
                    </div>
                </div>
                <div class="formula-box fragment">
                    <strong>The Core Insight:</strong> Instead of sequential hidden states, compute attention weights between all pairs of positions simultaneously.
                </div>
            </section>

            <!-- The Goal of Attention -->
            <section>
                <h2>The Goal of Attention</h2>
                <p>For each token, we want to create a new representation that is a <span style="color: #ff6f00;">weighted average</span> of all other tokens in the sequence.</p>
                <p class="fragment">The weights are not fixed; they are calculated on the fly based on how <span style="color: #ff6f00;">relevant</span> each token is to the current one we're processing.</p>
                <div class="formula-box fragment">
                    $$ \text{Output}_i = \sum_{j=1}^{T} \alpha_{ij} \cdot \text{Value}_j $$
                    <p style="margin-top: 10px;"><small>Where $\alpha_{ij}$ represents how much token $i$ should attend to token $j$</small></p>
                </div>
                <p class="fragment"><small><strong>Key Question:</strong> How do we compute the attention weights $\alpha_{ij}$?</small></p>
            </section>

            <!-- Step 1: QKV Projections -->
            <section>
                <h2>Step 1: Projecting Inputs into Q, K, V</h2>
                <p>We start with our input tensor `X` and project it into three distinct matrices: Queries, Keys, and Values, using learned weight matrices.</p>
                <div class="formula-box">
                    $$ Q = X W_Q \quad K = X W_K \quad V = X W_V $$
                </div>
                <div class="explanation">
                    <ul>
                        <li><span style="color: #ff6f00;">$X$</span>: The input tensor of token embeddings. Dimension: `[T, C]`</li>
                        <li><span style="color: #ff6f00;">$W_Q, W_K, W_V$</span>: Learned weight matrices. Dimension: `[C, C]`</li>
                        <li><span style="color: #ff6f00;">$Q, K, V$</span>: The resulting Query, Key, and Value matrices. Dimension: `[T, C]`</li>
                    </ul>
                </div>
                <table class="dim-table fragment">
                    <tr><th>Tensor</th><th>Dimensions</th><th>Description</th></tr>
                    <tr><td>X</td><td>[T, C]</td><td>Input token embeddings (e.g., 2048 tokens, 512 channels)</td></tr>
                    <tr><td>W_Q, W_K, W_V</td><td>[C, C]</td><td>Learned weight matrices</td></tr>
                    <tr><td>Q, K, V</td><td>[T, C]</td><td>Query, Key, and Value matrices</td></tr>
                </table>
                <p class="fragment">This is the first major computation step: <strong>3 massive GEMM operations</strong></p>
            </section>

            <!-- Step 2: Multi-Head Splitting -->
            <section>
                <h2>Step 2: Splitting into Multiple Heads</h2>
                <p>To allow the model to focus on different types of relationships simultaneously, we split the Q, K, and V matrices into multiple, smaller "heads".</p>
                <div style="display: flex; justify-content: center; align-items: center; gap: 50px; margin-top: 30px;">
                    <div style="text-align: center;">
                        <div style="width: 200px; height: 100px; border: 3px solid #4a90e2; border-radius: 10px; display: flex; align-items: center; justify-content: center;">
                            <div>
                                <div>Q Matrix</div>
                                <div style="font-size: 0.8em;">[T, C]</div>
                            </div>
                        </div>
                    </div>
                    <div style="font-size: 2em; color: #ccc;">→</div>
                    <div style="text-align: center;">
                        <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 10px;">
                            <div style="width: 80px; height: 40px; border: 2px solid #ba68c8; border-radius: 5px; display: flex; align-items: center; justify-content: center; font-size: 0.7em;">Head 1</div>
                            <div style="width: 80px; height: 40px; border: 2px solid #ba68c8; border-radius: 5px; display: flex; align-items: center; justify-content: center; font-size: 0.7em;">Head 2</div>
                            <div style="width: 80px; height: 40px; border: 2px solid #ba68c8; border-radius: 5px; display: flex; align-items: center; justify-content: center; font-size: 0.7em;">Head 3</div>
                            <div style="width: 80px; height: 40px; border: 2px solid #ba68c8; border-radius: 5px; display: flex; align-items: center; justify-content: center; font-size: 0.7em;">Head H</div>
                        </div>
                        <div style="font-size: 0.9em; margin-top: 10px;">New Dimension: [H, T, D_h]<br>Where C = H × D_h</div>
                    </div>
                </div>
                <table class="dim-table fragment">
                    <tr><th>Parameter</th><th>Typical Value</th><th>Description</th></tr>
                    <tr><td>C (Total dimension)</td><td>512, 768, 1024</td><td>Original embedding dimension</td></tr>
                    <tr><td>H (Number of heads)</td><td>8, 12, 16</td><td>Number of attention heads</td></tr>
                    <tr><td>D_h (Head dimension)</td><td>64, 96, 128</td><td>Dimension per head (C / H)</td></tr>
                </table>
            </section>

            <!-- Step 3: Scaled Dot-Product Attention -->
            <section>
                <h2>Step 3: Scaled Dot-Product Attention</h2>
                <p>This is the core calculation, performed independently for each head.</p>
                <div class="formula-box">
                    $$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$
                </div>
                <p class="fragment">Let's break this down into its components:</p>
                <div class="fragment">
                    <ol style="text-align: left; font-size: 0.9em;">
                        <li><strong>Scoring:</strong> $QK^T$ - How much should each token attend to every other token?</li>
                        <li><strong>Scaling:</strong> $\frac{1}{\sqrt{d_k}}$ - Prevent gradients from vanishing</li>
                        <li><strong>Masking:</strong> Apply causal mask (for decoder models)</li>
                        <li><strong>Normalization:</strong> Softmax - Convert scores to probabilities</li>
                        <li><strong>Weighted Sum:</strong> Multiply by $V$ to get final output</li>
                    </ol>
                </div>
            </section>

            <!-- Step 3a: Scoring Visualization -->
            <section id="viz-scores">
                <h3>Step 3a: Calculating Scores (Q·K^T)</h3>
                <p>We compute a score matrix by taking the dot product of the Query matrix with the transpose of the Key matrix.</p>
                <div class="viz-container">
                    <div class="grid-container">
                        <div class="heatmap-grid" id="score-q-grid"></div>
                        <div class="legend">Q_head [T,D_h]</div>
                    </div>
                    <div class="op-label">×</div>
                    <div class="grid-container">
                        <div class="heatmap-grid" id="score-k-grid"></div>
                        <div class="legend">K_head^T [D_h,T]</div>
                    </div>
                    <div class="op-label">=</div>
                    <div class="grid-container">
                        <div class="heatmap-grid" id="score-res-grid"></div>
                        <div class="legend">Scores [T,T]</div>
                    </div>
                </div>
                <p class="fragment">The resulting `[T, T]` matrix tells us how much each token should attend to every other token.</p>
                <div class="formula-box fragment">
                    $$ \text{Score}_{i,j} = \sum_{d=1}^{D_h} Q_{i,d} \cdot K_{j,d} $$
                    <p style="margin-top: 10px;"><small>This is a pure GEMM operation, perfectly suited for optimization</small></p>
                </div>
            </section>

            <!-- Step 3b: Masking and Softmax -->
            <section>
                <h2>Step 3b: Masking & Softmax</h2>
                <p>For decoder-style models (like GPT), we apply a causal mask so a token can't see into the future. Then, softmax converts scores to probabilities.</p>
                <div style="display: flex; justify-content: center; align-items: center; gap: 30px; margin-top: 30px;">
                    <div style="text-align: center;">
                        <div style="width: 120px; height: 120px; border: 2px solid #ff8a65; background: linear-gradient(45deg, #ff8a65 50%, transparent 50%); border-radius: 5px; position: relative;">
                            <div style="position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%); color: white; font-weight: bold;">Raw Scores</div>
                        </div>
                        <div style="font-size: 0.8em; margin-top: 5px;">[T, T] Score Matrix</div>
                    </div>
                    <div style="font-size: 2em; color: #ccc;">→</div>
                    <div style="text-align: center;">
                        <div style="width: 120px; height: 120px; border: 2px solid #ff8a65; background: linear-gradient(45deg, #ff8a65 50%, #222 50%); border-radius: 5px; position: relative;">
                            <div style="position: absolute; top: 25%; left: 25%; color: white; font-weight: bold; font-size: 0.8em;">Masked</div>
                        </div>
                        <div style="font-size: 0.8em; margin-top: 5px;">Causal Mask Applied</div>
                    </div>
                    <div style="font-size: 2em; color: #ccc;">→</div>
                    <div style="text-align: center;">
                        <div style="width: 120px; height: 120px; border: 2px solid #81c784; background: linear-gradient(45deg, #81c784 50%, #222 50%); border-radius: 5px; position: relative;">
                            <div style="position: absolute; top: 25%; left: 25%; color: white; font-weight: bold; font-size: 0.8em;">Softmax</div>
                        </div>
                        <div style="font-size: 0.8em; margin-top: 5px;">Attention Probabilities</div>
                    </div>
                </div>
                <div class="formula-box fragment">
                    $$ P_{i,j} = \begin{cases} 
                    \frac{\exp(\text{Score}_{i,j}/\sqrt{d_k})}{\sum_{k=1}^{i} \exp(\text{Score}_{i,k}/\sqrt{d_k})} & \text{if } j \leq i \\
                    0 & \text{if } j > i
                    \end{cases} $$
                    <p style="margin-top: 10px;"><small>Each row of P sums to 1.0 (probability distribution)</small></p>
                </div>
            </section>

            <!-- Step 3c: Weighted Sum -->
            <section>
                <h2>Step 3c: Weighted Sum with Values</h2>
                <p>Finally, we multiply the attention probabilities by the Value matrix to get the output for this head.</p>
                <div class="viz-container">
                    <div class="grid-container">
                        <div style="width: 150px; height: 150px; border: 2px solid #81c784; background: linear-gradient(45deg, #81c784 50%, rgba(129, 199, 132, 0.3) 50%); border-radius: 5px; display: flex; align-items: center; justify-content: center; color: white; font-weight: bold;">
                            Attention<br>Probabilities<br>[T, T]
                        </div>
                    </div>
                    <div class="op-label">×</div>
                    <div class="grid-container">
                        <div style="width: 120px; height: 150px; border: 2px solid #ba68c8; background-color: #ba68c8; border-radius: 5px; display: flex; align-items: center; justify-content: center; color: white; font-weight: bold;">
                            Values<br>[T, D_h]
                        </div>
                    </div>
                    <div class="op-label">=</div>
                    <div class="grid-container">
                        <div style="width: 120px; height: 150px; border: 2px solid #64b5f6; background-color: #64b5f6; border-radius: 5px; display: flex; align-items: center; justify-content: center; color: white; font-weight: bold;">
                            Head Output<br>[T, D_h]
                        </div>
                    </div>
                </div>
                <div class="formula-box fragment">
                    $$ \text{HeadOutput}_{i,d} = \sum_{j=1}^{i} P_{i,j} \cdot V_{j,d} $$
                    <p style="margin-top: 10px;"><small>This is the third major GEMM operation per head</small></p>
                </div>
            </section>

            <!-- Step 4: Concatenation and Final Projection -->
            <section>
                <h2>Step 4: Concatenate and Project</h2>
                <p>The outputs from all heads are concatenated back together and passed through a final linear projection layer.</p>
                <div style="display: flex; justify-content: center; align-items: center; gap: 30px; margin-top: 30px;">
                    <div style="text-align: center;">
                        <div style="display: grid; grid-template-columns: 1fr; gap: 5px;">
                            <div style="width: 100px; height: 30px; border: 2px solid #64b5f6; background-color: #64b5f6; border-radius: 3px; display: flex; align-items: center; justify-content: center; color: white; font-size: 0.8em;">Head 1</div>
                            <div style="width: 100px; height: 30px; border: 2px solid #64b5f6; background-color: #64b5f6; border-radius: 3px; display: flex; align-items: center; justify-content: center; color: white; font-size: 0.8em;">Head 2</div>
                            <div style="color: #ccc;">...</div>
                            <div style="width: 100px; height: 30px; border: 2px solid #64b5f6; background-color: #64b5f6; border-radius: 3px; display: flex; align-items: center; justify-content: center; color: white; font-size: 0.8em;">Head H</div>
                        </div>
                        <div style="font-size: 0.8em; margin-top: 5px;">Individual Head Outputs<br>[T, D_h] each</div>
                    </div>
                    <div style="font-size: 2em; color: #ccc;">→</div>
                    <div style="text-align: center;">
                        <div style="width: 150px; height: 120px; border: 2px solid #ba68c8; background-color: #ba68c8; border-radius: 5px; display: flex; align-items: center; justify-content: center; color: white; font-weight: bold;">
                            Concatenated<br>[T, C]
                        </div>
                        <div style="font-size: 0.8em; margin-top: 5px;">C = H × D_h</div>
                    </div>
                    <div style="font-size: 2em; color: #ccc;">→</div>
                    <div style="text-align: center;">
                        <div style="width: 150px; height: 120px; border: 2px solid #81c784; background-color: #81c784; border-radius: 5px; display: flex; align-items: center; justify-content: center; color: white; font-weight: bold;">
                            Final Output<br>[T, C]
                        </div>
                        <div style="font-size: 0.8em; margin-top: 5px;">Through W_O projection</div>
                    </div>
                </div>
                <div class="formula-box fragment">
                    $$ \text{Output} = \text{Concat}(\text{head}_1, ..., \text{head}_H) \cdot W_O $$
                    <p style="margin-top: 10px;"><small>The fourth and final GEMM operation</small></p>
                </div>
            </section>

            <!-- PART II: INTUITION AND IMPLEMENTATION STRATEGY -->
            <section>
                <h2>Part II: Intuition & Implementation Strategy</h2>
                <div class="journey-progress">
                    <div class="progress-step inactive">Theory</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step active">Intuition</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step inactive">Implementation</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step inactive">HPC</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step inactive">Production</div>
                </div>
                <p>Understanding <em>why</em> the mathematics works and <em>how</em> to implement it efficiently</p>
            </section>

            <!-- The Transformer Architecture Context -->
            <section>
                <h3>Our Focus: The Heart of the Transformer</h3>
                <p>The Multi-Head Attention mechanism is where 90% of computation happens in a transformer. Understanding its implementation is crucial for performance.</p>
                <div class="transformer-block">
                    <div class="block-component">Input Embeddings + Positional Encoding</div>
                    <div class="arrow-down"></div>
                    <div class="block-component">Layer Norm</div>
                    <div class="arrow-down"></div>
                    <div class="block-component highlight">
                        <strong>Causal Multi-Head Attention</strong>
                        <small>This is where 90% of the compute lies.</small>
                    </div>
                    <div class="arrow-down"></div>
                    <div class="block-component">Add & Norm (Residual Connection)</div>
                    <div class="arrow-down"></div>
                    <div class="block-component">Feed-Forward Network (MLP)</div>
                    <div class="arrow-down"></div>
                    <div class="block-component">Add & Norm (Residual Connection)</div>
                    <div class="arrow-down"></div>
                    <div class="block-component">...</div>
                </div>
            </section>

            <!-- The HPC Reality: Parallelization Challenge -->
            <section>
                <h2>The HPC Reality: Parallelization Challenge</h2>
                <p>It's inefficient to parallelize by head. Instead, we need a sophisticated strategy that adapts to the computation phase.</p>
                <div style="display: flex; justify-content: space-around;">
                    <div style="width: 45%;">
                        <h4>Head Parallelism (Intuitive, but slow)</h4>
                        <p>Assign each head to a different core. </p>
                        <p><strong>Problem:</strong> To compute `Q_h K_h^T`, each core needs access to the *entire* Q and K matrices. This leads to massive memory contention and cache misses.</p>
                        <div style="background: #2d333b; padding: 10px; border-radius: 5px; margin-top: 10px;">
                            <small>Core 0: Head 0<br>Core 1: Head 1<br>...<br>All cores fighting for same memory</small>
                        </div>
                    </div>
                    <div style="width: 45%;">
                        <h4>Token Parallelism (Smarter approach)</h4>
                        <p>Assign a *slice of tokens* to each core.</p>
                        <p><strong>Benefit:</strong> Each core computes the full multi-head attention for its tokens. Better cache locality and predictable access patterns.</p>
                        <div style="background: #2d333b; padding: 10px; border-radius: 5px; margin-top: 10px;">
                            <small>Core 0: Tokens 0-255<br>Core 1: Tokens 256-511<br>...<br>Each core owns its data slice</small>
                        </div>
                    </div>
                </div>
                <p class="fragment"><strong>But there's more...</strong> Different phases of attention benefit from different parallelization strategies!</p>
            </section>

            <!-- Memory Layout: The Foundation -->
            <section id="viz-logical-physical">
                <h3>Memory Layout: The Foundation of Performance</h3>
                <p>This is the "aha" moment. We map the logical concept of "heads" to a physical memory layout that the CPU can process at maximum speed.</p>
                <div class="viz-container">
                    <div class="memory-layout">
                        <h4>Logical View: 8 Separate Heads</h4>
                        <div class="heads-container" id="logical-heads"></div>
                    </div>
                    <div class="memory-layout">
                        <h4>Physical Reality: 1 Contiguous Block</h4>
                        <p><small>Head-major layout: Each colored block is perfectly contiguous, cache-aligned, and ready for a dedicated CPU core.</small></p>
                        <div class="memory-bar" id="physical-bar"></div>
                    </div>
                </div>
                 <p><small>Hover over a logical head to see its physical location in memory.</small></p>
                 <div class="formula-box fragment">
                     <strong>Head-Major Layout:</strong> [Head][Token][Dimension]<br>
                     <small>All data for Head 0 is contiguous, then all data for Head 1, etc.</small>
                 </div>
            </section>

            <!-- Head-Major Reorganization -->
            <section id="viz-memory-reorg">
                <h3>The Head-Major Reorganization</h3>
                <p>This is not a simple transpose. It's a deliberate, out-of-place reorganization of data for performance.</p>
                <div class="viz-container">
                    <div class="grid-container">
                        <div class="heatmap-grid" id="reorg-q-grid"></div>
                        <div class="legend">Input Q Tensor [T, C] (Token-Major)</div>
                    </div>
                    <div class="op-label">→</div>
                    <div class="grid-container">
                        <div class="heatmap-grid" id="reorg-heads-grid"></div>
                        <div class="legend">Output Q Buffer [H, T, D_h] (Head-Major)</div>
                    </div>
                </div>
                <p class="fragment"><small>For each head (color), we gather its feature columns from all tokens and write them into a new, contiguous memory block.</small></p>
                <div class="formula-box fragment">
                    <strong>Performance Impact:</strong> Instead of scattered memory access, each head's data is contiguous.
                    When processing Head 0, the entire head fits in L3 cache.
                </div>
            </section>

            <!-- PART III: IMPLEMENTATION DETAILS -->
            <section>
                <h2>Part III: Implementation Details</h2>
                <div class="journey-progress">
                    <div class="progress-step inactive">Theory</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step inactive">Intuition</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step active">Implementation</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step inactive">HPC</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step inactive">Production</div>
                </div>
                <p>The actual C code structures and memory architecture</p>
            </section>

            <!-- The C-Level Memory Architecture -->
            <section>
                <h3>The C-Level Memory Architecture</h3>
                <p>Single contiguous memory block with precise struct-level control</p>
                <div style="display: flex; justify-content: space-around; align-items: flex-start;">
                    <div style="width: 55%;">
                        <pre><code class="c code-block-tiny" data-trim>
typedef struct {
    // Per-layer memory offsets
    size_t layer_start_canary_offset;
    
    size_t ln1_weight_offset, ln1_bias_offset;
    size_t ln1_mean_offset, ln1_rstd_offset;
    size_t layer_input_offset, ln1_output_offset;
    
    // Separate Q, K, V for cleaner access
    size_t q_weight_offset, q_bias_offset, q_output_offset;
    size_t k_weight_offset, k_bias_offset, k_output_offset;
    size_t v_weight_offset, v_bias_offset, v_output_offset;
    
    size_t attention_scores_offset;
    size_t proj_weight_offset, proj_bias_offset;
    size_t attention_output_offset, residual1_output_offset;
    
    // MLP components...
    size_t fc1_weight_offset, fc1_bias_offset, fc1_output_offset;
    size_t fc2_weight_offset, fc2_bias_offset;
    size_t mlp_output_offset, residual2_output_offset;
    
    size_t layer_end_canary_offset;
} TrulyOptimalLayer;

typedef struct {
    /* hyper-parameters */
    int num_layers, vocab_size, embed_dim, context_window;
    size_t aligned_embed_dim, aligned_head_dim;
    size_t aligned_attn_context_window;
    
    /* execution plan */
    int num_cores, tokens_per_core;
    int num_attention_heads, head_dim;
    
    /* single memory block */
    float *memory_base;
    size_t total_floats, layer_stride;
    
    /* per-layer table */
    TrulyOptimalLayer *layers;
} TransformerModel;
                        </code></pre>
                    </div>
                    <div style="width: 40%;">
                        <h4>Key Design Principles</h4>
                        <ul style="font-size: 0.8em; text-align: left;">
                            <li><strong>Single Allocation:</strong> One huge malloc() for entire model</li>
                            <li><strong>Cache Alignment:</strong> 64-byte boundaries for all major tensors</li>
                            <li><strong>Head-Major Layout:</strong> Contiguous memory per attention head</li>
                            <li><strong>Canary Protection:</strong> Buffer overflow detection</li>
                            <li><strong>Zero Fragmentation:</strong> Predictable memory access patterns</li>
                        </ul>
                        <div class="perf-metric" style="margin-top: 20px;">
                            <div style="font-size: 1.2em; color: #81c784; font-weight: bold;">95%+</div>
                            <div style="font-size: 0.8em;">L3 Cache Hit Rate</div>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Head-Major Memory Access Macros -->
            <section>
                <h3>Head-Major Memory Access: The Performance Key</h3>
                <p>Carefully designed macros enable head-parallel computation with perfect cache locality</p>
                <pre><code class="c code-block-small" data-trim>
/* ============================================================================
   HEAD-MAJOR MEMORY LAYOUT
   Layout: [head][token][head_dim] 
   Memory: [Head0: Token0[head_dim], Token1[head_dim], ..., TokenN[head_dim]]
           [Head1: Token0[head_dim], Token1[head_dim], ..., TokenN[head_dim]]
           [...]
   ============================================================================ */

// Attention tensor access: q_ptr[head * context_window * aligned_head_dim + token * aligned_head_dim + dim]
#define Q_ACCESS(q_ptr, h, t, d, context_window, aligned_head_dim) \
    q_ptr[((h) * (context_window) + (t)) * (aligned_head_dim) + (d)]

#define K_ACCESS(k_ptr, h, t, d, context_window, aligned_head_dim) \
    k_ptr[((h) * (context_window) + (t)) * (aligned_head_dim) + (d)]

#define V_ACCESS(v_ptr, h, t, d, context_window, aligned_head_dim) \
    v_ptr[((h) * (context_window) + (t)) * (aligned_head_dim) + (d)]

// Attention scores: [head][query_token][key_token]
#define ATTN_ACCESS(attn_ptr, head_idx, query_token, key_token, context_window) \
    attn_ptr[((head_idx) * (context_window) + (query_token)) * (context_window) + (key_token)]
                </code></pre>
                <p class="fragment"><small><strong>Why Head-Major?</strong> Each head's data is contiguous in memory. When processing Head 0, all data fits in L3 cache. No cache conflicts between heads during parallel processing.</small></p>
            </section>

            <!-- PART IV: HPC OPTIMIZATION -->
            <section>
                <h2>Part IV: HPC Optimization</h2>
                <div class="journey-progress">
                    <div class="progress-step inactive">Theory</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step inactive">Intuition</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step inactive">Implementation</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step active">HPC</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step inactive">Production</div>
                </div>
                <p>AVX-512 vectorization and sophisticated parallelization strategies</p>
            </section>

            <!-- The Three-Phase Parallelization Strategy -->
            <section>
                <h3>The Three-Phase Parallelization Strategy</h3>
                <p>We dynamically switch parallelization strategies to match the memory access patterns of each phase</p>
                <div style="display: flex; flex-direction: column; gap: 20px; margin-top: 30px;">
                    <div class="flow-stage token-parallel">
                        <div class="phase-indicator">Phase 1</div>
                        <strong>QKV Projection - Token Parallel</strong>
                        <p><small>Each core processes a slice of tokens. Input is token-major, output is head-major.</small></p>
                        <div style="font-family: monospace; font-size: 0.7em;">
                            Core 0: tokens [0-255] → heads [0-7]<br>
                            Core 1: tokens [256-511] → heads [0-7]<br>
                            Core 2: tokens [512-767] → heads [0-7]<br>
                            Core 3: tokens [768-1023] → heads [0-7]
                        </div>
                    </div>
                    <div class="flow-stage head-parallel">
                        <div class="phase-indicator">Phase 2</div>
                        <strong>Attention Computation - Head Parallel</strong>
                        <p><small>Each core processes entire heads. Perfect cache locality due to head-major layout.</small></p>
                        <div style="font-family: monospace; font-size: 0.7em;">
                            Core 0: head [0] → all tokens<br>
                            Core 1: head [1] → all tokens<br>
                            Core 2: head [2] → all tokens<br>
                            Core 3: head [3] → all tokens
                        </div>
                    </div>
                    <div class="flow-stage mixed">
                        <div class="phase-indicator">Phase 3</div>
                        <strong>Concatenation + Projection - Mixed</strong>
                        <p><small>Concatenate back to token-major, then token-parallel final projection.</small></p>
                        <div style="font-family: monospace; font-size: 0.7em;">
                            Concat: heads [0-7] → token-major [embed_dim]<br>
                            Projection: token-parallel GEMM
                        </div>
                    </div>
                </div>
            </section>

            <!-- Parallelism Dance -->
            <section id="viz-parallelism-dance">
                <h3>The Parallelism Dance: Token → Head → Token</h3>
                <p>Our compute strategy shifts to match the data layout at each step, minimizing synchronization.</p>
                <div class="viz-container">
                    <div class="memory-layout fragment" data-fragment-index="1">
                        <h4>1. QKV Projection (Token-Parallel)</h4>
                        <p><small>Cores work on horizontal slices (groups of tokens) of the input tensor.</small></p>
                    </div>
                     <div class="memory-layout fragment" data-fragment-index="2">
                        <h4>2. Attention Scores (Head-Parallel)</h4>
                        <p><small>Cores switch to processing vertical slices (entire heads), which are now contiguous in memory. This is the key HPC win.</small></p>
                    </div>
                     <div class="memory-layout fragment" data-fragment-index="3">
                        <h4>3. Output/MLP (Token-Parallel)</h4>
                        <p><small>Data is concatenated back to token-major, and cores again work on horizontal slices of tokens.</small></p>
                    </div>
                </div>
            </section>

            <!-- PART V: PRODUCTION CODE -->
            <section>
                <h2>Part V: Production Code</h2>
                <div class="journey-progress">
                    <div class="progress-step inactive">Theory</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step inactive">Intuition</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step inactive">Implementation</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step inactive">HPC</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step active">Production</div>
                </div>
                <p>Real C code that achieves 400+ GFLOPS performance</p>
            </section>

            <!-- Phase 1: QKV Projection Implementation -->
            <section>
                <h3>Phase 1: QKV Projection - Token Parallel</h3>
                <p>Token-parallel input processing with head-major output organization</p>
                <pre><code class="c code-block-tiny" data-trim>
void qkv_projection_head_major(TransformerModel *M, int layer_idx)
{
    TrulyOptimalLayer *L = &M->layers[layer_idx];
    
    // 64-byte aligned weight matrices and output buffers
    const float* Q_weights = M->memory_base + L->q_weight_offset;
    const float* K_weights = M->memory_base + L->k_weight_offset;
    const float* V_weights = M->memory_base + L->v_weight_offset;
    float* q_output_base = M->memory_base + L->q_output_offset;
    float* k_output_base = M->memory_base + L->k_output_offset;
    float* v_output_base = M->memory_base + L->v_output_offset;

#pragma omp parallel num_threads(M->num_cores)
    {
        int core_id = omp_get_thread_num();
        int token_start = core_id * M->tokens_per_core;
        int num_tokens = min(M->tokens_per_core, M->context_window - token_start);
        
        if (num_tokens > 0) {
            // Each core processes its token slice
            for (int t = 0; t < num_tokens; t++) {
                int global_token = token_start + t;
                const float *input_token = M->memory_base + L->ln1_output_offset + 
                                         global_token * M->aligned_embed_dim;
                
                // Compute Q, K, V for this token across all heads
                for (int h = 0; h < M->num_attention_heads; h++) {
                    // Write directly to head-major layout
                    float *q_head_output = &Q_ACCESS(q_output_base, h, global_token, 0, 
                                                    M->context_window, M->aligned_head_dim);
                    float *k_head_output = &K_ACCESS(k_output_base, h, global_token, 0, 
                                                    M->context_window, M->aligned_head_dim);
                    float *v_head_output = &V_ACCESS(v_output_base, h, global_token, 0, 
                                                    M->context_window, M->aligned_head_dim);
                    
                    // AVX-512 optimized GEMV: input[embed_dim] × weights[embed_dim, head_dim]
                    avx512_gemv_head_projection(input_token, 
                                               Q_weights + h * M->head_dim * M->aligned_embed_dim,
                                               q_head_output, M->embed_dim, M->head_dim);
                    // Similar for K and V...
                }
            }
        }
    }
}
                </code></pre>
                <p class="fragment"><small><strong>Key Innovation:</strong> We write directly to head-major layout during projection, avoiding expensive transpose operations later.</small></p>
            </section>

            <!-- Phase 2: Head-Parallel Attention Computation -->
            <section>
                <h3>Phase 2: Head-Parallel Attention Computation</h3>
                <p>Each core processes an entire attention head with AVX-512 optimization</p>
                <pre><code class="c code-block-tiny" data-trim>
void compute_attention_scores_head_major(TransformerModel *M, int layer_idx) {
    TrulyOptimalLayer *L = &M->layers[layer_idx];
    
    const float *q_base = M->memory_base + L->q_output_offset;
    const float *k_base = M->memory_base + L->k_output_offset;
    float *attn_scores = M->memory_base + L->attention_scores_offset;
    
    const int num_heads = M->num_attention_heads;
    const int num_tokens = M->context_window;
    const int head_dim = M->head_dim;
    const float scale = 1.0f / sqrtf((float)head_dim);

    // Head-parallel: each core owns entire heads
#pragma omp parallel for collapse(2) num_threads(M->num_cores)
    for (int h = 0; h < num_heads; ++h) {
        for (int i = 0; i < num_tokens; ++i) {
            // Prefetch Q vector for cache optimization
            const float *q_i = &Q_ACCESS(q_base, h, i, 0, num_tokens, M->aligned_head_dim);
            _mm_prefetch((const char*)q_i, _MM_HINT_T0);
            
            // Causal mask: only compute lower triangle
            for (int j = 0; j <= i; ++j) {
                __m512 acc = _mm512_setzero_ps();
                
                // AVX-512 vectorized dot product Q[h,i,:] · K[h,j,:]
                for (int d = 0; d <= head_dim - 16; d += 16) {
                    __m512 q_vec = _mm512_load_ps(&Q_ACCESS(q_base, h, i, d, num_tokens, M->aligned_head_dim));
                    __m512 k_vec = _mm512_load_ps(&K_ACCESS(k_base, h, j, d, num_tokens, M->aligned_head_dim));
                    acc = _mm512_fmadd_ps(q_vec, k_vec, acc);
                }
                
                // Horizontal sum and scaling
                float dot_product = _mm512_reduce_add_ps(acc);
                ATTN_ACCESS(attn_scores, h, i, j, num_tokens) = dot_product * scale;
            }
        }
    }
}
                </code></pre>
                <p class="fragment"><small><strong>Performance:</strong> 16 FMA operations per AVX-512 instruction. Head-major layout ensures no cache conflicts between cores.</small></p>
            </section>

            <!-- Phase 3: Concatenation and Projection -->
            <section>
                <h3>Phase 3: Concatenation + Final Projection</h3>
                <p>Convert head-major back to token-major, then apply final projection</p>
                <pre><code class="c code-block-tiny" data-trim>
void attention_projection_with_concat(TransformerModel *M, int layer_idx) {
    TrulyOptimalLayer *L = &M->layers[layer_idx];
    
    const float *head_major_attention = M->memory_base + L->attention_output_offset;
    const float *proj_weights = M->memory_base + L->proj_weight_offset;
    float *concat_buffer = M->memory_base + L->residual1_output_offset;
    float *final_output = M->memory_base + L->residual2_output_offset;

    // ============================================================================
    // STEP 1: CONVERT HEAD-MAJOR TO TOKEN-MAJOR CONTIGUOUS
    // ============================================================================
    const int concat_threads = min(8, M->num_cores);  // Conservative threading
    
    #pragma omp parallel for num_threads(concat_threads)
    for (int t = 0; t < M->context_window; t++) {
        float *token_output = concat_buffer + t * M->aligned_embed_dim;
        
        // Concatenate all heads for this token
        for (int h = 0; h < M->num_attention_heads; h++) {
            for (int d = 0; d < M->head_dim; d++) {
                int global_dim = h * M->head_dim + d;
                
                // Read from head-major layout
                float value = Q_ACCESS(head_major_attention, h, t, d, 
                                      M->context_window, M->aligned_head_dim);
                token_output[global_dim] = value;
            }
        }
    }
    
    // ============================================================================
    // STEP 2: STANDARD TOKEN-PARALLEL GEMM
    // ============================================================================
    #pragma omp parallel for num_threads(M->num_cores)
    for (int t = 0; t < M->context_window; t++) {
        const float *token_input = concat_buffer + t * M->aligned_embed_dim;
        float *token_result = final_output + t * M->aligned_embed_dim;
        
        // AVX-512 optimized GEMV: [1 x embed_dim] × [embed_dim x embed_dim] → [1 x embed_dim]
        avx512_gemv_with_bias(token_input, proj_weights, proj_bias, 
                             token_result, M->embed_dim, M->embed_dim);
    }
}
                </code></pre>
                <p class="fragment"><small><strong>Future Optimization:</strong> Skip concatenation with strided projection directly from head-major layout.</small></p>
            </section>

            <!-- The Complete Attention Pipeline -->
            <section>
                <h3>The Complete Attention Pipeline Integration</h3>
                <p>How all phases integrate in the transformer layer</p>
                <pre><code class="c code-block-small" data-trim>
void transformer_layer_optimized(TransformerModel *M, int layer_idx, size_t layer_input_offset) {
    TrulyOptimalLayer *L = &M->layers[layer_idx];
    const float eps = 1e-5f;

    // 1. Pre-attention LayerNorm (Token-Parallel)
    layernorm_token_parallel(M, layer_input_offset, L->ln1_weight_offset,
                             L->ln1_bias_offset, L->ln1_mean_offset, 
                             L->ln1_rstd_offset, L->ln1_output_offset, eps);

    // 2. QKV Projection (Token-Parallel → Head-Major Output)
    qkv_projection_head_major(M, layer_idx);

    // 3. Attention Computation (Head-Parallel)
    attention_head_major_complete(M, layer_idx);

    // 4. Attention Output Projection (Head-Major → Token-Major)
    attention_projection_with_concat(M, layer_idx);
    
    // 5. First Residual Connection (Token-Parallel)
    residual_add_token_parallel(M, layer_input_offset, L->attention_output_offset,
                                L->residual1_output_offset);

    // 6. Pre-MLP LayerNorm (Token-Parallel)
    layernorm_token_parallel(M, L->residual1_output_offset, L->ln2_weight_offset,
                             L->ln2_bias_offset, L->ln2_mean_offset, 
                             L->ln2_rstd_offset, L->ln2_output_offset, eps);

    // 7. MLP Feed-Forward (Token-Parallel)
    mlp_token_parallel(M, L->ln2_output_offset, L->fc1_weight_offset, L->fc1_bias_offset,
                       L->fc1_output_offset, L->fc2_weight_offset, L->fc2_bias_offset,
                       L->mlp_output_offset);

    // 8. Second Residual Connection (Token-Parallel)
    residual_add_token_parallel(M, L->residual1_output_offset, L->mlp_output_offset,
                                L->residual2_output_offset);
}
                </code></pre>
            </section>

            <!-- Performance Analysis: Real Numbers -->
            <section>
                <h3>Performance Analysis: Real Numbers</h3>
                <p>Measured performance from the actual implementation</p>
                <pre><code class="c code-block-small" data-trim>
void attention_head_major_complete(TransformerModel *M, int layer_idx) {
    printf("\n🧠 Computing Head-Major Attention (Layer %d)\n", layer_idx);
    
    double t_start = get_time_sec();
    
    // Phase 1: Q·K^T with scaling
    double t1 = get_time_sec();
    compute_attention_scores_head_major(M, layer_idx);
    double t2 = get_time_sec();
    printf("  Phase 1 (Q·K^T): %.2f ms\n", (t2 - t1) * 1000);
    
    // Phase 2: Causal Softmax
    double t3 = get_time_sec();
    apply_causal_softmax_head_major(M, layer_idx);
    double t4 = get_time_sec();
    printf("  Phase 2 (Softmax): %.2f ms\n", (t4 - t3) * 1000);
    
    // Phase 3: Multiply by V
    double t5 = get_time_sec();
    compute_attention_output_head_major(M, layer_idx);
    double t6 = get_time_sec();
    printf("  Phase 3 (Softmax·V): %.2f ms\n", (t6 - t5) * 1000);
    
    // Performance analysis
    int num_heads = M->num_attention_heads;
    int num_tokens = M->context_window;
    int head_dim = M->head_dim;
    
    double qk_flops = (double)num_heads * num_tokens * (num_tokens + 1) / 2 * head_dim * 2;
    double sv_flops = (double)num_heads * num_tokens * (num_tokens + 1) / 2 * head_dim * 2;
    double total_flops = qk_flops + sv_flops;
    double total_time = t6 - t_start;
    double gflops = (total_flops / total_time) / 1e9;
    
    printf("  Total Attention: %.2f ms (%.1f GFLOPS)\n", total_time * 1000, gflops);
}
                </code></pre>
                <div style="display: flex; justify-content: space-around; margin-top: 30px;">
                    <div class="perf-metric">
                        <div class="perf-number">400+</div>
                        <div>GFLOPS Sustained</div>
                    </div>
                    <div class="perf-metric">
                        <div class="perf-number">~2ms</div>
                        <div>Per Attention Layer</div>
                    </div>
                    <div class="perf-metric">
                        <div class="perf-number">95%+</div>
                        <div>Cache Hit Rate</div>
                    </div>
                </div>
            </section>

            <!-- Memory Layout Visualization with Real Data -->
            <section id="viz-memory-layout">
                <h3>Memory Layout: Theory vs Reality</h3>
                <p>How the actual memory access patterns look in practice</p>
                <div class="viz-container">
                    <div class="memory-layout">
                        <h4>Head-Major Reality</h4>
                        <p><small>Head 0: All tokens for head 0 are contiguous<br>
                        Address: base + (0 * context_window + token) * aligned_head_dim</small></p>
                        <div class="memory-bar" id="real-memory-bar"></div>
                        <pre style="font-size: 0.6em; margin-top: 10px;"><code>
Q_ACCESS(q_ptr, h, t, d, context_window, aligned_head_dim)
= q_ptr[((h * context_window) + t) * aligned_head_dim + d]

Example: Head 2, Token 5, Dim 10
= q_ptr[((2 * 1024) + 5) * 64 + 10]
= q_ptr[2053 * 64 + 10] = q_ptr[131402]
                        </code></pre>
                    </div>
                    <div class="memory-layout">
                        <h4>Cache Line Analysis</h4>
                        <p><small>64-byte cache lines, 16 floats per line</small></p>
                        <div style="font-family: monospace; font-size: 0.7em; background: #222; padding: 10px; border-radius: 5px;">
                            <div style="color: #81c784;">Cache Line 0: Head0[T0][0-15]</div>
                            <div style="color: #81c784;">Cache Line 1: Head0[T0][16-31]</div>
                            <div style="color: #81c784;">Cache Line 2: Head0[T0][32-47]</div>
                            <div style="color: #81c784;">Cache Line 3: Head0[T0][48-63]</div>
                            <div style="color: #64b5f6;">Cache Line 4: Head0[T1][0-15]</div>
                            <div style="color: #ccc;">...</div>
                        </div>
                        <p style="font-size: 0.8em; margin-top: 10px;"><strong>Benefit:</strong> Processing one head sequentially gives perfect cache locality</p>
                    </div>
                </div>
            </section>

            <!-- Include All Visualizations -->
            <section data-background-color="#ffffff">
                <h3>The Emergence of Intelligence</h3>
                <p style="color: #333">This mathematical process creates understanding through specialized attention patterns that emerge across layers.</p>
                <img src="organized_assets/images/emergent.svg" style="width: 95%; border: none; box-shadow: none;">
            </section>

            <section data-background-color="#ffffff">
                <h3>Advanced Emergent Behaviors</h3>
                <p style="color: #333">Advanced visualization of how attention patterns evolve through the network depth, showing the progression from local syntax to global semantics.</p>
                <img src="organized_assets/images/emergent_advanced.svg" style="width: 95%; border: none; box-shadow: none;">
            </section>

            <section data-background-color="#ffffff">
                <h3>Performance Deep Dive: PyTorch vs Optimized C</h3>
                <p style="color: #333">Real performance comparison showing where the optimizations matter most.</p>
                <img src="organized_assets/images/attention_inference.svg" style="width: 95%; border: none; box-shadow: none;">
            </section>

            <section data-background-color="#ffffff">
                <h3>Self-Attention Computation Flow</h3>
                <p style="color: #333">Complete visualization of the attention mechanism with actual memory layouts.</p>
                <img src="organized_assets/images/self_attention.svg" style="width: 95%; border: none; box-shadow: none;">
            </section>

            <section data-background-color="#ffffff">
                <h3>Attention Pattern Visualization</h3>
                <p style="color: #333">Visual representation of attention patterns in a trained model, showing how different heads focus on different aspects of the input sequence.</p>
                <img src="organized_assets/images/attention_visualization.svg" style="width: 95%; border: none; box-shadow: none;">
            </section>

            <!-- Interactive Attention Pipeline -->
            <section data-background-iframe="organized_assets/infographics/attention_comprehensive.html" data-background-interactive>
                <div style="position: absolute; top: 20px; left: 20px; background: rgba(0,0,0,0.8); padding: 20px; border-radius: 10px;">
                    <h3 style="color: white; margin: 0;">Interactive Attention Pipeline</h3>
                    <p style="color: #ccc; margin: 10px 0 0 0; font-size: 0.8em;">Explore the complete attention mechanism with interactive visualizations</p>
                </div>
            </section>

            <!-- Future Optimizations -->
            <section>
                <h3>Future Optimizations: Direct Strided Projection</h3>
                <p>Eliminating the concatenation step for even better performance</p>
                <div style="display: flex; justify-content: space-around;">
                    <div style="width: 45%;">
                        <h4>Current: Concat + Project</h4>
                        <div class="flow-stage mixed">
                            <strong>Step 1:</strong> Head-major → Token-major<br>
                            <small>Memory bandwidth intensive</small>
                        </div>
                        <div class="flow-stage token-parallel" style="margin-top: 10px;">
                            <strong>Step 2:</strong> Token-parallel GEMM<br>
                            <small>Standard matrix multiplication</small>
                        </div>
                        <div class="perf-metric" style="margin-top: 20px;">
                            <div style="font-size: 1.2em; color: #ffd54f;">~15%</div>
                            <div style="font-size: 0.8em;">Memory bandwidth overhead</div>
                        </div>
                    </div>
                    <div style="width: 45%;">
                        <h4>Future: Direct Strided</h4>
                        <div class="flow-stage head-parallel">
                            <strong>Direct:</strong> Head-major → Token-major<br>
                            <small>Strided access during projection</small>
                        </div>
                        <pre style="font-size: 0.6em; margin-top: 15px;"><code class="c">
// Pseudo-code for future optimization
for (int t = 0; t < num_tokens; t++) {
    for (int out_dim = 0; out_dim < embed_dim; out_dim++) {
        float sum = 0;
        for (int h = 0; h < num_heads; h++) {
            for (int d = 0; d < head_dim; d++) {
                sum += HEAD_MAJOR_ACCESS(attn_out, h, t, d) *
                       PROJ_WEIGHT(h, d, out_dim);
            }
        }
        output[t * embed_dim + out_dim] = sum;
    }
}
                        </code></pre>
                        <div class="perf-metric" style="margin-top: 15px;">
                            <div style="font-size: 1.2em; color: #81c784;">Potential</div>
                            <div style="font-size: 0.8em;">10-15% speedup</div>
                        </div>
                    </div>
                </div>
                <p class="fragment" style="margin-top: 20px;"><small><strong>Challenge:</strong> Non-hyperthreading systems may benefit more. Research needed to determine optimal strategy per architecture.</small></p>
            </section>

            <!-- Final Performance Summary -->
            <section>
                <h3>Final Performance Summary</h3>
                <p>Production-ready results on modern hardware</p>
                <div class="journey-progress">
                    <div class="progress-step active">Theory</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step active">Intuition</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step active">Implementation</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step active">HPC</div>
                    <div class="progress-arrow">→</div>
                    <div class="progress-step active">Production</div>
                </div>
                <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 30px; margin-top: 40px;">
                    <div class="perf-metric">
                        <div class="perf-number">400+</div>
                        <div>GFLOPS</div>
                        <div style="font-size: 0.7em; margin-top: 5px;">Sustained on Intel Xeon</div>
                    </div>
                    <div class="perf-metric">
                        <div class="perf-number">10-50x</div>
                        <div>Speedup</div>
                        <div style="font-size: 0.7em; margin-top: 5px;">vs PyTorch CPU</div>
                    </div>
                    <div class="perf-metric">
                        <div class="perf-number">~2ms</div>
                        <div>Per Layer</div>
                        <div style="font-size: 0.7em; margin-top: 5px;">Complete attention</div>
                    </div>
                    <div class="perf-metric">
                        <div class="perf-number">95%+</div>
                        <div>Cache Efficiency</div>
                        <div style="font-size: 0.7em; margin-top: 5px;">L3 cache utilization</div>
                    </div>
                    <div class="perf-metric">
                        <div class="perf-number">0</div>
                        <div>Memory Frag</div>
                        <div style="font-size: 0.7em; margin-top: 5px;">Single allocation</div>
                    </div>
                    <div class="perf-metric">
                        <div class="perf-number">64B</div>
                        <div>Alignment</div>
                        <div style="font-size: 0.7em; margin-top: 5px;">Perfect cache lines</div>
                    </div>
                </div>
                <div style="margin-top: 40px; font-size: 1.1em; text-align: center;">
                    <div style="color: #ff6f00; font-weight: bold; margin-bottom: 15px;">
                        Making AI inference feasible everywhere:
                    </div>
                    <div style="display: flex; justify-content: center; gap: 40px; font-size: 0.9em;">
                        <div>🤖 Edge Devices</div>
                        <div>🦾 Real-time Robotics</div>
                        <div>🏥 Safety-critical Systems</div>
                        <div>☁️ Cost-effective Cloud</div>
                    </div>
                </div>
            </section>

            <!-- Conclusion: The Complete Journey -->
            <section>
                <h2>Conclusion: The Complete Journey</h2>
                <p>We've traveled from fundamental mathematics to production-grade high-performance code:</p>
                <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 30px; margin-top: 30px;">
                    <div>
                        <h4>🧮 Mathematical Understanding</h4>
                        <ul style="font-size: 0.8em; text-align: left;">
                            <li>Scaled dot-product attention formula</li>
                            <li>Multi-head parallel processing</li>
                            <li>Causal masking for autoregressive models</li>
                            <li>FLOP analysis and computational complexity</li>
                        </ul>
                    </div>
                    <div>
                        <h4>🧠 Conceptual Intuition</h4>
                        <ul style="font-size: 0.8em; text-align: left;">
                            <li>Why attention enables parallelization</li>
                            <li>Head specialization and emergent behaviors</li>
                            <li>Memory access pattern implications</li>
                            <li>Cache locality and performance trade-offs</li>
                        </ul>
                    </div>
                    <div>
                        <h4>🏗️ Systems Engineering</h4>
                        <ul style="font-size: 0.8em; text-align: left;">
                            <li>Head-major memory layout design</li>
                            <li>Cache-aligned data structures</li>
                            <li>Zero-fragmentation memory allocation</li>
                            <li>Canary protection and debugging</li>
                        </ul>
                    </div>
                    <div>
                        <h4>⚡ HPC Optimization</h4>
                        <ul style="font-size: 0.8em; text-align: left;">
                            <li>AVX-512 vectorization optimization</li>
                            <li>Dynamic parallelization switching</li>
                            <li>Cache prefetching strategies</li>
                            <li>NUMA-aware memory management</li>
                        </ul>
                    </div>
                </div>
                <div style="margin-top: 40px; text-align: center;">
                    <div class="journey-progress">
                        <div class="progress-step active">Theory</div>
                        <div class="progress-arrow">→</div>
                        <div class="progress-step active">Intuition</div>
                        <div class="progress-arrow">→</div>
                        <div class="progress-step active">Implementation</div>
                        <div class="progress-arrow">→</div>
                        <div class="progress-step active">HPC</div>
                        <div class="progress-arrow">→</div>
                        <div class="progress-step active">Production</div>
                    </div>
                    <div style="font-size: 1.3em; color: #ff6f00; font-weight: bold; margin-top: 20px;">
                        From $QK^T$ to 400+ GFLOPS: The Complete Journey
                    </div>
                </div>
            </section>

            <!-- Thank You -->
            <section>
                <h2>Thank You</h2>
                <h3>Questions & Discussion</h3>
                <div style="margin-top: 50px; font-size: 0.9em;">
                    <p>This presentation showcases the complete journey from mathematical foundations to production-grade high-performance attention mechanisms.</p>
                    <div style="margin-top: 30px; display: flex; justify-content: center; gap: 50px;">
                        <div style="text-align: center;">
                            <div style="font-size: 2em;">🧮</div>
                            <div>Mathematical Rigor</div>
                        </div>
                        <div style="text-align: center;">
                            <div style="font-size: 2em;">🧠</div>
                            <div>Conceptual Clarity</div>
                        </div>
                        <div style="text-align: center;">
                            <div style="font-size: 2em;">🏗️</div>
                            <div>Systems Design</div>
                        </div>
                        <div style="text-align: center;">
                            <div style="font-size: 2em;">⚡</div>
                            <div>HPC Performance</div>
                        </div>
                        <div style="text-align: center;">
                            <div style="font-size: 2em;">🚀</div>
                            <div>Production Ready</div>
                        </div>
                    </div>
                    <p style="margin-top: 30px; font-style: italic;">Ready to deploy high-performance AI inference anywhere.</p>
                </div>
            </section>

        </div>
    </div>

  <script src="../reveal.js/dist/reveal.js"></script>
  <script src="../reveal.js/plugin/zoom/zoom.js"></script>
  <script src="../reveal.js/plugin/notes/notes.js"></script>
  <script src="../reveal.js/plugin/search/search.js"></script>
  <script src="../reveal.js/plugin/markdown/markdown.js"></script>
  <script src="../reveal.js/plugin/highlight/highlight.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.3.1/plugin/math/math.min.js"></script>
  <script>
        document.addEventListener('DOMContentLoaded', function() {
            Reveal.initialize({
                controls: true,
                progress: true,
                center: true,
                hash: true,
                transition: 'slide',
                plugins: [RevealZoom, RevealHighlight, RevealMarkdown, RevealNotes, RevealMath.KaTeX ]
            });

            const HEAD_COLORS = ['#e57373', '#81c784', '#64b5f6', '#ffd54f', '#ba68c8', '#ff8a65', '#a1887f', '#90a4ae'];

            function createGrid(container, rows, cols, colorFn, isCausal) {
                if (!container) return;
                container.innerHTML = '';
                container.style.gridTemplateColumns = `repeat(${cols}, 15px)`;
                container.style.gridTemplateRows = `repeat(${rows}, 15px)`;
                for (let i = 0; i < rows; i++) {
                    for (let j = 0; j < cols; j++) {
                        const cell = document.createElement('div');
                        cell.classList.add('heatmap-cell');
                        cell.style.backgroundColor = colorFn(i, j);
                        if (isCausal && j > i) { cell.style.opacity = 0.1; }
                        else { cell.style.opacity = Math.random() * 0.6 + 0.3; }
                        container.appendChild(cell);
                    }
                }
            }

            function createRealMemoryLayout() {
                const memoryBar = document.getElementById('real-memory-bar');
                if (!memoryBar) return;
                
                memoryBar.innerHTML = '';
                
                // Simulate head-major layout: 8 heads, each with contiguous memory
                for (let h = 0; h < 8; h++) {
                    for (let t = 0; t < 8; t++) { // 8 tokens for visualization
                        const block = document.createElement('div');
                        block.style.width = '12px';
                        block.style.height = '20px';
                        block.style.backgroundColor = HEAD_COLORS[h];
                        block.style.border = '1px solid #333';
                        block.style.display = 'inline-block';
                        block.style.margin = '1px';
                        block.title = `Head ${h}, Token ${t}`;
                        memoryBar.appendChild(block);
                    }
                    
                    // Add small gap between heads
                    const gap = document.createElement('div');
                    gap.style.width = '4px';
                    gap.style.height = '20px';
                    gap.style.display = 'inline-block';
                    memoryBar.appendChild(gap);
                }
            }

            function setupVizSlides() {
                // Reorganization Viz
                if (document.getElementById('viz-memory-reorg')) {
                    createGrid(document.getElementById('reorg-q-grid'), 8, 16, (i, j) => HEAD_COLORS[j % 8]);
                    createGrid(document.getElementById('reorg-heads-grid'), 8, 16, (i, j) => HEAD_COLORS[Math.floor(j/2)]);
                }

                // Scores Viz
                if (document.getElementById('viz-scores')) {
                    createGrid(document.getElementById('score-q-grid'), 8, 2, () => HEAD_COLORS[0]);
                    createGrid(document.getElementById('score-k-grid'), 2, 8, () => HEAD_COLORS[0]);
                    createGrid(document.getElementById('score-res-grid'), 8, 8, () => '#ff8a65', true);
                }

                // Logical vs Physical Viz
                const logicalContainer = document.getElementById('logical-heads');
                const physicalBar = document.getElementById('physical-bar');
                if (document.getElementById('viz-logical-physical') && logicalContainer && physicalBar) {
                    logicalContainer.innerHTML = '';
                    physicalBar.innerHTML = '';

                    for (let h = 0; h < 8; h++) {
                        const headContainer = document.createElement('div');
                        const headGrid = document.createElement('div');
                        headGrid.classList.add('heatmap-grid');
                        const legend = document.createElement('div');
                        legend.classList.add('legend');
                        legend.innerText = `Head ${h}`;
                        headContainer.appendChild(headGrid);
                        headContainer.appendChild(legend);
                        logicalContainer.appendChild(headContainer);
                        createGrid(headGrid, 8, 2, () => HEAD_COLORS[h]);

                        const memBlock = document.createElement('div');
                        memBlock.classList.add('mem-block');
                        memBlock.style.width = '12.5%';
                        memBlock.style.backgroundColor = HEAD_COLORS[h];
                        memBlock.id = `physical-block-${h}`;
                        physicalBar.appendChild(memBlock);

                        headContainer.addEventListener('mouseover', () => { 
                            document.getElementById(`physical-block-${h}`).classList.add('highlight-box');
                        });
                        headContainer.addEventListener('mouseout', () => { 
                            document.getElementById(`physical-block-${h}`).classList.remove('highlight-box');
                        });
                    }
                }

                // Real memory layout
                if (document.getElementById('viz-memory-layout')) {
                    createRealMemoryLayout();
                }
            }

            Reveal.on('ready', event => { setupVizSlides(); });
            Reveal.on('slidechanged', event => { setupVizSlides(); });
        });
    </script>
</body>
</html>