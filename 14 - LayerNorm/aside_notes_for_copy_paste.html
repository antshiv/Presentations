<!-- Slide 1: Title -->
<aside class="notes">
Welcome everyone! Today we're exploring LayerNorm - the complete story from mathematical foundation to modern AI revolution. We'll cover what it is, why it matters, how it works, and when it changed everything. You'll see how this spans three critical areas: Mathematics - the internal covariate shift and distribution control; Revolution - the breakthrough from BERT to GPT-2 architecture; and Implementation - HPC optimization with memory and SIMD techniques. LayerNorm is truly the foundation that enabled modern transformers.
</aside>

<!-- Slide 2: The Problem: Internal Covariate Shift -->
<aside class="notes">
The fundamental challenge is that neural networks work best when inputs have stable statistical properties, but deep networks naturally create chaos. As we go deeper, the activations shift and explode unpredictably. This chart shows the problem - by layer 10, 20, 30, the distributions become completely unmanageable. This is internal covariate shift - the core problem LayerNorm solves. Without stable inputs, each layer spends all its time adapting to chaos instead of learning meaningful patterns.
</aside>

<!-- Slide 3: What Does "Variance" Actually Mean? -->
<aside class="notes">
Let's make this concrete. When we say "high variance," we mean unpredictable, chaotic signals that make learning impossible. Look at these training symptoms: loss curves that spike randomly, gradients that explode or vanish, activations going to extreme values. The analogy is perfect - it's like trying to learn to paint when someone keeps changing your canvas, paint colors, and brushes mid-stroke. LayerNorm gives us the stable, predictable canvas we need.
</aside>

<!-- Slide 4: LayerNorm: The Photo Editor's Solution -->
<aside class="notes">
Think of each layer as a photo editor working on an image. High variance means chaotic, flickering input - impossible to work with. LayerNorm is like giving each editor a clean, neutral canvas first. Watch this photo progression: raw chaotic input → LayerNorm normalization → clean, standardized output ready for the next layer. Every layer gets consistent, workable input instead of fighting unpredictable chaos. This is the core insight that makes deep learning possible.
</aside>

<!-- Slide 5: Where Exactly Is LayerNorm Applied? -->
<aside class="notes">
This is crucial - LayerNorm appears TWICE in every transformer layer! Let's trace the complete computation flow. Input goes to LayerNorm₁ before Multi-Head Attention, then through attention, gets added via residual connection. Then LayerNorm₂ before the Feed-Forward Network, through FFN, another residual. The complete formula shows: y = x + Attention(LayerNorm₁(x)), then z = y + FFN(LayerNorm₂(y)). Each LayerNorm has separate γ and β parameters. This pattern repeats - GPT-3 has 96 layers, so 192 LayerNorm operations total!
</aside>

<!-- Slide 6: The Math: Forcing Stable Distributions -->
<aside class="notes">
LayerNorm ensures every layer receives inputs with mean=0, variance=1, then learns perfect adjustments. These charts show the dramatic difference: raw activations are chaotic with shifting mean 2.3 and growing variance 5.7, causing training instability. With LayerNorm, we get stable mean=0, variance=1, enabling consistent training. The mathematical forcing of stable distributions is what makes deep networks trainable.
</aside>

<!-- Slide 7: Interactive: The γ and β Parameters -->
<aside class="notes">
LayerNorm first creates a neutral canvas with mean=0 and variance=1, then learns the perfect γ (scaling) and β (shifting) for each layer. These sliders show how γ controls the spread - higher γ makes features more pronounced, lower γ compresses them. β controls the center point - it shifts the entire distribution. The network learns these parameters during training to find the optimal statistical properties for each layer's specific function.
</aside>

<!-- Slide 8: Step 1: The Mean (μ) -->
<aside class="notes">
First step: find the average value of all features in our input token. The formula μ = (1/C) Σ x_i means we sum every feature value and divide by C, the embedding dimension. For a typical transformer with C=4096, we're averaging 4096 numbers into one scalar. This x_i is the i-th feature of our token vector. This gives us the center point of our data distribution.
</aside>

<!-- Slide 9: Step 2: The Variance (σ²) -->
<aside class="notes">
Next, calculate how much our features vary from that mean. σ² = (1/C) Σ (x_i - μ)² - for each feature, find the difference from mean, square it (ensuring positive values and penalizing outliers), sum all squared differences, then average them. This squared difference approach prevents positive and negative deviations from canceling out, and the squaring amplifies the effect of outliers that we want to control.
</aside>

<!-- Slide 10: Step 3: Normalization -->
<aside class="notes">
Now we create the clean, standardized vector. The formula x̂_i = (x_i - μ) / √(σ² + ε) does two things: x_i - μ centers the data around zero, and dividing by √(σ² + ε) normalizes the spread. The tiny ε (epsilon, usually 1e-5) prevents division by zero if variance happens to be zero. The result x̂ has mean=0 and variance=1 - exactly what we want for stable learning.
</aside>

<!-- Slide 11: Step 4 & Dimensions -->
<aside class="notes">
Finally, scale and shift using learned parameters: y = γ ⊙ x̂ + β. The element-wise multiply by γ and add β lets the network learn optimal scaling and shifting. Notice the dimensions for batch size=1: Input x is [C], Mean μ is scalar, Variance σ² is scalar, Gamma γ is [C], Beta β is [C], Output y is [C]. This B=1 simplification is crucial for HPC optimization - no complex batch indexing needed.
</aside>

<!-- Slide 12: The LayerNorm Revolution: BERT vs GPT-2 -->
<aside class="notes">
Here's the story of how 2 lines of code changed AI forever. 2018: BERT uses Post-LayerNorm - normalize AFTER adding the residual connection. 2019: GPT-2 makes the breakthrough - normalize BEFORE the function call. This timeline shows the progression: BERT pioneered transformers but was limited by Post-LN architecture. GPT-2's Pre-LN innovation enabled the scaling that led to GPT-3, ChatGPT, and the modern LLM era.
</aside>

<!-- Slide 13: BERT's Approach: Post-LN (Normalizing After) -->
<aside class="notes">
BERT's equation: y = LayerNorm(x + F(x)). The residual connection x gets added BEFORE normalization. Look at this diagram - the residual path goes through LayerNorm, creating gradient flow issues. The problem: gradients must flow through the normalization operation, the residual path isn't "clean", making it harder to train very deep networks. BERT was limited to around 24 layers because of this architectural choice.
</aside>

<!-- Slide 14: GPT-2's Innovation: Pre-LN (Normalizing Before) -->
<aside class="notes">
GPT-2's breakthrough: y = x + F(LayerNorm(x)). LayerNorm happens BEFORE the function, keeping the residual path completely clean! See that clean path indicator - the residual connection x stays untouched, providing a clean gradient highway. Benefits: clean gradient flow through residuals, much more stable training, can train very deep networks (96+ layers), enabled GPT-3, ChatGPT, and modern LLMs.
</aside>

<!-- Slide 15: The Critical Mathematical Difference -->
<aside class="notes">
The gradient flow tells the whole story. Post-LN: gradients explode as they flow backward - each layer amplifies the gradient, creating instability that gets worse with depth. Pre-LN: gradients stay stable through the clean residual path - the uninterrupted highway from output to input enables deep network training. This mathematical difference is why modern transformers can train with hundreds of layers while BERT was limited to 24.
</aside>

<!-- Slide 16: The Code Change That Changed Everything -->
<aside class="notes">
This architectural revolution was literally moving 2 lines of code. BERT style: layer_norm(x + attention(x)) and layer_norm(x + ffn(x)) - normalization happens AFTER adding residual. GPT-2 style: x + attention(layer_norm(x)) and x + ffn(layer_norm(x)) - normalization happens BEFORE the function call. Moving layer_norm() from line 4 to line 2 enabled GPT-3, ChatGPT, and the entire modern LLM era. Sometimes the most profound advances come from the simplest architectural changes.
</aside>

<!-- Slide 17: End-to-End LayerNorm: Memory Layout to Core Execution -->
<aside class="notes">
Now let's see how this actually runs on modern hardware. This diagram shows the complete journey from memory to CPU execution. In DRAM, we have aligned memory layout with layer input offsets, token slices with padding to prevent false sharing, gamma and beta weights, and crucially - ln1_mean_offset and ln1_rstd_offset for storing intermediate values. Data flows through the cache hierarchy: shared L3, private L2, L1 closest to cores. Finally reaches execution units with AVX-512 registers processing 16 floats at once through three passes: compute mean, compute variance, normalize and scale.
</aside>

<!-- Slide 18: SIMD Optimization -->
<aside class="notes">
Here's production-level LayerNorm code with AVX-512 optimization. This function processes token slices with mean and rstd caching for backward pass. Three passes highlighted: Pass 1 computes mean using _mm512_add_ps for 16-way parallelism with prefetch hints. Pass 2 computes variance using _mm512_fmadd_ps - fused multiply-add does diff*diff+accumulator in single instruction. Pass 3 normalizes, scales, and shifts using vectorized operations with gamma and beta. Key: we store mean_cache_slice[t] and rstd_cache_slice[t] for fast backward pass. Performance gains: 8-12x speedup over naive implementation, ~80% of theoretical memory bandwidth, 90%+ L1 hit rate.
</aside>

<!-- Slide 19: The Lasting Impact -->
<aside class="notes">
LayerNorm represents a perfect example of how deep understanding across multiple layers creates transformative technology. Mathematical insight: understanding internal covariate shift led to the normalization formula that stabilizes training. Architectural innovation: moving LayerNorm by 2 lines enabled deep network training and unlocked the modern era. Implementation mastery: SIMD optimization achieves near-hardware-limit performance through careful memory management and vectorization. This is the kind of full-stack thinking - from mathematical foundations to hardware optimization - that drives real AI progress. Questions?
</aside>