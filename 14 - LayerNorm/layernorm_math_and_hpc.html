<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>LayerNorm: Math to Optimized Implementation</title>

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/dist/reset.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/dist/reveal.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/dist/theme/black.css" id="theme">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/plugin/highlight/monokai.css" id="highlight-theme">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/3.9.1/gsap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.3.1/reveal.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.3.1/plugin/highlight/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.3.1/plugin/markdown/markdown.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.3.1/plugin/notes/notes.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.3.1/plugin/math/math.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>

    <style>
        .reveal { font-family: 'Inter', sans-serif; font-size: 28px; }
        .reveal h1, .reveal h2, .reveal h3, .reveal h4 { text-transform: none; font-weight: 700; }
        .reveal pre code { max-height: 600px; font-size: 0.6em; line-height: 1.2; }
        .reveal section img { border: none; box-shadow: none; }
        .reveal .r-stack { display: flex; flex-direction: column; justify-content: center; align-items: center; height: 100%; font-size: 0.9em; }
        .reveal .r-stack p, .reveal .r-stack ul li { font-size: 0.9em; line-height: 1.4; }
        .llm-layer-svg { width: 95%; height: 95%; margin: auto; display: block; }
        .llm-box { fill: #2a2a2e; stroke: #666; stroke-width: 1.5; rx: 8; ry: 8; }
        .llm-text { fill: #FFF; font-size: 12px; text-anchor: middle; font-family: 'Inter', sans-serif; }
        .llm-dim-text { fill: #AAA; font-size: 10px; text-anchor: middle; font-family: 'Inter', sans-serif; }
        .llm-label { fill: #42affa; font-size: 16px; font-weight: bold; text-anchor: middle; }
        .input-box { fill: #00BCD4; }
        .output-box { fill: #4CAF50; }
        .op-box { fill: #FFC107; }
        .arrow-line { stroke: #FFF; stroke-width: 1.5; fill: none; marker-end: url(#arrowhead); }
        .chart-container { width: 80%; max-width: 600px; margin: 20px auto; }
        .math-insight { background: rgba(66, 175, 250, 0.1); border: 2px solid #42affa; border-radius: 8px; padding: 15px; margin: 15px 0; }
        .math-insight h4 { color: #42affa; margin-bottom: 8px; }
        .dual-chart-container { display: flex; justify-content: space-around; width: 100%; }
        .chart-half { width: 45%; }
        .stats-box { background: rgba(255, 255, 255, 0.05); border-radius: 8px; padding: 10px; margin-top: 10px; }
        .stats-box p { margin: 5px 0; font-size: 0.7em; }
    </style>
</head>
<body>
    <div class="reveal">
        <div class="slides">

            <section>
                <h1>LayerNorm: Math to Optimized Implementation</h1>
                <h2>Understanding the Mathematical Foundation</h2>
                <p><small>From Distribution Control to High-Performance C Code</small></p>
                <aside class="notes">
                    <p>Welcome back. Today we're diving deep into LayerNorm - not just how to optimize it, but why it's mathematically essential for training stable neural networks.</p>
                    <p>We'll start with the core mathematical problem LayerNorm solves, show you what it's actually doing with visual charts, and then look at production-level C code that implements this math efficiently.</p>
                    <p>By the end, you'll understand both the mathematical intuition and the systems implementation that makes modern transformers possible.</p>
                </aside>
            </section>

            <section>
                <h2>The Core Problem: Internal Covariate Shift</h2>
                <div class="r-stack">
                    <p class="fragment">Neural networks work best when inputs have stable statistical properties</p>
                    
                    <div class="chart-container fragment">
                        <canvas id="problemChart" width="800" height="350"></canvas>
                    </div>
                    
                    <div class="math-insight fragment">
                        <h4>ðŸŽ¯ Mathematical Problem</h4>
                        <p>As network weights change during training, the distribution of activations constantly shifts. This makes it hard for subsequent layers to learn stable patterns.</p>
                    </div>
                </div>
                
                <aside class="notes">
                    <p>Here's the fundamental problem LayerNorm solves. Look at these three distributions representing activations in a layer during training.</p>
                    <p>Initially, activations might be nicely centered around zero. But as training progresses and weights change, the mean shifts and the variance explodes. This is called internal covariate shift.</p>
                    <p>When the next layer receives inputs with constantly changing statistical properties, it can't learn stable patterns. This slows training and makes it unstable.</p>
                </aside>
            </section>

            <section>
                <h2>LayerNorm: Forcing Stable Distributions</h2>
                <div class="r-stack">
                    <p class="fragment">LayerNorm ensures every layer receives inputs with mean=0, variance=1</p>
                    
                    <div class="dual-chart-container fragment">
                        <div class="chart-half">
                            <h4 style="color: #FF5722; text-align: center;">Before LayerNorm</h4>
                            <canvas id="beforeChart" width="300" height="200"></canvas>
                            <div class="stats-box">
                                <p>Î¼ = 2.3 (shifting)</p>
                                <p>ÏƒÂ² = 5.7 (growing)</p>
                                <p>Training: Unstable</p>
                            </div>
                        </div>
                        <div class="chart-half">
                            <h4 style="color: #4CAF50; text-align: center;">After LayerNorm</h4>
                            <canvas id="afterChart" width="300" height="200"></canvas>
                            <div class="stats-box">
                                <p>Î¼ = 0.0 (stable)</p>
                                <p>ÏƒÂ² = 1.0 (controlled)</p>
                                <p>Training: Stable</p>
                            </div>
                        </div>
                    </div>
                </div>
                
                <aside class="notes">
                    <p>LayerNorm solves this by forcing a standard normal distribution. The left chart shows what happens without LayerNorm - the distribution drifts and spreads unpredictably.</p>
                    <p>The right chart shows the effect of LayerNorm - every layer gets inputs with zero mean and unit variance, regardless of what happened in previous layers.</p>
                    <p>This stability allows much higher learning rates and faster convergence. It's the difference between a network that trains and one that doesn't.</p>
                </aside>
            </section>

            <section>
                <h2>The Math: What LayerNorm Actually Does</h2>
                <div class="r-stack">
                    <p class="fragment">Formula: $	ext{output} = \gamma \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta$</p>
                    
                    <div class="chart-container fragment">
                        <canvas id="transformChart" width="800" height="400"></canvas>
                    </div>
                    
                    <div class="math-insight fragment">
                        <h4>ðŸ”¬ Step by Step</h4>
                        <p>1. Compute mean (Î¼) and variance (ÏƒÂ²) across embedding dimensions<br>
                        2. Subtract mean and divide by standard deviation (normalize)<br>
                        3. Scale by learned Î³ and shift by learned Î² (restore flexibility)</p>
                    </div>
                </div>
                
                <aside class="notes">
                    <p>Let me show you what LayerNorm actually does mathematically. The chart shows three distributions: the original input with some arbitrary mean and variance, the normalized version with zero mean and unit variance, and the final output after learned scaling and shifting.</p>
                    <p>The formula looks complex but it's straightforward. For each token, you compute the mean and variance across all embedding dimensions. Then you subtract the mean and divide by the standard deviation. This gives you a standard normal distribution.</p>
                    <p>Finally, you scale by gamma and shift by beta. These are learned parameters that let the network adjust the final distribution as needed. This gives back the representational power while maintaining training stability.</p>
                </aside>
            </section>

            <section>
                <h2>LayerNorm vs Gaussian Distribution: What's the Difference?</h2>
                <div class="r-stack">
                    <div class="math-insight fragment">
                        <h4>ðŸ¤” Common Confusion</h4>
                        <p>"Why not just force a standard normal distribution and stop there?"</p>
                    </div>
                    
                    <div class="chart-container fragment">
                        <canvas id="conceptChart" width="800" height="400"></canvas>
                    </div>
                    
                    <div class="fragment" style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-top: 20px;">
                        <div class="math-insight">
                            <h4>ðŸ”§ LayerNorm (Process)</h4>
                            <p>â€¢ An <strong>algorithm</strong> that transforms data<br>
                            â€¢ Uses standard normal as <strong>intermediate step</strong><br>
                            â€¢ Learns optimal final distribution via Î³, Î²<br>
                            â€¢ Goal: <strong>stable + flexible</strong></p>
                        </div>
                        <div class="math-insight">
                            <h4>ðŸ“Š Gaussian Distribution (Concept)</h4>
                            <p>â€¢ A <strong>mathematical description</strong><br>
                            â€¢ Fixed bell curve shape<br>
                            â€¢ No learning involved<br>
                            â€¢ Goal: <strong>describe probability</strong></p>
                        </div>
                    </div>
                </div>
                <aside class="notes">
                    <p>This is a crucial distinction that confuses many people. LayerNorm is not trying to create a Gaussian distribution - it's using the standard normal distribution as a clean intermediate step.</p>
                    <p>Think of it like photo editing. You first normalize the photo to a neutral state - proper exposure, balanced contrast. That's like the standard normal step. But you don't stop there! You then apply your artistic style - adjust colors, contrast, mood. That's what gamma and beta do.</p>
                    <p>The chart shows three distributions: the messy input, the clean intermediate standard normal, and the final learned distribution after gamma and beta. The network learns to transform the standardized data into whatever distribution works best for the next layer.</p>
                    <p>If we stopped at standard normal, we'd severely limit the network's representational power. Every layer would get the same rigid bell curve input. By adding learnable parameters, we give the network back its flexibility while maintaining training stability.</p>
                </aside>
            </section>

            <section>
                <h2>Why LayerNorm Everywhere? The Mathematical Necessity</h2>
                <div class="chart-container">
                    <canvas id="layerDistributionChart" width="800" height="400"></canvas>
                </div>
                <div class="r-stack">
                    <p class="fragment">Each operation in a Transformer shifts the activation distribution</p>
                    <p class="fragment">Without normalization, distributions drift and training becomes unstable</p>
                    <p class="fragment">LayerNorm restores the "sweet spot" - zero mean, unit variance</p>
                </div>
                <aside class="notes">
                    <p>This chart shows what happens to activations as they flow through a Transformer layer. The blue line shows the initial distribution - nicely centered and well-behaved.</p>
                    <p>After the attention operation, the distribution shifts and spreads out - the red line. This is why we need LayerNorm before the MLP. Without it, the MLP receives inputs with unpredictable statistics.</p>
                    <p>After the MLP, the distribution shifts again - the yellow line. This is why we need another LayerNorm before the next layer.</p>
                    <p>LayerNorm isn't just an optimization - it's mathematically necessary. Each operation changes the statistical properties of the data, and subsequent operations depend on those properties being stable.</p>
                </aside>
            </section>

            <section>
                <h2>Transformer Architecture: LayerNorm Placement</h2>
                <div class="r-stack">
                    <svg width="100%" height="500" viewBox="0 0 800 500" class="fragment">
                        <defs><marker id="arrowhead" markerWidth="10" markerHeight="7" refX="0" refY="3.5" orient="auto"><polygon points="0 0, 10 3.5, 0 7" fill="#FFF"/></marker></defs>
                        
                        <!-- Title -->
                        <text x="400" y="30" class="llm-label" font-size="18">Pre-Normalization: LayerNorm Before Every Operation</text>
                        
                        <!-- Input -->
                        <rect x="350" y="60" width="100" height="30" class="llm-box" fill="#00BCD4"/>
                        <text x="400" y="80" class="llm-text">Input Tokens</text>
                        <path d="M400 90 L400 110" class="arrow-line"/>
                        
                        <!-- LayerNorm 1 -->
                        <rect x="320" y="120" width="160" height="25" class="llm-box" fill="#E91E63"/>
                        <text x="400" y="137" class="llm-text">LayerNorm</text>
                        <path d="M400 145 L400 165" class="arrow-line"/>
                        
                        <!-- Multi-Head Attention -->
                        <rect x="300" y="175" width="200" height="40" class="llm-box" fill="#9C27B0"/>
                        <text x="400" y="200" class="llm-text">Multi-Head Attention</text>
                        <path d="M400 215 L400 235" class="arrow-line"/>
                        
                        <!-- Residual Connection 1 -->
                        <circle cx="400" cy="245" r="15" fill="#FFC107" stroke="#666" stroke-width="2"/>
                        <text x="400" y="250" class="llm-text" font-size="14">+</text>
                        <path d="M385 245 L200 245 L200 75 L350 75" class="arrow-line" stroke-width="1" stroke-dasharray="5,5"/>
                        <path d="M400 260 L400 280" class="arrow-line"/>
                        
                        <!-- LayerNorm 2 -->
                        <rect x="320" y="290" width="160" height="25" class="llm-box" fill="#E91E63"/>
                        <text x="400" y="307" class="llm-text">LayerNorm</text>
                        <path d="M400 315 L400 335" class="arrow-line"/>
                        
                        <!-- MLP -->
                        <rect x="350" y="345" width="100" height="40" class="llm-box" fill="#4CAF50"/>
                        <text x="400" y="370" class="llm-text">MLP</text>
                        <path d="M400 385 L400 405" class="arrow-line"/>
                        
                        <!-- Residual Connection 2 -->
                        <circle cx="400" cy="415" r="15" fill="#FFC107" stroke="#666" stroke-width="2"/>
                        <text x="400" y="420" class="llm-text" font-size="14">+</text>
                        <path d="M385 415 L250 415 L250 260 L385 260" class="arrow-line" stroke-width="1" stroke-dasharray="5,5"/>
                        <path d="M400 430 L400 450" class="arrow-line"/>
                        
                        <!-- Output -->
                        <rect x="340" y="460" width="120" height="30" class="llm-box" fill="#FF5722"/>
                        <text x="400" y="480" class="llm-text">To Next Layer</text>
                        
                        <!-- Annotations -->
                        <text x="520" y="140" class="llm-dim-text">Stabilizes attention inputs</text>
                        <text x="520" y="310" class="llm-dim-text">Stabilizes MLP inputs</text>
                        <text x="520" y="250" class="llm-dim-text">Residual preserves gradients</text>
                        <text x="520" y="420" class="llm-dim-text">Combines processed info</text>
                    </svg>
                </div>
                <aside class="notes">
                    <p>This diagram shows the GPT-2 style pre-normalization architecture. LayerNorm appears before every major operation - before attention and before the MLP.</p>
                    <p>Why this pattern? Each operation distorts the activation distribution. Attention mixes information across positions, which can create correlations that shift the mean and variance. The MLP applies non-linear transformations that further change the distribution.</p>
                    <p>By placing LayerNorm before each operation, we ensure that every operation receives inputs with stable statistical properties. This makes training more stable and allows higher learning rates.</p>
                    <p>The residual connections are crucial too - they provide a direct path for gradients to flow backward, preventing the vanishing gradient problem. But they also mean that the distribution keeps getting modified by these residual additions.</p>
                    <p>This is why we need LayerNorm everywhere - it's not just good practice, it's mathematically necessary for deep networks to train effectively.</p>
                </aside>
            </section>

            <section>
                <h2>Memory Layout: The HPC Foundation</h2>
                <div class="r-stack">
                    <svg width="100%" height="500" viewBox="0 0 1000 500" class="fragment">
                        <defs><marker id="arrowhead2" markerWidth="10" markerHeight="7" refX="0" refY="3.5" orient="auto"><polygon points="0 0, 10 3.5, 0 7" fill="#FFF"/></marker></defs>
                        <!-- Title -->
                        <text x="500" y="30" class="llm-label" font-size="20">Contiguous Memory Arena: 2MB Huge Pages + 64-byte Alignment</text>
                        
                        <!-- Memory Arena Box -->
                        <rect x="50" y="60" width="900" height="400" fill="none" stroke="#42affa" stroke-width="3" rx="10"/>
                        <text x="60" y="85" class="llm-text" font-size="14">Single mmap() allocation - Zero fragmentation</text>
                        
                        <!-- LayerNorm Weights Section -->
                        <rect x="80" y="100" width="200" height="80" class="llm-box" fill="#E91E63"/>
                        <text x="180" y="125" class="llm-text">LayerNorm Weights</text>
                        <text x="180" y="140" class="llm-dim-text">Î³[embed_dim], Î²[embed_dim]</text>
                        <text x="180" y="155" class="llm-dim-text">Shared across ALL tokens</text>
                        <text x="180" y="170" class="llm-dim-text">64-byte aligned</text>
                        
                        <!-- Token Data Section -->
                        <rect x="320" y="100" width="500" height="150" class="llm-box" fill="#00BCD4"/>
                        <text x="570" y="125" class="llm-text">Token Data: [context_window Ã— aligned_embed_dim]</text>
                        
                        <!-- Core 0 slice -->
                        <rect x="340" y="140" width="100" height="40" class="llm-box" fill="#4CAF50"/>
                        <text x="390" y="165" class="llm-dim-text">Core 0</text>
                        
                        <!-- Core 1 slice -->
                        <rect x="450" y="140" width="100" height="40" class="llm-box" fill="#4CAF50"/>
                        <text x="500" y="165" class="llm-dim-text">Core 1</text>
                        
                        <!-- Core N slice -->
                        <rect x="560" y="140" width="100" height="40" class="llm-box" fill="#4CAF50"/>
                        <text x="610" y="165" class="llm-dim-text">Core N</text>
                        
                        <text x="570" y="200" class="llm-dim-text">Each core: tokens_per_core Ã— aligned_embed_dim</text>
                        <text x="570" y="215" class="llm-dim-text">Sequential access, no false sharing</text>
                        <text x="570" y="230" class="llm-dim-text">get_slice() for core-local access</text>
                        
                        <!-- Algorithm Steps -->
                        <text x="500" y="290" class="llm-label" font-size="16">LayerNorm Algorithm Ã— Memory Layout</text>
                        
                        <!-- Step 1 -->
                        <rect x="80" y="310" width="250" height="60" class="llm-box" fill="#FFC107"/>
                        <text x="205" y="330" class="llm-text">1. Per-token mean & variance</text>
                        <text x="205" y="345" class="llm-dim-text">Compute across embed_dim</text>
                        <text x="205" y="360" class="llm-dim-text">Cache-friendly: 64-byte strides</text>
                        
                        <!-- Step 2 -->
                        <rect x="370" y="310" width="250" height="60" class="llm-box" fill="#9C27B0"/>
                        <text x="495" y="330" class="llm-text">2. Normalize & scale</text>
                        <text x="495" y="345" class="llm-dim-text">Broadcast Î³, Î² to all tokens</text>
                        <text x="495" y="360" class="llm-dim-text">AVX-512: 16 floats/instruction</text>
                        
                        <!-- Step 3 -->
                        <rect x="660" y="310" width="250" height="60" class="llm-box" fill="#FF5722"/>
                        <text x="785" y="330" class="llm-text">3. Token-parallel output</text>
                        <text x="785" y="345" class="llm-dim-text">Each core writes its slice</text>
                        <text x="785" y="360" class="llm-dim-text">No memory contention</text>
                        
                        <!-- Memory arrows -->
                        <path d="M280 140 L320 140" class="arrow-line" marker-end="url(#arrowhead2)"/>
                        <path d="M180 180 L340 310" class="arrow-line" marker-end="url(#arrowhead2)"/>
                        <path d="M570 250 L495 310" class="arrow-line" marker-end="url(#arrowhead2)"/>
                        
                        <!-- Performance callout -->
                        <rect x="80" y="400" width="840" height="40" fill="rgba(66, 175, 250, 0.2)" stroke="#42affa" rx="5"/>
                        <text x="500" y="425" class="llm-text">ðŸš€ Result: Mathematical necessity + optimal memory layout = fast, stable training</text>
                    </svg>
                </div>
                <aside class="notes">
                    <p>Now that we understand the math, let's see how to implement it efficiently. This is the key to our performance - the memory layout. Everything is allocated in one contiguous arena using 2MB huge pages, which reduces TLB misses dramatically.</p>
                    <p>LayerNorm weights gamma and beta are 1D tensors with embed_dim elements, shared across all tokens. They're 64-byte aligned and stored once per layer. This is crucial - we're not storing separate weights per token.</p>
                    <p>The token data is laid out sequentially, and we use token-parallel processing. Each CPU core gets a slice of tokens using the get_slice function. This avoids false sharing because each core works on different cache lines.</p>
                    <p>The algorithm maps perfectly to this layout. We compute mean and variance per token across the embedding dimension with cache-friendly access patterns. Then we broadcast the shared gamma and beta parameters to normalize each token. Finally, each core writes its output slice independently.</p>
                    <p>This design bridges the gap between mathematical necessity and practical implementation. LayerNorm is mathematically required for training, but this memory architecture makes it fast enough for real use.</p>
                </aside>
            </section>

            <section>
                <h2>From Math to Optimized C Implementation</h2>
                <p>Production-level C code that implements the math efficiently:</p>
                <pre><code class="c" data-trim data-noescape>
void layernorm_forward_unrolled(const float * __restrict input,
                                const float * __restrict gamma,
                                const float * __restrict beta,
                                float * __restrict output,
                                int tokens, int d_model, float eps) {
    #pragma omp parallel for schedule(static)
    for (int t = 0; t < tokens; ++t) {
        const float * __restrict x = input + t * d_model;
        float * __restrict out = output + t * d_model;
        
        // Step 1: Calculate mean using 4-way SIMD unrolling
        __m512 acc0 = _mm512_setzero_ps();
        __m512 acc1 = _mm512_setzero_ps();
        __m512 acc2 = _mm512_setzero_ps();
        __m512 acc3 = _mm512_setzero_ps();
        
        int i = 0;
        for (; i <= d_model - 64; i += 64) {
            _mm_prefetch((const char*)(x + i + 64), _MM_HINT_T0);
            
            __m512 v0 = _mm512_load_ps(x + i);
            __m512 v1 = _mm512_load_ps(x + i + 16);
            __m512 v2 = _mm512_load_ps(x + i + 32);
            __m512 v3 = _mm512_load_ps(x + i + 48);
            
            acc0 = _mm512_add_ps(acc0, v0);
            acc1 = _mm512_add_ps(acc1, v1);
            acc2 = _mm512_add_ps(acc2, v2);
            acc3 = _mm512_add_ps(acc3, v3);
        }
        
        // Reduce and compute final mean, variance, normalization...
        // Single-pass algorithm: mean, variance, normalize in one loop
    }
}
                </code></pre>
                <aside class="notes">
                    <p>This is where the math meets the machine. This production C code implements the LayerNorm formula we saw earlier, but it's optimized for modern CPUs.</p>
                    <p>We're using AVX-512 instructions to process 16 floats at a time, and we're unrolling the loop 4 ways to process 64 floats per iteration. The software prefetching hints help keep data in cache.</p>
                    <p>The key insight is that we're doing all three mathematical steps - mean, variance, and normalization - in one pass through the data. This minimizes memory bandwidth and maximizes cache efficiency.</p>
                    <p>This is what makes the mathematical theory practical. You can have the most elegant algorithm in the world, but if it's too slow to use, it doesn't matter. The memory layout and vectorization make LayerNorm fast enough for real-time AI inference.</p>
                </aside>
            </section>

            <section>
                <h2>Conclusion</h2>
                <div class="r-stack">
                    <p>LayerNorm bridges mathematics and engineering in AI systems.</p>
                    <p class="fragment"><strong>Mathematical necessity:</strong> Without it, deep networks can't train stably.</p>
                    <p class="fragment"><strong>Engineering challenge:</strong> Make it fast enough for practical use.</p>
                    <p class="fragment"><strong>HPC solution:</strong> Memory layout + vectorization + parallelization.</p>
                    <p class="fragment">From Gaussian distributions to optimized C code - that's modern AI engineering.</p>
                </div>
                <aside class="notes">
                    <p>LayerNorm is a perfect example of how mathematics and engineering work together in AI. The math tells us we need stable distributions for training, and the engineering tells us how to compute it efficiently.</p>
                    <p>The key insight is that LayerNorm isn't just a "nice to have" - it's mathematically necessary. Without it, activations drift and training becomes impossible. That's why it appears before every operation in modern architectures.</p>
                    <p>The optimization techniques - memory layout, vectorization, parallelization - they're what make this mathematical necessity practical on real hardware. You can have the best algorithm in the world, but if it's slow, nobody will use it.</p>
                    <p>This is the essence of modern AI engineering: understanding both the mathematical foundations and the systems implementation that makes theory practical.</p>
                    <p>Thank you for watching. If this was helpful, please like and subscribe. I'll see you in the next video.</p>
                </aside>
            </section>

        </div>
    </div>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            Reveal.initialize({
                controls: true,
                progress: true,
                center: true,
                hash: true,
                transition: 'slide',
                plugins: [ RevealHighlight, RevealMarkdown, RevealNotes, RevealMath.KaTeX ]
            });

            // Problem chart - showing distribution drift during training
            const problemCtx = document.getElementById('problemChart').getContext('2d');
            const xValues = [];
            const initialDist = [];
            const epoch50Dist = [];
            const epoch100Dist = [];
            
            for (let x = -6; x <= 6; x += 0.1) {
                xValues.push(x);
                // Initial distribution (epoch 1)
                initialDist.push(Math.exp(-0.5 * x * x) / Math.sqrt(2 * Math.PI));
                // After 50 epochs - shifted
                epoch50Dist.push(Math.exp(-0.5 * Math.pow((x - 1.5) / 1.2, 2)) / (1.2 * Math.sqrt(2 * Math.PI)));
                // After 100 epochs - severely shifted
                epoch100Dist.push(Math.exp(-0.5 * Math.pow((x - 3.0) / 2.0, 2)) / (2.0 * Math.sqrt(2 * Math.PI)));
            }
            
            new Chart(problemCtx, {
                type: 'line',
                data: {
                    labels: xValues,
                    datasets: [{
                        label: 'Epoch 1 (Stable)',
                        data: initialDist,
                        borderColor: '#4ECDC4',
                        backgroundColor: 'rgba(78, 205, 196, 0.1)',
                        fill: true,
                        tension: 0.4
                    }, {
                        label: 'Epoch 50 (Shifting)',
                        data: epoch50Dist,
                        borderColor: '#FFC107',
                        backgroundColor: 'rgba(255, 193, 7, 0.1)',
                        fill: true,
                        tension: 0.4
                    }, {
                        label: 'Epoch 100 (Drifted)',
                        data: epoch100Dist,
                        borderColor: '#FF6B6B',
                        backgroundColor: 'rgba(255, 107, 107, 0.1)',
                        fill: true,
                        tension: 0.4
                    }]
                },
                options: {
                    responsive: true,
                    plugins: {
                        title: {
                            display: true,
                            text: 'Internal Covariate Shift: Distributions Drift During Training',
                            color: '#fff'
                        },
                        legend: { labels: { color: '#fff' } }
                    },
                    scales: {
                        x: { title: { display: true, text: 'Activation Value', color: '#fff' }, ticks: { color: '#fff' }, grid: { color: 'rgba(255, 255, 255, 0.1)' } },
                        y: { title: { display: true, text: 'Probability Density', color: '#fff' }, ticks: { color: '#fff' }, grid: { color: 'rgba(255, 255, 255, 0.1)' } }
                    }
                }
            });

            // Before LayerNorm chart
            const beforeCtx = document.getElementById('beforeChart').getContext('2d');
            const beforeXValues = [];
            const unstableDist = [];
            
            for (let x = -8; x <= 8; x += 0.2) {
                beforeXValues.push(x);
                unstableDist.push(Math.exp(-0.5 * Math.pow((x - 2.3) / 2.4, 2)) / (2.4 * Math.sqrt(2 * Math.PI)));
            }
            
            new Chart(beforeCtx, {
                type: 'line',
                data: {
                    labels: beforeXValues,
                    datasets: [{
                        label: 'Unstable Distribution',
                        data: unstableDist,
                        borderColor: '#FF6B6B',
                        backgroundColor: 'rgba(255, 107, 107, 0.2)',
                        fill: true,
                        tension: 0.4
                    }]
                },
                options: {
                    responsive: true,
                    plugins: { legend: { display: false } },
                    scales: {
                        x: { ticks: { color: '#fff' }, grid: { color: 'rgba(255, 255, 255, 0.1)' } },
                        y: { ticks: { color: '#fff' }, grid: { color: 'rgba(255, 255, 255, 0.1)' } }
                    }
                }
            });

            // After LayerNorm chart
            const afterCtx = document.getElementById('afterChart').getContext('2d');
            const afterXValues = [];
            const stableDist = [];
            
            for (let x = -4; x <= 4; x += 0.1) {
                afterXValues.push(x);
                stableDist.push(Math.exp(-0.5 * x * x) / Math.sqrt(2 * Math.PI));
            }
            
            new Chart(afterCtx, {
                type: 'line',
                data: {
                    labels: afterXValues,
                    datasets: [{
                        label: 'Stable Distribution',
                        data: stableDist,
                        borderColor: '#4ECDC4',
                        backgroundColor: 'rgba(78, 205, 196, 0.2)',
                        fill: true,
                        tension: 0.4
                    }]
                },
                options: {
                    responsive: true,
                    plugins: { legend: { display: false } },
                    scales: {
                        x: { ticks: { color: '#fff' }, grid: { color: 'rgba(255, 255, 255, 0.1)' } },
                        y: { ticks: { color: '#fff' }, grid: { color: 'rgba(255, 255, 255, 0.1)' } }
                    }
                }
            });

            // Transform chart - showing the 3-step transformation
            const transformCtx = document.getElementById('transformChart').getContext('2d');
            const transformXValues = [];
            const originalDist = [];
            const normalizedDist = [];
            const finalDist = [];
            
            for (let x = -4; x <= 4; x += 0.1) {
                transformXValues.push(x);
                // Original distribution (mean=1.5, std=0.8)
                originalDist.push(Math.exp(-0.5 * Math.pow((x - 1.5) / 0.8, 2)) / (0.8 * Math.sqrt(2 * Math.PI)));
                // Normalized distribution (mean=0, std=1)
                normalizedDist.push(Math.exp(-0.5 * x * x) / Math.sqrt(2 * Math.PI));
                // Final distribution after learned scaling and shifting (gamma=1.2, beta=0.3)
                finalDist.push(Math.exp(-0.5 * Math.pow((x - 0.3) / 1.2, 2)) / (1.2 * Math.sqrt(2 * Math.PI)));
            }
            
            new Chart(transformCtx, {
                type: 'line',
                data: {
                    labels: transformXValues,
                    datasets: [{
                        label: 'Original Input',
                        data: originalDist,
                        borderColor: '#FF6B6B',
                        backgroundColor: 'rgba(255, 107, 107, 0.1)',
                        fill: true,
                        tension: 0.4
                    }, {
                        label: 'Normalized (Î¼=0, Ïƒ=1)',
                        data: normalizedDist,
                        borderColor: '#4ECDC4',
                        backgroundColor: 'rgba(78, 205, 196, 0.1)',
                        fill: true,
                        tension: 0.4
                    }, {
                        label: 'Final Output (Î³, Î² learned)',
                        data: finalDist,
                        borderColor: '#45B7D1',
                        backgroundColor: 'rgba(69, 183, 209, 0.1)',
                        fill: true,
                        tension: 0.4
                    }]
                },
                options: {
                    responsive: true,
                    plugins: {
                        title: { display: true, text: 'LayerNorm: 3-Step Mathematical Transformation', color: '#fff' },
                        legend: { labels: { color: '#fff' } }
                    },
                    scales: {
                        x: { title: { display: true, text: 'Value', color: '#fff' }, ticks: { color: '#fff' }, grid: { color: 'rgba(255, 255, 255, 0.1)' } },
                        y: { title: { display: true, text: 'Probability Density', color: '#fff' }, ticks: { color: '#fff' }, grid: { color: 'rgba(255, 255, 255, 0.1)' } }
                    }
                }
            });

            // Concept chart - LayerNorm vs Gaussian Distribution
            const conceptCtx = document.getElementById('conceptChart').getContext('2d');
            const conceptXValues = [];
            const messyInput = [];
            const standardNormal = [];
            const learnedOutput = [];
            
            for (let x = -5; x <= 5; x += 0.1) {
                conceptXValues.push(x);
                // Messy input distribution (arbitrary shape)
                messyInput.push(Math.exp(-0.5 * Math.pow((x - 2.1) / 1.8, 2)) / (1.8 * Math.sqrt(2 * Math.PI)));
                // Standard normal (intermediate step)
                standardNormal.push(Math.exp(-0.5 * x * x) / Math.sqrt(2 * Math.PI));
                // Learned output distribution (network's choice via gamma, beta)
                learnedOutput.push(Math.exp(-0.5 * Math.pow((x + 0.5) / 1.3, 2)) / (1.3 * Math.sqrt(2 * Math.PI)));
            }
            
            new Chart(conceptCtx, {
                type: 'line',
                data: {
                    labels: conceptXValues,
                    datasets: [{
                        label: '1. Messy Input (Arbitrary)',
                        data: messyInput,
                        borderColor: '#FF6B6B',
                        backgroundColor: 'rgba(255, 107, 107, 0.1)',
                        fill: true,
                        tension: 0.4,
                        borderDash: [5, 5]
                    }, {
                        label: '2. Standard Normal (Intermediate)',
                        data: standardNormal,
                        borderColor: '#4ECDC4',
                        backgroundColor: 'rgba(78, 205, 196, 0.1)',
                        fill: true,
                        tension: 0.4
                    }, {
                        label: '3. Learned Output (Network\'s Choice)',
                        data: learnedOutput,
                        borderColor: '#45B7D1',
                        backgroundColor: 'rgba(69, 183, 209, 0.1)',
                        fill: true,
                        tension: 0.4,
                        borderWidth: 3
                    }]
                },
                options: {
                    responsive: true,
                    plugins: {
                        title: { 
                            display: true, 
                            text: 'LayerNorm: Process vs Concept', 
                            color: '#fff',
                            font: { size: 16 }
                        },
                        legend: { 
                            labels: { color: '#fff' },
                            position: 'bottom'
                        }
                    },
                    scales: {
                        x: { 
                            title: { display: true, text: 'Value', color: '#fff' }, 
                            ticks: { color: '#fff' }, 
                            grid: { color: 'rgba(255, 255, 255, 0.1)' } 
                        },
                        y: { 
                            title: { display: true, text: 'Probability Density', color: '#fff' }, 
                            ticks: { color: '#fff' }, 
                            grid: { color: 'rgba(255, 255, 255, 0.1)' } 
                        }
                    }
                }
            });

            // Layer distribution chart showing distribution drift through operations
            const layerCtx = document.getElementById('layerDistributionChart').getContext('2d');
            const layerXValues = [];
            const initialLayerDist = [];
            const afterAttentionDist = [];
            const afterMLPDist = [];
            
            for (let x = -5; x <= 5; x += 0.1) {
                layerXValues.push(x);
                // Initial well-behaved distribution
                initialLayerDist.push(Math.exp(-0.5 * x * x) / Math.sqrt(2 * Math.PI));
                // After attention - shifted and spread out
                afterAttentionDist.push(Math.exp(-0.5 * Math.pow((x - 1.8) / 1.4, 2)) / (1.4 * Math.sqrt(2 * Math.PI)));
                // After MLP - further shifted and distorted
                afterMLPDist.push(Math.exp(-0.5 * Math.pow((x + 0.7) / 2.1, 2)) / (2.1 * Math.sqrt(2 * Math.PI)));
            }
            
            new Chart(layerCtx, {
                type: 'line',
                data: {
                    labels: layerXValues,
                    datasets: [{
                        label: 'Initial (Stable)',
                        data: initialLayerDist,
                        borderColor: '#4ECDC4',
                        backgroundColor: 'rgba(78, 205, 196, 0.1)',
                        fill: true,
                        tension: 0.4
                    }, {
                        label: 'After Attention (Shifted)',
                        data: afterAttentionDist,
                        borderColor: '#FF6B6B',
                        backgroundColor: 'rgba(255, 107, 107, 0.1)',
                        fill: true,
                        tension: 0.4
                    }, {
                        label: 'After MLP (Drifted)',
                        data: afterMLPDist,
                        borderColor: '#FFC107',
                        backgroundColor: 'rgba(255, 193, 7, 0.1)',
                        fill: true,
                        tension: 0.4
                    }]
                },
                options: {
                    responsive: true,
                    plugins: {
                        title: { display: true, text: 'Distribution Drift Through Transformer Operations', color: '#fff' },
                        legend: { labels: { color: '#fff' } }
                    },
                    scales: {
                        x: { title: { display: true, text: 'Activation Value', color: '#fff' }, ticks: { color: '#fff' }, grid: { color: 'rgba(255, 255, 255, 0.1)' } },
                        y: { title: { display: true, text: 'Probability Density', color: '#fff' }, ticks: { color: '#fff' }, grid: { color: 'rgba(255, 255, 255, 0.1)' } }
                    }
                }
            });
        });
    </script>
</body>
</html>