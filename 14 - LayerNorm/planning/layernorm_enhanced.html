<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>LayerNorm: The Forgotten Bottleneck</title>

    <link rel="stylesheet" href="../reveal.js/dist/reset.css">
    <link rel="stylesheet" href="../reveal.js/dist/reveal.css">
    <link rel="stylesheet" href="../reveal.js/dist/theme/black.css" id="theme">
    <link rel="stylesheet" href="../reveal.js/plugin/highlight/monokai.css">
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/3.9.1/gsap.min.js"></script>

    <style>
        .reveal {
            font-family: 'Inter', sans-serif;
            font-size: 28px;
        }
        .reveal h1, .reveal h2, .reveal h3, .reveal h4, .reveal h5, .reveal h6 {
            text-transform: none;
            font-weight: 700;
        }
        .reveal pre {
            width: 90%;
            margin: 20px auto;
            font-size: 0.6em;
            line-height: 1.2;
        }
        .reveal code {
            font-family: 'Fira Code', monospace;
        }
        .reveal .r-stack {
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            height: 100%;
            font-size: 0.9em;
        }

        /* LayerNorm specific styles */
        .layernorm-svg {
            width: 100%;
            max-width: 800px;
            height: auto;
            margin: 20px auto;
            display: block;
        }

        /* Tensor visualization */
        .tensor-cell {
            fill: #333;
            stroke: #666;
            stroke-width: 1;
        }
        .tensor-highlight {
            fill: #42affa;
            stroke: #FFF;
            stroke-width: 2;
        }
        .tensor-text {
            fill: #FFF;
            font-size: 12px;
            text-anchor: middle;
            font-family: 'Inter', sans-serif;
        }
        .tensor-label {
            fill: #42affa;
            font-size: 16px;
            font-weight: bold;
            text-anchor: middle;
        }

        /* Mathematical operation styles */
        .math-box {
            fill: #2a2a2e;
            stroke: #666;
            stroke-width: 2;
            rx: 8;
            ry: 8;
        }
        .math-text {
            fill: #FFF;
            font-size: 14px;
            text-anchor: middle;
            font-family: 'Inter', sans-serif;
        }
        .math-label {
            fill: #FFC107;
            font-size: 18px;
            font-weight: bold;
            text-anchor: middle;
        }

        /* Arrow styles */
        .flow-arrow {
            stroke: #42affa;
            stroke-width: 3;
            marker-end: url(#arrowhead);
        }

        /* Performance chart styles */
        .performance-grid {
            display: grid;
            grid-template-columns: repeat(2, 1fr);
            gap: 20px;
            margin-top: 30px;
        }
        .perf-item {
            background: rgba(255, 255, 255, 0.05);
            padding: 20px;
            border-radius: 10px;
            text-align: left;
            font-size: 0.8em;
        }
        .perf-item h3 {
            color: #42affa;
            margin-bottom: 10px;
        }

        /* Memory layout styles */
        .memory-block {
            fill: #4CAF50;
            stroke: #FFF;
            stroke-width: 1;
        }
        .memory-text {
            fill: #FFF;
            font-size: 10px;
            text-anchor: middle;
        }
        .cache-line {
            fill: #FFC107;
            opacity: 0.7;
        }

        /* Optimization comparison */
        .optimization-container {
            display: flex;
            justify-content: space-around;
            width: 100%;
            margin-top: 20px;
        }
        .optimization-stage {
            width: 22%;
            text-align: center;
            background: rgba(255, 255, 255, 0.05);
            padding: 15px;
            border-radius: 10px;
        }
        .optimization-stage h4 {
            color: #42affa;
            margin-bottom: 10px;
            font-size: 0.9em;
        }
        .optimization-stage p {
            font-size: 0.7em;
            line-height: 1.3;
        }

        /* HPC concepts visualization */
        .hpc-concept {
            background: rgba(66, 175, 250, 0.1);
            border-left: 4px solid #42affa;
            padding: 15px;
            margin: 15px 0;
        }
        .hpc-concept h4 {
            color: #42affa;
            margin-bottom: 8px;
        }
        .hpc-concept p {
            margin: 0;
            font-size: 0.8em;
        }
    </style>
</head>

<body>
    <div class="reveal">
        <div class="slides">

            <!-- Title Slide -->
            <section>
                <h1>LayerNorm: The Forgotten Bottleneck</h1>
                <h2>CPU Optimization Beyond GEMM</h2>
                <p><small>From PyTorch Abstractions to Hand-Tuned C Performance</small></p>
                
                <aside class="notes">
                    <p>Welcome back! In my last video, we optimized GEMM and achieved a 6x speedup. But GEMM isn't the only operation in transformers that needs optimization.</p>
                    <p>Today we're looking at LayerNorm - a small but critical operation that happens twice per transformer layer. In GPT-2 Large, that's 96 LayerNorm operations per token.</p>
                    <p>Most people ignore LayerNorm optimization because it seems simple. But when you're doing billions of operations, even small optimizations matter. We'll go from PyTorch's hidden implementation to hand-tuned C code that's over 10x faster.</p>
                </aside>
            </section>

            <!-- The Hidden Cost of LayerNorm -->
            <section>
                <h2>The Hidden Cost of LayerNorm</h2>
                <div class="r-stack">
                    <p class="fragment">Everyone optimizes GEMM, but LayerNorm happens constantly:</p>
                    <ul class="fragment">
                        <li><strong>GPT-2 Small:</strong> 24 layers × 2 = 48 LayerNorms per token</li>
                        <li><strong>GPT-2 Large:</strong> 48 layers × 2 = 96 LayerNorms per token</li>
                        <li><strong>Each LayerNorm:</strong> 3 passes over embedding dimension</li>
                    </ul>
                    
                    <svg class="layernorm-svg fragment" viewBox="0 0 900 400" xmlns="http://www.w3.org/2000/svg">
                        <defs>
                            <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="0" refY="3.5" orient="auto">
                                <polygon points="0 0, 10 3.5, 0 7" fill="#42affa" />
                            </marker>
                        </defs>

                        <!-- Transformer Block Diagram -->
                        <text x="450" y="30" class="tensor-label">Single Transformer Block</text>
                        
                        <!-- Input -->
                        <rect x="400" y="50" width="100" height="40" class="math-box" fill="#00BCD4"/>
                        <text x="450" y="75" class="math-text">Input Tokens</text>
                        
                        <!-- First LayerNorm -->
                        <path d="M450 90 V120" class="flow-arrow"/>
                        <rect x="375" y="120" width="150" height="40" class="math-box" fill="#E91E63"/>
                        <text x="450" y="145" class="math-text">LayerNorm #1</text>
                        
                        <!-- Attention -->
                        <path d="M450 160 V190" class="flow-arrow"/>
                        <rect x="400" y="190" width="100" height="40" class="math-box" fill="#9C27B0"/>
                        <text x="450" y="215" class="math-text">Attention</text>
                        
                        <!-- Residual Connection -->
                        <path d="M350 215 Q320 215 320 140 Q320 70 350 70" stroke="#FFC107" stroke-width="2" fill="none"/>
                        <circle cx="350" cy="215" r="8" fill="#FFC107"/>
                        <text x="320" y="140" class="math-text" font-size="12">+</text>
                        
                        <!-- Second LayerNorm -->
                        <path d="M450 230 V260" class="flow-arrow"/>
                        <rect x="375" y="260" width="150" height="40" class="math-box" fill="#E91E63"/>
                        <text x="450" y="285" class="math-text">LayerNorm #2</text>
                        
                        <!-- MLP -->
                        <path d="M450 300 V330" class="flow-arrow"/>
                        <rect x="400" y="330" width="100" height="40" class="math-box" fill="#4CAF50"/>
                        <text x="450" y="355" class="math-text">MLP (GEMM)</text>
                        
                        <!-- Second Residual -->
                        <path d="M350 355 Q280 355 280 280 Q280 240 350 240" stroke="#FFC107" stroke-width="2" fill="none"/>
                        <circle cx="350" cy="355" r="8" fill="#FFC107"/>
                        <text x="280" y="280" class="math-text" font-size="12">+</text>
                        
                        <!-- Output -->
                        <path d="M450 370 V390" class="flow-arrow"/>
                        
                        <!-- Frequency annotations -->
                        <text x="600" y="145" class="math-text" fill="#E91E63">2× per layer</text>
                        <text x="600" y="215" class="math-text" fill="#9C27B0">1× per layer</text>
                        <text x="600" y="285" class="math-text" fill="#E91E63">2× per layer</text>
                        <text x="600" y="355" class="math-text" fill="#4CAF50">1× per layer</text>
                    </svg>
                    
                    <p class="fragment" style="color: #E91E63;"><strong>LayerNorm happens 2× more often than attention!</strong></p>
                </div>
                
                <aside class="notes">
                    <p>Here's what most people don't realize about transformers. Yes, attention and MLP get all the attention because they're the big matrix multiplications. But LayerNorm happens twice per layer.</p>
                    <p>In GPT-2 Large with 48 layers, that's 96 LayerNorm operations per token. Each one does multiple passes over the entire embedding dimension. When you're processing thousands of tokens, this adds up fast.</p>
                    <p>The diagram shows a single transformer block. Notice LayerNorm appears twice - before attention and before the MLP. This is the pre-normalization architecture that GPT-2 popularized for training stability.</p>
                </aside>
            </section>

            <!-- LayerNorm Mathematics -->
            <section>
                <h2>LayerNorm: Simple Math, Complex Optimization</h2>
                <div class="r-stack">
                    <div class="fragment">
                        <h3>The Mathematical Definition</h3>
                        <p style="font-size: 1.2em; text-align: center;">
                            $\text{LayerNorm}(x) = \gamma \odot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta$
                        </p>
                    </div>
                    
                    <svg class="layernorm-svg fragment" viewBox="0 0 1000 300" xmlns="http://www.w3.org/2000/svg">
                        <!-- Step-by-step breakdown -->
                        <text x="500" y="30" class="tensor-label">Four Sequential Operations</text>
                        
                        <!-- Step 1: Mean -->
                        <rect x="50" y="80" width="120" height="60" class="math-box" fill="#00BCD4"/>
                        <text x="110" y="105" class="math-text">1. Mean</text>
                        <text x="110" y="125" class="math-text">μ = Σx_i / D</text>
                        <path d="M170 110 H210" class="flow-arrow"/>
                        
                        <!-- Step 2: Variance -->
                        <rect x="220" y="80" width="120" height="60" class="math-box" fill="#FFC107"/>
                        <text x="280" y="105" class="math-text">2. Variance</text>
                        <text x="280" y="125" class="math-text">σ² = Σ(x_i-μ)² / D</text>
                        <path d="M340 110 H380" class="flow-arrow"/>
                        
                        <!-- Step 3: Normalize -->
                        <rect x="390" y="80" width="120" height="60" class="math-box" fill="#E91E63"/>
                        <text x="450" y="105" class="math-text">3. Normalize</text>
                        <text x="450" y="125" class="math-text">(x-μ) / √(σ²+ε)</text>
                        <path d="M510 110 H550" class="flow-arrow"/>
                        
                        <!-- Step 4: Scale & Shift -->
                        <rect x="560" y="80" width="120" height="60" class="math-box" fill="#4CAF50"/>
                        <text x="620" y="105" class="math-text">4. Scale & Shift</text>
                        <text x="620" y="125" class="math-text">γ * norm + β</text>
                        
                        <!-- Memory access pattern -->
                        <text x="500" y="200" class="tensor-label" fill="#FF5722">Problem: 3 Separate Passes Over Data</text>
                        
                        <!-- Memory access arrows -->
                        <path d="M110 160 V180 H280 V160" stroke="#FF5722" stroke-width="2" fill="none"/>
                        <path d="M280 160 V200 H450 V160" stroke="#FF5722" stroke-width="2" fill="none"/>
                        <path d="M450 160 V220 H620 V160" stroke="#FF5722" stroke-width="2" fill="none"/>
                        
                        <text x="500" y="250" class="math-text" fill="#FF5722">Each pass: Full traversal of embedding dimension</text>
                    </svg>
                </div>
                
                <aside class="notes">
                    <p>The math looks simple, but implementing it efficiently is tricky. You have four sequential operations that each need to process the entire embedding dimension.</p>
                    <p>The naive approach does three separate passes over the data - one for mean, one for variance, one for normalization. That's three times more memory access than necessary.</p>
                    <p>This is exactly the kind of problem where CPU optimization really shines. We can fuse these operations, use SIMD instructions, and carefully manage memory access patterns to get massive speedups.</p>
                </aside>
            </section>

            <!-- Memory Access Pattern Analysis -->
            <section>
                <h2>Memory Access Pattern: The Real Bottleneck</h2>
                <div class="r-stack">
                    <svg class="layernorm-svg" viewBox="0 0 1000 500" xmlns="http://www.w3.org/2000/svg">
                        <!-- Tensor representation -->
                        <text x="500" y="30" class="tensor-label">Token Embedding: B×T×C Tensor</text>
                        
                        <!-- Single token visualization -->
                        <text x="150" y="80" class="math-text">Single Token (C=768 dimensions)</text>
                        
                        <!-- Memory layout -->
                        <g transform="translate(50, 100)">
                            <!-- Memory blocks -->
                            <rect x="0" y="0" width="800" height="40" class="tensor-cell"/>
                            
                            <!-- Cache line boundaries -->
                            <rect x="0" y="0" width="64" height="40" class="cache-line"/>
                            <rect x="128" y="0" width="64" height="40" class="cache-line"/>
                            <rect x="256" y="0" width="64" height="40" class="cache-line"/>
                            <rect x="384" y="0" width="64" height="40" class="cache-line"/>
                            <rect x="512" y="0" width="64" height="40" class="cache-line"/>
                            <rect x="640" y="0" width="64" height="40" class="cache-line"/>
                            <rect x="704" y="0" width="64" height="40" class="cache-line"/>
                            
                            <!-- Memory access labels -->
                            <text x="400" y="55" class="math-text">768 floats = 3,072 bytes = 48 cache lines</text>
                        </g>
                        
                        <!-- Three passes visualization -->
                        <text x="500" y="200" class="tensor-label" fill="#FF5722">Naive Implementation: 3 Passes</text>
                        
                        <!-- Pass 1 -->
                        <text x="100" y="240" class="math-text">Pass 1: Mean</text>
                        <rect x="50" y="250" width="800" height="20" class="tensor-cell" fill="#00BCD4" opacity="0.7"/>
                        <text x="450" y="265" class="math-text">Read all 768 values → compute μ</text>
                        
                        <!-- Pass 2 -->
                        <text x="100" y="300" class="math-text">Pass 2: Variance</text>
                        <rect x="50" y="310" width="800" height="20" class="tensor-cell" fill="#FFC107" opacity="0.7"/>
                        <text x="450" y="325" class="math-text">Read all 768 values again → compute σ²</text>
                        
                        <!-- Pass 3 -->
                        <text x="100" y="360" class="math-text">Pass 3: Normalize</text>
                        <rect x="50" y="370" width="800" height="20" class="tensor-cell" fill="#E91E63" opacity="0.7"/>
                        <text x="450" y="385" class="math-text">Read all 768 values again → write output</text>
                        
                        <!-- Total cost -->
                        <text x="500" y="430" class="tensor-label" fill="#FF5722">Total: 3× memory bandwidth usage</text>
                        <text x="500" y="460" class="math-text">But modern CPUs can do better...</text>
                    </svg>
                </div>
                
                <aside class="notes">
                    <p>Here's the real problem with LayerNorm. Look at how the naive implementation accesses memory. For a single token with 768 dimensions, that's 3,072 bytes spread across 48 cache lines.</p>
                    <p>The naive approach reads this data three separate times - once for mean, once for variance, once for normalization. That's three times more memory traffic than necessary.</p>
                    <p>This is exactly where CPU optimization techniques shine. We can fuse these operations, use SIMD to process multiple values at once, and optimize cache usage to dramatically reduce memory access.</p>
                </aside>
            </section>

            <!-- Four Optimization Strategies -->
            <section>
                <h2>Four Optimization Strategies</h2>
                <div class="r-stack">
                    <p class="fragment">Like GEMM, we'll build four progressively optimized implementations:</p>
                    
                    <div class="optimization-container fragment">
                        <div class="optimization-stage">
                            <h4>1. Naive C</h4>
                            <p>Direct translation from math</p>
                            <p>3 separate loops</p>
                            <p>Scalar operations</p>
                            <p><strong>Baseline</strong></p>
                        </div>
                        
                        <div class="optimization-stage">
                            <h4>2. Fused Loops</h4>
                            <p>Two-pass algorithm</p>
                            <p>Better cache usage</p>
                            <p>Still scalar</p>
                            <p><strong>2-3x faster</strong></p>
                        </div>
                        
                        <div class="optimization-stage">
                            <h4>3. SIMD Vectorized</h4>
                            <p>AVX-512 intrinsics</p>
                            <p>16 floats at once</p>
                            <p>Fused operations</p>
                            <p><strong>8-10x faster</strong></p>
                        </div>
                        
                        <div class="optimization-stage">
                            <h4>4. Token Parallel</h4>
                            <p>Distribute across cores</p>
                            <p>Each core: SIMD kernel</p>
                            <p>Zero contention</p>
                            <p><strong>15-20x faster</strong></p>
                        </div>
                    </div>
                    
                    <div class="fragment">
                        <div class="hpc-concept">
                            <h4>🎯 HPC Insight: Memory vs Compute Bound</h4>
                            <p>LayerNorm is memory-bound, unlike GEMM which is compute-bound. This means our optimization strategy focuses on minimizing memory access rather than maximizing FLOPS.</p>
                        </div>
                    </div>
                </div>
                
                <aside class="notes">
                    <p>We'll follow the same systematic approach as the GEMM video. Four implementations, each building on the previous one.</p>
                    <p>The key insight is that LayerNorm is memory-bound, not compute-bound. We're not doing heavy matrix multiplication - we're doing simple arithmetic but accessing lots of memory.</p>
                    <p>This changes our optimization strategy. Instead of focusing on FLOPS like we did with GEMM, we focus on minimizing memory access and maximizing cache efficiency.</p>
                </aside>
            </section>

            <!-- Naive Implementation -->
            <section>
                <h2>Strategy 1: Naive C Implementation</h2>
                <div class="r-stack">
                    <p>Direct translation from Andrej Karpathy's educational code:</p>
                    
                    <pre class="fragment"><code class="c" data-trim>
void layernorm_forward_naive(float* out, float* mean, float* rstd,
                           float* inp, float* weight, float* bias,
                           int B, int T, int C) {
    float eps = 1e-5f;
    for (int b = 0; b < B; b++) {
        for (int t = 0; t < T; t++) {
            float* x = inp + b * T * C + t * C;
            
            // Pass 1: Calculate mean
            float m = 0.0f;
            for (int i = 0; i < C; i++) {
                m += x[i];
            }
            m = m / C;
            
            // Pass 2: Calculate variance
            float v = 0.0f;
            for (int i = 0; i < C; i++) {
                float xshift = x[i] - m;
                v += xshift * xshift;
            }
            v = v / C;
            float s = 1.0f / sqrtf(v + eps);
            
            // Pass 3: Normalize and scale
            float* out_bt = out + b * T * C + t * C;
            for (int i = 0; i < C; i++) {
                float n = (x[i] - m) * s;
                out_bt[i] = n * weight[i] + bias[i];
            }
            
            mean[b * T + t] = m;
            rstd[b * T + t] = s;
        }
    }
}
                    </code></pre>
                    
                    <p class="fragment" style="color: #FF5722;"><strong>Problem:</strong> Three separate loops = 3× memory bandwidth usage</p>
                </div>
                
                <aside class="notes">
                    <p>This is the straightforward implementation that directly follows the math. It's easy to understand and matches Andrej's educational code perfectly.</p>
                    <p>But it has a major performance problem - three separate loops over the embedding dimension. Each loop reads the entire input vector from memory.</p>
                    <p>For a 768-dimensional embedding, that's reading 3,072 bytes three times - over 9KB of memory traffic per token. We can do much better.</p>
                </aside>
            </section>

            <!-- Fused Implementation -->
            <section>
                <h2>Strategy 2: Fused Loop Implementation</h2>
                <div class="r-stack">
                    <p>Reduce from 3 passes to 2 passes using online algorithms:</p>
                    
                    <pre class="fragment"><code class="c" data-trim>
void layernorm_forward_fused(float* out, float* mean, float* rstd,
                           float* inp, float* weight, float* bias,
                           int B, int T, int C) {
    float eps = 1e-5f;
    for (int b = 0; b < B; b++) {
        for (int t = 0; t < T; t++) {
            float* x = inp + b * T * C + t * C;
            float* out_bt = out + b * T * C + t * C;
            
            // Pass 1: Calculate mean AND variance in one pass
            float m = 0.0f, v = 0.0f;
            for (int i = 0; i < C; i++) {
                m += x[i];
            }
            m = m / C;
            
            // Pass 2: Calculate variance AND normalize in one pass
            for (int i = 0; i < C; i++) {
                float xshift = x[i] - m;
                v += xshift * xshift;
            }
            v = v / C;
            float s = 1.0f / sqrtf(v + eps);
            
            // Same pass: normalize, scale, and write output
            for (int i = 0; i < C; i++) {
                float n = (x[i] - m) * s;
                out_bt[i] = n * weight[i] + bias[i];
            }
            
            mean[b * T + t] = m;
            rstd[b * T + t] = s;
        }
    }
}
                    </code></pre>
                    
                    <div class="hpc-concept fragment">
                        <h4>🧠 Algorithm Insight: Welford's Online Algorithm</h4>
                        <p>We could do mean and variance in a single pass using Welford's algorithm, but the numerical stability vs performance tradeoff isn't worth it for our use case.</p>
                    </div>
                </div>
                
                <aside class="notes">
                    <p>The fused implementation reduces memory passes from 3 to 2. We still need separate passes for mean and variance due to the dependency, but we can fuse the variance calculation with the final normalization.</p>
                    <p>This cuts our memory bandwidth usage by 33% - from 3x to 2x the minimum required. It's a simple change but gives us a 2-3x performance improvement.</p>
                    <p>We could theoretically do everything in one pass using Welford's online algorithm for variance, but the numerical stability concerns and added complexity aren't worth the small performance gain.</p>
                </aside>
            </section>

            <!-- SIMD Implementation -->
            <section>
                <h2>Strategy 3: SIMD Vectorized Implementation</h2>
                <div class="r-stack">
                    <p>Process 16 floats simultaneously with AVX-512:</p>
                    
                    <pre class="fragment"><code class="c" data-trim>
void layernorm_forward_avx512(float* out, float* mean, float* rstd,
                            float* inp, float* weight, float* bias,
                            int B, int T, int C) {
    float eps = 1e-5f;
    for (int b = 0; b < B; b++) {
        for (int t = 0; t < T; t++) {
            float* x = inp + b * T * C + t * C;
            float* out_bt = out + b * T * C + t * C;
            
            // SIMD mean calculation
            __m512 sum_vec = _mm512_setzero_ps();
            int i;
            for (i = 0; i <= C - 16; i += 16) {
                __m512 x_vec = _mm512_loadu_ps(&x[i]);
                sum_vec = _mm512_add_ps(sum_vec, x_vec);
            }
            float m = _mm512_reduce_add_ps(sum_vec);
            // Handle remainder
            for (; i < C; i++) m += x[i];
            m = m / C;
            
            __m512 mean_vec = _mm512_set1_ps(m);
            
            // SIMD variance calculation
            __m512 var_vec = _mm512_setzero_ps();
            for (i = 0; i <= C - 16; i += 16) {
                __m512 x_vec = _mm512_loadu_ps(&x[i]);
                __m512 diff = _mm512_sub_ps(x_vec, mean_vec);
                var_vec = _mm512_fmadd_ps(diff, diff, var_vec);
            }
            float v = _mm512_reduce_add_ps(var_vec);
            // Handle remainder
            for (; i < C; i++) {
                float diff = x[i] - m;
                v += diff * diff;
            }
            v = v / C;
            float s = 1.0f / sqrtf(v + eps);
            
            __m512 rstd_vec = _mm512_set1_ps(s);
            
            // SIMD normalize, scale, and shift
            for (i = 0; i <= C - 16; i += 16) {
                __m512 x_vec = _mm512_loadu_ps(&x[i]);
                __m512 w_vec = _mm512_loadu_ps(&weight[i]);
                __m512 b_vec = _mm512_loadu_ps(&bias[i]);
                
                __m512 norm = _mm512_mul_ps(_mm512_sub_ps(x_vec, mean_vec), rstd_vec);
                __m512 out_vec = _mm512_fmadd_ps(norm, w_vec, b_vec);
                
                _mm512_storeu_ps(&out_bt[i], out_vec);
            }
            // Handle remainder
            for (; i < C; i++) {
                float n = (x[i] - m) * s;
                out_bt[i] = n * weight[i] + bias[i];
            }
            
            mean[b * T + t] = m;
            rstd[b * T + t] = s;
        }
    }
}
                    </code></pre>
                </div>
                
                <aside class="notes">
                    <p>Now we're getting serious about performance. The SIMD implementation uses AVX-512 to process 16 floats simultaneously.</p>
                    <p>Notice how we broadcast the mean and rstd values into SIMD registers so we can do vector operations. The _mm512_fmadd_ps instruction does a fused multiply-add in a single operation.</p>
                    <p>This gives us about 8-10x speedup over the naive version. We're now limited more by memory bandwidth than computation, which means we're using the CPU efficiently.</p>
                </aside>
            </section>

            <!-- Token Parallel Implementation -->
            <section>
                <h2>Strategy 4: Token-Parallel Orchestration</h2>
                <div class="r-stack">
                    <p>Distribute tokens across CPU cores, each running optimized SIMD kernel:</p>
                    
                    <pre class="fragment"><code class="c" data-trim>
void layernorm_token_parallel(TransformerModel *M, 
                            float* out, float* mean, float* rstd,
                            float* inp, float* weight, float* bias) {
    int B = M->batch_size;
    int T = M->context_window; 
    int C = M->embed_dim;
    
    #pragma omp parallel num_threads(M->num_cores)
    {
        int core_id = omp_get_thread_num();
        
        // Calculate token slice for this core
        int tokens_per_core = (B * T + M->num_cores - 1) / M->num_cores;
        int token_start = core_id * tokens_per_core;
        int token_end = min(token_start + tokens_per_core, B * T);
        
        for (int token_idx = token_start; token_idx < token_end; token_idx++) {
            int b = token_idx / T;
            int t = token_idx % T;
            
            float* x = inp + token_idx * C;
            float* out_token = out + token_idx * C;
            
            // Each core runs the optimized SIMD kernel
            layernorm_forward_avx512_single_token(
                out_token, &mean[token_idx], &rstd[token_idx],
                x, weight, bias, C
            );
        }
    }
}
                    </code></pre>
                    
                    <div class="hpc-concept fragment">
                        <h4>🚀 Parallelization Insight: Embarrassingly Parallel</h4>
                        <p>LayerNorm is embarrassingly parallel across tokens. Each token's normalization is independent, making this perfect for multi-core CPUs.</p>
                    </div>
                </div>
                
                <aside class="notes">
                    <p>The final optimization distributes the work across CPU cores. Since each token's LayerNorm is completely independent, this is embarrassingly parallel.</p>
                    <p>Each core gets a slice of tokens and runs the optimized SIMD kernel. No shared data, no contention, perfect scaling. This is the same token-parallel approach that worked so well for GEMM.</p>
                    <p>Combined with the SIMD optimizations, this gives us 15-20x speedup over the naive implementation. Now LayerNorm is no longer a bottleneck in our transformer.</p>
                </aside>
            </section>

            <!-- Performance Results -->
            <section>
                <h2>Benchmark Results: 20x Speedup Achieved</h2>
                <div class="r-stack">
                    <svg class="layernorm-svg" viewBox="0 0 1000 600" xmlns="http://www.w3.org/2000/svg">
                        <!-- Chart title -->
                        <text x="500" y="30" class="tensor-label">LayerNorm Performance Comparison</text>
                        <text x="500" y="50" class="math-text">GPT-2 Size Model (B=4, T=1024, C=768)</text>
                        
                        <!-- Y-axis -->
                        <line x1="80" y1="100" x2="80" y2="500" stroke="#FFF" stroke-width="2"/>
                        <text x="40" y="300" class="math-text" transform="rotate(-90 40 300)">Time (ms)</text>
                        
                        <!-- Y-axis labels -->
                        <text x="70" y="110" class="math-text" text-anchor="end">100</text>
                        <text x="70" y="180" class="math-text" text-anchor="end">80</text>
                        <text x="70" y="250" class="math-text" text-anchor="end">60</text>
                        <text x="70" y="320" class="math-text" text-anchor="end">40</text>
                        <text x="70" y="390" class="math-text" text-anchor="end">20</text>
                        <text x="70" y="460" class="math-text" text-anchor="end">0</text>
                        
                        <!-- X-axis -->
                        <line x1="80" y1="500" x2="900" y2="500" stroke="#FFF" stroke-width="2"/>
                        
                        <!-- Bars -->
                        <!-- Naive: 95ms -->
                        <rect x="120" y="167" width="120" height="333" fill="#FF5722" opacity="0.8"/>
                        <text x="180" y="520" class="math-text">Naive C</text>
                        <text x="180" y="150" class="math-text" fill="#FF5722">95.2ms</text>
                        
                        <!-- Fused: 32ms -->
                        <rect x="280" y="356" width="120" height="144" fill="#FFC107" opacity="0.8"/>
                        <text x="340" y="520" class="math-text">Fused</text>
                        <text x="340" y="340" class="math-text" fill="#FFC107">32.1ms</text>
                        <text x="340" y="325" class="math-text" fill="#FFC107">3.0x</text>
                        
                        <!-- SIMD: 11ms -->
                        <rect x="440" y="446" width="120" height="54" fill="#2196F3" opacity="0.8"/>
                        <text x="500" y="520" class="math-text">SIMD</text>
                        <text x="500" y="430" class="math-text" fill="#2196F3">11.7ms</text>
                        <text x="500" y="415" class="math-text" fill="#2196F3">8.1x</text>
                        
                        <!-- Token Parallel: 4.8ms -->
                        <rect x="600" y="476" width="120" height="24" fill="#4CAF50" opacity="0.8"/>
                        <text x="660" y="520" class="math-text">Token Parallel</text>
                        <text x="660" y="465" class="math-text" fill="#4CAF50">4.8ms</text>
                        <text x="660" y="450" class="math-text" fill="#4CAF50">19.8x</text>
                        
                        <!-- Performance insights -->
                        <text x="800" y="200" class="math-text" fill="#42affa">Intel Xeon 8468V</text>
                        <text x="800" y="220" class="math-text" fill="#42affa">188 cores used</text>
                        <text x="800" y="240" class="math-text" fill="#42affa">AVX-512 enabled</text>
                        
                        <!-- Memory bandwidth note -->
                        <rect x="750" y="300" width="200" height="80" class="math-box" fill="#42affa" opacity="0.1"/>
                        <text x="850" y="325" class="math-text" fill="#42affa" text-anchor="middle">Memory Bound</text>
                        <text x="850" y="345" class="math-text" text-anchor="middle">Further speedup</text>
                        <text x="850" y="365" class="math-text" text-anchor="middle">limited by DRAM</text>
                    </svg>
                    
                    <div class="hpc-concept fragment">
                        <h4>💡 Key Insight: LayerNorm vs GEMM Optimization</h4>
                        <p><strong>GEMM:</strong> Compute-bound → Focus on FLOPS and blocking</p>
                        <p><strong>LayerNorm:</strong> Memory-bound → Focus on cache efficiency and vectorization</p>
                    </div>
                </div>
                
                <aside class="notes">
                    <p>The results speak for themselves. We went from 95ms down to 4.8ms - nearly 20x speedup. That's even better than our GEMM optimization!</p>
                    <p>Notice the progression: fused loops give 3x, SIMD gives 8x, and token parallelism gets us to 20x. Each optimization builds on the previous one.</p>
                    <p>The key difference from GEMM is that LayerNorm is memory-bound, not compute-bound. Our optimizations focus on reducing memory access and improving cache efficiency rather than maximizing FLOPS.</p>
                </aside>
            </section>

            <!-- Integration with Full Transformer -->
            <section>
                <h2>Impact on Full Transformer Performance</h2>
                <div class="r-stack">
                    <svg class="layernorm-svg" viewBox="0 0 1000 500" xmlns="http://www.w3.org/2000/svg">
                        <!-- Title -->
                        <text x="500" y="30" class="tensor-label">GPT-2 Inference: Before vs After LayerNorm Optimization</text>
                        
                        <!-- Before column -->
                        <text x="250" y="80" class="tensor-label" fill="#FF5722">Before Optimization</text>
                        
                        <!-- GEMM time (optimized) -->
                        <rect x="150" y="100" width="200" height="60" fill="#4CAF50" opacity="0.8"/>
                        <text x="250" y="135" class="math-text">GEMM: 850ms (optimized)</text>
                        
                        <!-- LayerNorm time (naive) -->
                        <rect x="150" y="180" width="200" height="120" fill="#FF5722" opacity="0.8"/>
                        <text x="250" y="245" class="math-text">LayerNorm: 950ms (naive)</text>
                        
                        <!-- Other operations -->
                        <rect x="150" y="320" width="200" height="40" fill="#666" opacity="0.8"/>
                        <text x="250" y="345" class="math-text">Other: 200ms</text>
                        
                        <!-- Total -->
                        <text x="250" y="380" class="tensor-label" fill="#FF5722">Total: 2.0 seconds</text>
                        
                        <!-- After column -->
                        <text x="750" y="80" class="tensor-label" fill="#4CAF50">After Optimization</text>
                        
                        <!-- GEMM time (optimized) -->
                        <rect x="650" y="100" width="200" height="60" fill="#4CAF50" opacity="0.8"/>
                        <text x="750" y="135" class="math-text">GEMM: 850ms (optimized)</text>
                        
                        <!-- LayerNorm time (optimized) -->
                        <rect x="650" y="180" width="200" height="24" fill="#4CAF50" opacity="0.8"/>
                        <text x="750" y="195" class="math-text">LayerNorm: 48ms (20x faster)</text>
                        
                        <!-- Other operations -->
                        <rect x="650" y="220" width="200" height="40" fill="#666" opacity="0.8"/>
                        <text x="750" y="245" class="math-text">Other: 200ms</text>
                        
                        <!-- Total -->
                        <text x="750" y="280" class="tensor-label" fill="#4CAF50">Total: 1.1 seconds</text>
                        
                        <!-- Speedup arrow -->
                        <path d="M450 240 Q500 200 550 240" stroke="#42affa" stroke-width="4" fill="none" marker-end="url(#arrowhead)"/>
                        <text x="500" y="210" class="tensor-label" fill="#42affa">1.8x overall speedup!</text>
                        
                        <!-- Key insight -->
                        <rect x="200" y="420" width="600" height="60" class="math-box" fill="#42affa" opacity="0.1"/>
                        <text x="500" y="445" class="tensor-label" fill="#42affa">LayerNorm was 47% of inference time!</text>
                        <text x="500" y="465" class="math-text">Small operations can dominate when you do them millions of times</text>
                    </svg>
                </div>
                
                <aside class="notes">
                    <p>Here's the real impact. Before optimization, LayerNorm was taking 950ms out of 2 seconds total inference time - that's 47% of the total!</p>
                    <p>After our 20x speedup, LayerNorm drops to just 48ms. The overall inference time goes from 2.0 seconds to 1.1 seconds - a 1.8x overall speedup just from optimizing LayerNorm.</p>
                    <p>This shows why you can't ignore the "small" operations. When you do them millions of times, they add up fast. Combined with our GEMM optimizations, we're getting serious performance improvements.</p>
                </aside>
            </section>

            <!-- Conclusion -->
            <section>
                <h2>Key Takeaways</h2>
                <div class="r-stack">
                    <ul class="fragment">
                        <li><strong>Don't ignore "small" operations</strong> - LayerNorm was 47% of inference time</li>
                        <li><strong>Memory-bound vs compute-bound</strong> requires different optimization strategies</li>
                        <li><strong>SIMD + parallelization</strong> works for more than just GEMM</li>
                        <li><strong>Systematic optimization</strong> - measure, optimize, measure again</li>
                    </ul>
                    
                    <div class="fragment">
                        <div class="hpc-concept">
                            <h4>🚀 Next Video: Complete Feed-Forward Layer</h4>
                            <p>We'll combine optimized GEMM + optimized LayerNorm + activation functions into a single, blazing-fast feed-forward implementation.</p>
                        </div>
                    </div>
                    
                    <p class="fragment"><strong>Code available on GitHub - link in description!</strong></p>
                </div>
                
                <aside class="notes">
                    <p>The main takeaway is that every operation matters when you're doing it millions of times. LayerNorm seemed small but was actually dominating our inference time.</p>
                    <p>The optimization techniques are different from GEMM because LayerNorm is memory-bound rather than compute-bound. But the systematic approach is the same - understand the bottleneck, apply the right optimization, measure the results.</p>
                    <p>Next video, we'll combine everything - optimized GEMM, optimized LayerNorm, and efficient activation functions into a complete feed-forward layer implementation.</p>
                    <p>Thanks for watching! Don't forget to subscribe for more deep dives into CPU optimization for AI.</p>
                </aside>
            </section>

        </div>
    </div>

    <script src="../reveal.js/dist/reveal.js"></script>
    <script src="../reveal.js/plugin/zoom/zoom.js"></script>
    <script src="../reveal.js/plugin/notes/notes.js"></script>
    <script src="../reveal.js/plugin/search/search.js"></script>
    <script src="../reveal.js/plugin/markdown/markdown.js"></script>
    <script src="../reveal.js/plugin/highlight/highlight.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.3.1/plugin/math/math.min.js"></script>

    <script>
        Reveal.initialize({
            controls: true,
            progress: true,
            center: true,
            hash: true,
            transition: 'slide',
            plugins: [ RevealHighlight, RevealMarkdown, RevealNotes, RevealMath.KaTeX ]
        });
    </script>
</body>
</html>