<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>LayerNorm for HPC and AI on CPU</title>

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/dist/reset.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/dist/reveal.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/dist/theme/black.css" id="theme">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/plugin/highlight/monokai.css" id="highlight-theme">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/3.9.1/gsap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.3.1/reveal.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.3.1/plugin/highlight/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.3.1/plugin/markdown/markdown.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.3.1/plugin/notes/notes.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.3.1/plugin/math/math.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>

    <style>
        .reveal { font-family: 'Inter', sans-serif; font-size: 28px; }
        .reveal h1, .reveal h2, .reveal h3, .reveal h4 { text-transform: none; font-weight: 700; }
        .reveal pre code { max-height: 600px; font-size: 0.6em; line-height: 1.2; }
        .reveal section img { border: none; box-shadow: none; }
        .reveal .r-stack { display: flex; flex-direction: column; justify-content: center; align-items: center; height: 100%; font-size: 0.9em; }
        .reveal .r-stack p, .reveal .r-stack ul li { font-size: 0.9em; line-height: 1.4; }
        .llm-layer-svg { width: 95%; height: 95%; margin: auto; display: block; }
        .llm-box { fill: #2a2a2e; stroke: #666; stroke-width: 1.5; rx: 8; ry: 8; }
        .llm-text { fill: #FFF; font-size: 12px; text-anchor: middle; font-family: 'Inter', sans-serif; }
        .llm-dim-text { fill: #AAA; font-size: 10px; text-anchor: middle; font-family: 'Inter', sans-serif; }
        .llm-label { fill: #42affa; font-size: 16px; font-weight: bold; text-anchor: middle; }
        .input-box { fill: #00BCD4; }
        .output-box { fill: #4CAF50; }
        .op-box { fill: #FFC107; }
        .arrow-line { stroke: #FFF; stroke-width: 1.5; fill: none; marker-end: url(#arrowhead); }
        .benchmark-table { width: 90%; margin: 20px auto; border-collapse: collapse; font-size: 0.7em; }
        .benchmark-table th, .benchmark-table td { border: 1px solid #555; padding: 8px; text-align: center; }
        .benchmark-table th { background-color: #333; color: #eee; }
        .benchmark-table tr:nth-child(even) { background-color: #222; }
        .chart-container { width: 80%; max-width: 600px; margin: 20px auto; }
        .kernel-grid { display: grid; grid-template-columns: repeat(2, 1fr); gap: 20px; margin-top: 30px; }
        .kernel-item { background: rgba(255, 255, 255, 0.05); padding: 20px; border-radius: 10px; text-align: left; font-size: 0.8em; }
        .kernel-item h3 { color: #42affa; margin-bottom: 10px; }
    </style>
</head>
<body>
    <div class="reveal">
        <div class="slides">

            <section>
                <h1>LayerNorm for HPC and AI on CPU</h1>
                <h2>From Math to Optimized C Implementation</h2>
                <p><small>Part 2: Building a Fast CPU-Based AI Runtime</small></p>
                <aside class="notes">
                    <p>Welcome back. After covering GEMM in the last video, today we're diving into LayerNorm - another fundamental operation in Transformers that needs serious optimization.</p>
                    <p>I've implemented three different LayerNorm kernels in my C-Transformer project, from a basic version to a production-level implementation with 4-way SIMD unrolling and software prefetching. We'll look at the actual code, the math behind it, and why LayerNorm matters for training stability.</p>
                    <p>By the end, you'll understand what LayerNorm does, why it works, and how to make it run fast on CPUs.</p>
                </aside>
            </section>

            <section>
                <h2>What is Layer Normalization?</h2>
                <h3>Solving the Internal Covariate Shift Problem</h3>
                <div class="r-stack">
                    <p class="fragment">LayerNorm addresses a fundamental problem in deep networks: internal covariate shift.</p>
                    <p class="fragment">As activations flow through layers, their distributions can shift dramatically, making training unstable.</p>
                    <p class="fragment">LayerNorm normalizes each token's embedding to have zero mean and unit variance.</p>
                    <p class="fragment">Unlike BatchNorm, it works per-sample, making it perfect for variable-length sequences.</p>
                    <p class="fragment">GPT-2 uses "pre-normalization" - LayerNorm before each operation, not after.</p>
                </div>
                <aside class="notes">
                    <p>LayerNorm solves a real problem. In deep networks, the distribution of activations can shift as you train, making it hard for later layers to learn effectively. This is called internal covariate shift.</p>
                    <p>The solution is to normalize the activations to have a consistent distribution - zero mean and unit variance. This stabilizes training and lets you use higher learning rates.</p>
                    <p>The key difference from BatchNorm is that LayerNorm normalizes across features for each sample independently. This makes it work well with variable-length sequences where you can't easily batch normalize.</p>
                    <p>In my C-Transformer, I follow the GPT-2 approach with pre-normalization. You apply LayerNorm before the attention and feed-forward operations, not after. This tends to train more stably.</p>
                </aside>
            </section>

            <section>
                <h2>The Math: Transforming Distributions</h2>
                <div class="chart-container">
                    <canvas id="distributionChart" width="400" height="300"></canvas>
                </div>
                <div class="r-stack">
                    <p class="fragment">LayerNorm transforms arbitrary distributions into standard normal distributions.</p>
                    <p class="fragment">Formula: $\text{output} = \gamma \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta$</p>
                    <p class="fragment">Where $\mu$ and $\sigma^2$ are computed per token across embedding dimensions.</p>
                </div>
                <aside class="notes">
                    <p>Let me show you what LayerNorm actually does mathematically. The chart shows three distributions: the original input with some arbitrary mean and variance, the normalized version with zero mean and unit variance, and the final output after learned scaling and shifting.</p>
                    <p>The formula looks complex but it's straightforward. For each token, you compute the mean and variance across all embedding dimensions. Then you subtract the mean and divide by the standard deviation. This gives you a standard normal distribution.</p>
                    <p>Finally, you scale by gamma and shift by beta. These are learned parameters that let the network adjust the final distribution as needed. This gives back the representational power while maintaining training stability.</p>
                </aside>
            </section>

            <section>
                <h2>Memory Layout: The HPC Foundation</h2>
                <div class="r-stack">
                    <svg width="100%" height="500" viewBox="0 0 1000 500" class="fragment">
                        <defs><marker id="arrowhead" markerWidth="10" markerHeight="7" refX="0" refY="3.5" orient="auto"><polygon points="0 0, 10 3.5, 0 7" fill="#FFF"/></marker></defs>
                        <!-- Title -->
                        <text x="500" y="30" class="llm-label" font-size="20">Contiguous Memory Arena: 2MB Huge Pages + 64-byte Alignment</text>
                        
                        <!-- Memory Arena Box -->
                        <rect x="50" y="60" width="900" height="400" fill="none" stroke="#42affa" stroke-width="3" rx="10"/>
                        <text x="60" y="85" class="llm-text" font-size="14">Single mmap() allocation - Zero fragmentation</text>
                        
                        <!-- LayerNorm Weights Section -->
                        <rect x="80" y="100" width="200" height="80" class="llm-box" fill="#E91E63"/>
                        <text x="180" y="125" class="llm-text">LayerNorm Weights</text>
                        <text x="180" y="140" class="llm-dim-text">γ[embed_dim], β[embed_dim]</text>
                        <text x="180" y="155" class="llm-dim-text">Shared across ALL tokens</text>
                        <text x="180" y="170" class="llm-dim-text">64-byte aligned</text>
                        
                        <!-- Token Data Section -->
                        <rect x="320" y="100" width="500" height="150" class="llm-box" fill="#00BCD4"/>
                        <text x="570" y="125" class="llm-text">Token Data: [context_window × aligned_embed_dim]</text>
                        
                        <!-- Core 0 slice -->
                        <rect x="340" y="140" width="100" height="40" class="llm-box" fill="#4CAF50"/>
                        <text x="390" y="165" class="llm-dim-text">Core 0</text>
                        
                        <!-- Core 1 slice -->
                        <rect x="450" y="140" width="100" height="40" class="llm-box" fill="#4CAF50"/>
                        <text x="500" y="165" class="llm-dim-text">Core 1</text>
                        
                        <!-- Core N slice -->
                        <rect x="560" y="140" width="100" height="40" class="llm-box" fill="#4CAF50"/>
                        <text x="610" y="165" class="llm-dim-text">Core N</text>
                        
                        <text x="570" y="200" class="llm-dim-text">Each core: tokens_per_core × aligned_embed_dim</text>
                        <text x="570" y="215" class="llm-dim-text">Sequential access, no false sharing</text>
                        <text x="570" y="230" class="llm-dim-text">get_slice() for core-local access</text>
                        
                        <!-- Algorithm Steps -->
                        <text x="500" y="290" class="llm-label" font-size="16">LayerNorm Algorithm × Memory Layout</text>
                        
                        <!-- Step 1 -->
                        <rect x="80" y="310" width="250" height="60" class="llm-box" fill="#FFC107"/>
                        <text x="205" y="330" class="llm-text">1. Per-token mean & variance</text>
                        <text x="205" y="345" class="llm-dim-text">Compute across embed_dim</text>
                        <text x="205" y="360" class="llm-dim-text">Cache-friendly: 64-byte strides</text>
                        
                        <!-- Step 2 -->
                        <rect x="370" y="310" width="250" height="60" class="llm-box" fill="#9C27B0"/>
                        <text x="495" y="330" class="llm-text">2. Normalize & scale</text>
                        <text x="495" y="345" class="llm-dim-text">Broadcast γ, β to all tokens</text>
                        <text x="495" y="360" class="llm-dim-text">AVX-512: 16 floats/instruction</text>
                        
                        <!-- Step 3 -->
                        <rect x="660" y="310" width="250" height="60" class="llm-box" fill="#FF5722"/>
                        <text x="785" y="330" class="llm-text">3. Token-parallel output</text>
                        <text x="785" y="345" class="llm-dim-text">Each core writes its slice</text>
                        <text x="785" y="360" class="llm-dim-text">No memory contention</text>
                        
                        <!-- Memory arrows -->
                        <path d="M280 140 L320 140" class="arrow-line"/>
                        <path d="M180 180 L340 310" class="arrow-line"/>
                        <path d="M570 250 L495 310" class="arrow-line"/>
                        
                        <!-- Performance callout -->
                        <rect x="80" y="400" width="840" height="40" fill="rgba(66, 175, 250, 0.2)" stroke="#42affa" rx="5"/>
                        <text x="500" y="425" class="llm-text">🚀 Result: 6.8x speedup through optimal memory layout + SIMD + parallelization</text>
                    </svg>
                </div>
                <aside class="notes">
                    <p>This is the key to our performance - the memory layout. Everything is allocated in one contiguous arena using 2MB huge pages, which reduces TLB misses dramatically.</p>
                    <p>LayerNorm weights gamma and beta are 1D tensors with embed_dim elements, shared across all tokens. They're 64-byte aligned and stored once per layer. This is crucial - we're not storing separate weights per token.</p>
                    <p>The token data is laid out sequentially, and we use token-parallel processing. Each CPU core gets a slice of tokens using the get_slice function. This avoids false sharing because each core works on different cache lines.</p>
                    <p>The algorithm maps perfectly to this layout. We compute mean and variance per token across the embedding dimension with cache-friendly access patterns. Then we broadcast the shared gamma and beta parameters to normalize each token. Finally, each core writes its output slice independently.</p>
                    <p>This design is why we get 6.8x speedup - it's not just SIMD or parallelization alone, it's the entire memory architecture working together.</p>
                </aside>
            </section>

            <section>
                <h2>Naive Implementation: The Baseline</h2>
                <p>Three separate loops - simple but inefficient:</p>
                <pre><code class="c" data-trim data-noescape>
void layernorm_forward_naive(float* out, float* inp, float* weight, float* bias,
                             int tokens, int d_model, float eps) {
    for (int t = 0; t < tokens; t++) {
        float* x = inp + t * d_model;
        float* o = out + t * d_model;
        
        // Step 1: Calculate mean
        float mean = 0.0f;
        for (int i = 0; i < d_model; i++) {
            mean += x[i];
        }
        mean /= d_model;
        
        // Step 2: Calculate variance
        float var = 0.0f;
        for (int i = 0; i < d_model; i++) {
            float diff = x[i] - mean;
            var += diff * diff;
        }
        var /= d_model;
        float rstd = 1.0f / sqrtf(var + eps);
        
        // Step 3: Normalize and scale
        for (int i = 0; i < d_model; i++) {
            o[i] = ((x[i] - mean) * rstd) * weight[i] + bias[i];
        }
    }
}
                </code></pre>
                <aside class="notes">
                    <p>This is the straightforward implementation. Three separate loops over the embedding dimension - one for mean, one for variance, one for the final normalization and scaling.</p>
                    <p>It's correct and easy to understand, but it's inefficient. We're doing three passes over the data, which means three times the memory bandwidth. And we're not using any vectorization.</p>
                    <p>But this is a good baseline. It matches the mathematical definition exactly, so we can use it to verify our optimized versions are correct.</p>
                </aside>
            </section>

            <section>
                <h2>Token-Parallel Implementation</h2>
                <p>Add OpenMP parallelization across tokens:</p>
                <pre><code class="c" data-trim data-noescape>
void layernorm_token_parallel(TransformerModel *M, float* out, float* inp,
                              float* weight, float* bias, int tokens, int d_model) {
    float eps = 1e-5f;
    
    #pragma omp parallel num_threads(M->num_cores)
    {
        int core_id = omp_get_thread_num();
        int tokens_per_core = (tokens + M->num_cores - 1) / M->num_cores;
        int start_token = core_id * tokens_per_core;
        int end_token = (start_token + tokens_per_core > tokens) ? tokens : start_token + tokens_per_core;
        int num_tokens = end_token - start_token;
        
        if (num_tokens > 0) {
            float* input = inp + start_token * d_model;
            float* output = out + start_token * d_model;
            
            // Call optimized kernel for this token slice
            layernorm_forward_unrolled(input, weight, bias, output, num_tokens, d_model, eps);
        }
    }
}
                </code></pre>
                <aside class="notes">
                    <p>The first optimization is parallelization. We divide the tokens across CPU cores using OpenMP. Each core gets a slice of tokens to process independently.</p>
                    <p>This avoids memory contention because each core is working on different memory regions. The memory access pattern is still sequential within each core, which is cache-friendly.</p>
                    <p>Note that we call into a separate `layernorm_forward_unrolled` function. This is where the real optimization happens - that's our vectorized kernel that processes multiple tokens using SIMD instructions.</p>
                </aside>
            </section>

            <section>
                <h2>Production Implementation: 4-Way SIMD Unrolling</h2>
                <p>AVX-512 with software prefetching and FMA instructions:</p>
                <pre><code class="c" data-trim data-noescape>
void layernorm_forward_unrolled(const float * __restrict input,
                                const float * __restrict gamma,
                                const float * __restrict beta,
                                float * __restrict output,
                                int tokens, int d_model, float eps) {
    #pragma omp parallel for schedule(static)
    for (int t = 0; t < tokens; ++t) {
        const float * __restrict x = input + t * d_model;
        float * __restrict out = output + t * d_model;
        
        // 4-way SIMD unrolling for mean calculation
        __m512 acc0 = _mm512_setzero_ps();
        __m512 acc1 = _mm512_setzero_ps();
        __m512 acc2 = _mm512_setzero_ps();
        __m512 acc3 = _mm512_setzero_ps();
        
        int i = 0;
        for (; i <= d_model - 64; i += 64) {
            // Software prefetching
            _mm_prefetch((const char*)(x + i + 64), _MM_HINT_T0);
            
            __m512 v0 = _mm512_load_ps(x + i);
            __m512 v1 = _mm512_load_ps(x + i + 16);
            __m512 v2 = _mm512_load_ps(x + i + 32);
            __m512 v3 = _mm512_load_ps(x + i + 48);
            
            acc0 = _mm512_add_ps(acc0, v0);
            acc1 = _mm512_add_ps(acc1, v1);
            acc2 = _mm512_add_ps(acc2, v2);
            acc3 = _mm512_add_ps(acc3, v3);
        }
        // ... variance and normalization with similar unrolling
    }
}
                </code></pre>
                <aside class="notes">
                    <p>This is the production-level implementation. We're using AVX-512 instructions to process 16 floats at a time, and we're unrolling the loop 4 ways to process 64 floats per iteration.</p>
                    <p>The software prefetching hints help keep data in cache. We're telling the CPU to fetch the next chunk of data while we're processing the current chunk.</p>
                    <p>This pattern continues for variance calculation and the final normalization step. Each step uses the same 4-way unrolling with SIMD instructions.</p>
                    <p>The key insight is that we're doing all three steps - mean, variance, and normalization - in one pass through the data. This minimizes memory bandwidth and maximizes cache efficiency.</p>
                </aside>
            </section>

            <section>
                <h2>Why LayerNorm Everywhere? The Mathematical Necessity</h2>
                <div class="chart-container">
                    <canvas id="layerDistributionChart" width="800" height="400"></canvas>
                </div>
                <div class="r-stack">
                    <p class="fragment">Each operation in a Transformer shifts the activation distribution</p>
                    <p class="fragment">Without normalization, distributions drift and training becomes unstable</p>
                    <p class="fragment">LayerNorm restores the "sweet spot" - zero mean, unit variance</p>
                </div>
                <aside class="notes">
                    <p>This chart shows what happens to activations as they flow through a Transformer layer. The blue line shows the initial distribution - nicely centered and well-behaved.</p>
                    <p>After the attention operation, the distribution shifts and spreads out - the red line. This is why we need LayerNorm before the MLP. Without it, the MLP receives inputs with unpredictable statistics.</p>
                    <p>After the MLP, the distribution shifts again - the yellow line. This is why we need another LayerNorm before the next layer.</p>
                    <p>LayerNorm isn't just an optimization - it's mathematically necessary. Each operation changes the statistical properties of the data, and subsequent operations depend on those properties being stable.</p>
                </aside>
            </section>

            <section>
                <h2>Transformer Architecture: LayerNorm Placement</h2>
                <div class="r-stack">
                    <svg width="100%" height="500" viewBox="0 0 800 500" class="fragment">
                        <defs><marker id="arrowhead" markerWidth="10" markerHeight="7" refX="0" refY="3.5" orient="auto"><polygon points="0 0, 10 3.5, 0 7" fill="#FFF"/></marker></defs>
                        
                        <!-- Title -->
                        <text x="400" y="30" class="llm-label" font-size="18">Pre-Normalization: LayerNorm Before Every Operation</text>
                        
                        <!-- Input -->
                        <rect x="350" y="60" width="100" height="30" class="llm-box" fill="#00BCD4"/>
                        <text x="400" y="80" class="llm-text">Input Tokens</text>
                        <path d="M400 90 L400 110" class="arrow-line"/>
                        
                        <!-- LayerNorm 1 -->
                        <rect x="320" y="120" width="160" height="25" class="llm-box" fill="#E91E63"/>
                        <text x="400" y="137" class="llm-text">LayerNorm</text>
                        <path d="M400 145 L400 165" class="arrow-line"/>
                        
                        <!-- Multi-Head Attention -->
                        <rect x="300" y="175" width="200" height="40" class="llm-box" fill="#9C27B0"/>
                        <text x="400" y="200" class="llm-text">Multi-Head Attention</text>
                        <path d="M400 215 L400 235" class="arrow-line"/>
                        
                        <!-- Residual Connection 1 -->
                        <circle cx="400" cy="245" r="15" fill="#FFC107" stroke="#666" stroke-width="2"/>
                        <text x="400" y="250" class="llm-text" font-size="14">+</text>
                        <path d="M385 245 L200 245 L200 75 L350 75" class="arrow-line" stroke-width="1" stroke-dasharray="5,5"/>
                        <path d="M400 260 L400 280" class="arrow-line"/>
                        
                        <!-- LayerNorm 2 -->
                        <rect x="320" y="290" width="160" height="25" class="llm-box" fill="#E91E63"/>
                        <text x="400" y="307" class="llm-text">LayerNorm</text>
                        <path d="M400 315 L400 335" class="arrow-line"/>
                        
                        <!-- MLP -->
                        <rect x="350" y="345" width="100" height="40" class="llm-box" fill="#4CAF50"/>
                        <text x="400" y="370" class="llm-text">MLP</text>
                        <path d="M400 385 L400 405" class="arrow-line"/>
                        
                        <!-- Residual Connection 2 -->
                        <circle cx="400" cy="415" r="15" fill="#FFC107" stroke="#666" stroke-width="2"/>
                        <text x="400" y="420" class="llm-text" font-size="14">+</text>
                        <path d="M385 415 L250 415 L250 260 L385 260" class="arrow-line" stroke-width="1" stroke-dasharray="5,5"/>
                        <path d="M400 430 L400 450" class="arrow-line"/>
                        
                        <!-- Output -->
                        <rect x="340" y="460" width="120" height="30" class="llm-box" fill="#FF5722"/>
                        <text x="400" y="480" class="llm-text">To Next Layer</text>
                        
                        <!-- Annotations -->
                        <text x="520" y="140" class="llm-dim-text">Stabilizes attention inputs</text>
                        <text x="520" y="310" class="llm-dim-text">Stabilizes MLP inputs</text>
                        <text x="520" y="250" class="llm-dim-text">Residual preserves gradients</text>
                        <text x="520" y="420" class="llm-dim-text">Combines processed info</text>
                    </svg>
                </div>
                <aside class="notes">
                    <p>This diagram shows the GPT-2 style pre-normalization architecture. LayerNorm appears before every major operation - before attention and before the MLP.</p>
                    <p>Why this pattern? Each operation distorts the activation distribution. Attention mixes information across positions, which can create correlations that shift the mean and variance. The MLP applies non-linear transformations that further change the distribution.</p>
                    <p>By placing LayerNorm before each operation, we ensure that every operation receives inputs with stable statistical properties. This makes training more stable and allows higher learning rates.</p>
                    <p>The residual connections are crucial too - they provide a direct path for gradients to flow backward, preventing the vanishing gradient problem. But they also mean that the distribution keeps getting modified by these residual additions.</p>
                    <p>This is why we need LayerNorm everywhere - it's not just good practice, it's mathematically necessary for deep networks to train effectively.</p>
                </aside>
            </section>

            <section>
                <h2>Conclusion</h2>
                <div class="r-stack">
                    <p>LayerNorm is the mathematical foundation that makes deep networks trainable.</p>
                    <p class="fragment">It's not just normalization - it's distribution control for stable learning.</p>
                    <p class="fragment">Every operation needs it because every operation changes activation statistics.</p>
                    <p class="fragment">From mathematical necessity to optimized implementation: HPC makes theory practical.</p>
                    <p class="fragment"><strong>Next Video:</strong> Complete Feed-Forward Layer - We'll combine optimized GEMM + optimized LayerNorm + activation functions into a single, blazing-fast feed-forward implementation.</p>
                </div>
                <aside class="notes">
                    <p>LayerNorm is a perfect example of how mathematics and engineering work together in AI. The math tells us we need stable distributions for training, and the engineering tells us how to compute it efficiently.</p>
                    <p>The key insight is that LayerNorm isn't just a "nice to have" - it's mathematically necessary. Without it, activations drift and training becomes impossible. That's why it appears before every operation in modern architectures.</p>
                    <p>The optimization techniques - memory layout, vectorization, parallelization - they're what make this mathematical necessity practical on real hardware. You can have the best algorithm in the world, but if it's slow, nobody will use it.</p>
                    <p>Next video, we'll put it all together. We'll combine the optimized GEMM from last video with this optimized LayerNorm, add activation functions, and build a complete feed-forward layer that runs fast on CPUs.</p>
                    <p>Thank you for watching. If this was helpful, please like and subscribe. I'll see you in the next video.</p>
                </aside>
            </section>

        </div>
    </div>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            Reveal.initialize({
                controls: true,
                progress: true,
                center: true,
                hash: true,
                transition: 'slide',
                plugins: [ RevealHighlight, RevealMarkdown, RevealNotes, RevealMath.KaTeX ]
            });

            // Initialize Chart.js for distribution visualization
            const ctx = document.getElementById('distributionChart').getContext('2d');
            
            // Generate data for three distributions
            const xValues = [];
            const originalDist = [];
            const normalizedDist = [];
            const finalDist = [];
            
            for (let x = -4; x <= 4; x += 0.1) {
                xValues.push(x);
                // Original distribution (mean=1.5, std=0.8)
                originalDist.push(Math.exp(-0.5 * Math.pow((x - 1.5) / 0.8, 2)) / (0.8 * Math.sqrt(2 * Math.PI)));
                // Normalized distribution (mean=0, std=1)
                normalizedDist.push(Math.exp(-0.5 * x * x) / Math.sqrt(2 * Math.PI));
                // Final distribution after learned scaling and shifting (gamma=1.2, beta=0.3)
                finalDist.push(Math.exp(-0.5 * Math.pow((x - 0.3) / 1.2, 2)) / (1.2 * Math.sqrt(2 * Math.PI)));
            }
            
            new Chart(ctx, {
                type: 'line',
                data: {
                    labels: xValues,
                    datasets: [{
                        label: 'Original Input',
                        data: originalDist,
                        borderColor: '#FF6B6B',
                        backgroundColor: 'rgba(255, 107, 107, 0.1)',
                        fill: true,
                        tension: 0.4
                    }, {
                        label: 'Normalized (μ=0, σ=1)',
                        data: normalizedDist,
                        borderColor: '#4ECDC4',
                        backgroundColor: 'rgba(78, 205, 196, 0.1)',
                        fill: true,
                        tension: 0.4
                    }, {
                        label: 'Final Output (γ, β learned)',
                        data: finalDist,
                        borderColor: '#45B7D1',
                        backgroundColor: 'rgba(69, 183, 209, 0.1)',
                        fill: true,
                        tension: 0.4
                    }]
                },
                options: {
                    responsive: true,
                    plugins: {
                        title: {
                            display: true,
                            text: 'LayerNorm: Distribution Transformation',
                            color: '#fff'
                        },
                        legend: {
                            labels: {
                                color: '#fff'
                            }
                        }
                    },
                    scales: {
                        x: {
                            title: {
                                display: true,
                                text: 'Value',
                                color: '#fff'
                            },
                            ticks: {
                                color: '#fff'
                            },
                            grid: {
                                color: 'rgba(255, 255, 255, 0.1)'
                            }
                        },
                        y: {
                            title: {
                                display: true,
                                text: 'Probability Density',
                                color: '#fff'
                            },
                            ticks: {
                                color: '#fff'
                            },
                            grid: {
                                color: 'rgba(255, 255, 255, 0.1)'
                            }
                        }
                    }
                }
            });

            // Layer distribution chart showing distribution drift through operations
            const layerCtx = document.getElementById('layerDistributionChart').getContext('2d');
            
            const layerXValues = [];
            const initialDist = [];
            const afterAttentionDist = [];
            const afterMLPDist = [];
            
            for (let x = -5; x <= 5; x += 0.1) {
                layerXValues.push(x);
                // Initial well-behaved distribution
                initialDist.push(Math.exp(-0.5 * x * x) / Math.sqrt(2 * Math.PI));
                // After attention - shifted and spread out
                afterAttentionDist.push(Math.exp(-0.5 * Math.pow((x - 1.8) / 1.4, 2)) / (1.4 * Math.sqrt(2 * Math.PI)));
                // After MLP - further shifted and distorted
                afterMLPDist.push(Math.exp(-0.5 * Math.pow((x + 0.7) / 2.1, 2)) / (2.1 * Math.sqrt(2 * Math.PI)));
            }
            
            new Chart(layerCtx, {
                type: 'line',
                data: {
                    labels: layerXValues,
                    datasets: [{
                        label: 'Initial (Stable)',
                        data: initialDist,
                        borderColor: '#4ECDC4',
                        backgroundColor: 'rgba(78, 205, 196, 0.1)',
                        fill: true,
                        tension: 0.4
                    }, {
                        label: 'After Attention (Shifted)',
                        data: afterAttentionDist,
                        borderColor: '#FF6B6B',
                        backgroundColor: 'rgba(255, 107, 107, 0.1)',
                        fill: true,
                        tension: 0.4
                    }, {
                        label: 'After MLP (Drifted)',
                        data: afterMLPDist,
                        borderColor: '#FFC107',
                        backgroundColor: 'rgba(255, 193, 7, 0.1)',
                        fill: true,
                        tension: 0.4
                    }]
                },
                options: {
                    responsive: true,
                    plugins: {
                        title: {
                            display: true,
                            text: 'Distribution Drift Through Transformer Operations',
                            color: '#fff'
                        },
                        legend: {
                            labels: {
                                color: '#fff'
                            }
                        }
                    },
                    scales: {
                        x: {
                            title: {
                                display: true,
                                text: 'Activation Value',
                                color: '#fff'
                            },
                            ticks: {
                                color: '#fff'
                            },
                            grid: {
                                color: 'rgba(255, 255, 255, 0.1)'
                            }
                        },
                        y: {
                            title: {
                                display: true,
                                text: 'Probability Density',
                                color: '#fff'
                            },
                            ticks: {
                                color: '#fff'
                            },
                            grid: {
                                color: 'rgba(255, 255, 255, 0.1)'
                            }
                        }
                    }
                }
            });
        });
    </script>
</body>
</html>