<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>LayerNorm: The Complete Story</title>

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/dist/reset.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/dist/reveal.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/dist/theme/black.css" id="theme">
    <link rel="stylesheet" href="../reveal.js/plugin/highlight/monokai.css" />
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;700&display=swap" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/3.11.4/gsap.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
        <script src="https://cdn.tailwindcss.com"></script>

    <style>
        :root {
            --r-main-font: 'Inter', sans-serif;
            --r-heading-font: 'Inter', sans-serif;
            --c-blue: #42a5f5;
            --c-green: #66bb6a;
            --c-orange: #ffa726;
            --c-red: #ef5350;
            --c-purple: #ab47bc;
            --c-bert: #ff7043;
            --c-gpt: #4ecdc4;
            --c-bg: #263238;
            --c-text: #eceff1;
        }
        .reveal { 
            font-family: var(--r-main-font); 
            font-size: 20px;
            background-color: #1a1a1a;
            font-family: 'Inter', sans-serif;
        }
        .reveal h1, .reveal h2, .reveal h3, .reveal h4 { 
            text-transform: none; 
            font-weight: 700; 
            color: var(--c-text); 
        }
        .reveal .highlight { color: var(--c-orange); }
        .reveal .bert-color { color: var(--c-bert); }
        .reveal .gpt-color { color: var(--c-gpt); }
        .reveal .dim { opacity: 0.7; }
        .reveal .smaller { font-size: 0.8em; }
        .reveal .larger { font-size: 1.2em; }
        .reveal .fragment.highlight-current-blue { background: var(--c-blue); }
        
        .chart-container {
            background: rgba(255, 255, 255, 0.05);
            border-radius: 15px;
            padding: 20px;
            margin: 20px 0;
        }
        
        .math-insight {
            background: rgba(66, 165, 245, 0.1);
            border: 2px solid var(--c-blue);
            border-radius: 10px;
            padding: 20px;
            margin: 20px 0;
        }
        
        .two-col {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 30px;
            align-items: center;
        }
        
        .three-col {
            display: grid;
            grid-template-columns: 1fr 1fr 1fr;
            gap: 30px;
            align-items: center;
        }
        
        .viz-svg {
            width: 100%;
            max-width: 400px;
            height: auto;
        }
        
        .svg-arrow {
            stroke: #fff;
            stroke-width: 2;
            fill: none;
            marker-end: url(#arrowhead);
        }
        
        .svg-dashed-arrow {
            stroke: var(--c-blue);
            stroke-width: 2;
            fill: none;
            stroke-dasharray: 5,5;
        }
        
        .svg-box {
            fill: rgba(255,255,255,0.1);
            stroke: #fff;
            stroke-width: 2;
        }
        
        .svg-text {
            fill: #fff;
            text-anchor: middle;
            dominant-baseline: middle;
            font-size: 14px;
            font-weight: bold;
        }
        
        .svg-label {
            fill: #fff;
            text-anchor: middle;
            dominant-baseline: middle;
            font-size: 16px;
            font-weight: bold;
        }
        
        .svg-desc {
            fill: #ccc;
            text-anchor: middle;
            dominant-baseline: middle;
            font-size: 12px;
        }
        
        .timeline {
            position: relative;
            padding: 20px 0;
        }
        
        .timeline-item {
            background: rgba(255,255,255,0.05);
            border-radius: 10px;
            padding: 15px;
            margin: 10px 0;
            border-left: 4px solid var(--c-blue);
        }
        
        .code-comparison {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 20px 0;
        }
        
        .code-block {
            background: #1e1e1e;
            border-radius: 10px;
            padding: 15px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
        }
        
        .photo-container {
            display: flex;
            align-items: center;
            justify-content: space-between;
            margin: 40px 0;
        }
        
        .photo-card {
            text-align: center;
            padding: 20px;
            background: rgba(255,255,255,0.05);
            border-radius: 15px;
            flex: 1;
            margin: 0 10px;
        }
        
        .photo-element {
            width: 200px;
            height: 150px;
            border-radius: 10px;
            margin: 10px auto;
            transition: all 0.5s ease;
        }
        
        .section-progress {
            position: absolute;
            top: 10px;
            right: 10px;
            background: rgba(0,0,0,0.7);
            padding: 8px 12px;
            border-radius: 20px;
            font-size: 0.7em;
            z-index: 1000;
        }
        .formula-box {
            font-size: 72px;
        }
                .label { font-size: 14px; font-weight: 500; fill: #334155; }
        .title { font-size: 20px; font-weight: 700; fill: #0f172a; }
        .sub-label { font-size: 14px; fill: #475569; }
        .small-label { font-size: 10px; fill: #475569; }
        .mono-label { font-family: 'monospace'; font-size: 11px; fill: #1e293b; }
        .exec-label { font-family: 'monospace'; font-size: 11px; font-weight: 500; fill: white; }
        .arrow-path { stroke-width: 2.5; fill: none; }
        .data-path { stroke-dasharray: 6 4; animation: flow 1.5s linear infinite; }
        .gamma-path { stroke-dasharray: 4 4; animation: flow 2s linear infinite; }
        @keyframes flow { from { stroke-dashoffset: 20; } to { stroke-dashoffset: 0; } }
    </style>
</head>
<body>
    <div class="reveal">
        <div class="slides">

            <!-- SLIDE 1: Title -->
            <section data-section="intro">
                <div class="section-progress">Part I: Foundation</div>
                <h1>LayerNorm: The Complete Story</h1>
                <h3>From Mathematical Foundation to Modern AI Revolution</h3>
                <p class="dim">What it is • Why it matters • How it works • When it changed everything</p>
                
                <div style="margin-top: 40px;">
                    <div class="three-col">
                        <div style="text-align: center;">
                            <div style="font-size: 40px; color: var(--c-blue);">🧮</div>
                            <h4>Mathematics</h4>
                            <p class="smaller dim">Internal covariate shift<br>Distribution control</p>
                        </div>
                        <div style="text-align: center;">
                            <div style="font-size: 40px; color: var(--c-orange);">⚡</div>
                            <h4>Revolution</h4>
                            <p class="smaller dim">BERT → GPT-2<br>Architectural breakthrough</p>
                        </div>
                        <div style="text-align: center;">
                            <div style="font-size: 40px; color: var(--c-green);">🔧</div>
                            <h4>Implementation</h4>
                            <p class="smaller dim">HPC optimization<br>Memory & SIMD</p>
                        </div>
                    </div>
                </div>
                
                <aside class="notes">
                    Welcome everyone! Today we're exploring LayerNorm - one of the most important yet underappreciated innovations in modern AI. This presentation will take you from basic concepts to production-level HPC implementation. We'll cover the mathematical foundation, the revolutionary architectural insight, and how to optimize it for real-world performance. LayerNorm is the unsung hero that made GPT-3, ChatGPT, and the entire transformer revolution possible.
                </aside>
            </section>

            <!-- SLIDE 2: The Problem -->
            <section data-section="intro">
                <h2>The Problem: Internal Covariate Shift</h2>
                <p class="smaller">Neural networks work best when inputs have stable statistical properties, but deep networks naturally create chaos.</p>
                
                <div class="chart-container">
                    <canvas id="problemChart" width="600" height="300"></canvas>
                </div>
                
                <div class="math-insight">
                    <h4>🎯 The Core Problem</h4>
                    <p class="smaller">As network weights change during training, the distribution of activations constantly shifts. Later layers can't learn stable patterns when their inputs are chaotic.</p>
                </div>
                
                <aside class="notes">
                    Imagine trying to learn when every piece of information comes in a completely different format. One day numbers are 0-1, the next day 1000-2000. This is internal covariate shift - the fundamental problem LayerNorm solves. Look at this chart showing how activations explode by layer 10 and become astronomical by layer 20. Without LayerNorm, training deep networks was nearly impossible because each layer had to constantly readjust to changing input distributions.
                </aside>
            </section>

            <!-- SLIDE 3: Photo Editor Analogy -->
            <section data-section="intro">
                <h2>The Intuition: LayerNorm as a Photo Editor</h2>
                <p class="smaller">Think of LayerNorm as the "auto-adjust" button in photo editing software.</p>
                
                <div class="photo-container">
                    <div class="photo-card">
                        <h4>Raw Photo</h4>
                        <div id="chaotic-photo" class="photo-element"></div>
                        <p class="smaller dim">Chaotic, hard to work with</p>
                    </div>
                    <div style="font-size: 60px; color: var(--c-orange);">→</div>
                    <div class="photo-card">
                        <h4>Auto-Normalize</h4>
                        <div id="processing-photo" class="photo-element"></div>
                        <p class="smaller dim">LayerNorm processing...</p>
                    </div>
                    <div style="font-size: 60px; color: var(--c-orange);">→</div>
                    <div class="photo-card">
                        <h4>Clean Canvas</h4>
                        <div id="normalized-photo" class="photo-element"></div>
                        <p class="smaller dim">Ready for next operation</p>
                    </div>
                </div>
                
                <aside class="notes">
                    Here's the perfect analogy - LayerNorm is like a photo editor's auto-adjust button. On the left, your raw photo is over-exposed with wonky colors, hard to work with. LayerNorm hits auto-adjust, normalizing brightness and contrast. Now you have a clean, standardized canvas ready for the next editing step. In neural networks, each layer gets this normalized version of the previous layer's output, letting it focus on learning patterns instead of fighting chaos.
                </aside>
            </section>

            <!-- SLIDE 4: Complete Transformer Architecture -->
            <section data-section="math">
                <h2>Where Exactly Is LayerNorm Applied?</h2>
                <p class="smaller">Let's see the complete computation flow and where LayerNorm fits to control variance.</p>
                
                <svg class="viz-svg" viewBox="0 0 1400 500" style="background-color: #1a1a1a;">
                    <!-- Complete transformer layer diagram showing both LayerNorm applications -->
                    <text x="700" y="25" class="svg-label" font-size="16">Complete Transformer Layer: TWO LayerNorm Applications</text>
                    
                    <!-- Input through both LayerNorms and functions -->
                    <rect x="50" y="100" width="90" height="50" fill="var(--c-blue)" opacity="0.7"/>
                    <text x="95" y="125" class="svg-text">Input</text>
                    
                    <!-- LayerNorm 1 -->
                    <rect x="190" y="80" width="100" height="90" fill="var(--c-green)" opacity="0.8"/>
                    <text x="240" y="125" class="svg-text">LayerNorm₁</text>
                    
                    <!-- Attention -->
                    <rect x="340" y="90" width="100" height="70" fill="var(--c-purple)" opacity="0.7"/>
                    <text x="390" y="125" class="svg-text">Attention</text>
                    
                    <!-- Residual 1 -->
                    <circle cx="500" cy="125" r="15" fill="var(--c-orange)"/>
                    <text x="500" y="130" class="svg-text">+</text>
                    
                    <!-- LayerNorm 2 -->
                    <rect x="560" y="80" width="100" height="90" fill="var(--c-green)" opacity="0.8"/>
                    <text x="610" y="125" class="svg-text">LayerNorm₂</text>
                    
                    <!-- FFN -->
                    <rect x="710" y="90" width="100" height="70" fill="var(--c-purple)" opacity="0.7"/>
                    <text x="760" y="125" class="svg-text">FFN</text>
                    
                    <!-- Residual 2 -->
                    <circle cx="870" cy="125" r="15" fill="var(--c-orange)"/>
                    <text x="870" y="130" class="svg-text">+</text>
                    
                    <!-- Output -->
                    <rect x="930" y="100" width="90" height="50" fill="var(--c-orange)" opacity="0.7"/>
                    <text x="975" y="125" class="svg-text">Output</text>
                </svg>
                
                <aside class="notes">
                    This is crucial - LayerNorm appears TWICE in every transformer layer! First, LayerNorm₁ before the attention mechanism normalizes before computing Q, K, V. Second, LayerNorm₂ before the feed-forward network provides another normalization. Notice the pattern: Input → LayerNorm → Function → Residual → LayerNorm → Function → Residual. Each LayerNorm has separate γ and β parameters. This pattern repeats for every layer - GPT-3 has 96 layers, so 192 LayerNorm operations! The complete formula: z = y + FFN(LayerNorm₂(y)) where y = x + Attention(LayerNorm₁(x)).
                </aside>
            </section>

            <!-- SLIDE 5: Complete Formula -->
            <section>
                <h2>The Full LayerNorm Formula</h2>
                <p class="smaller">At a high level, the entire operation is captured by this equation:</p>
                <div class="formula-box" style="font-size:72px;">
                    $$ y = \gamma \odot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta $$
                </div>
                <p class="fragment smaller">Let's break this down piece by piece.</p>
                
                <aside class="notes">
                    Here's the complete LayerNorm equation. Don't worry if this looks complex - we'll break down each piece step by step. This single equation powers modern AI. y equals gamma element-wise multiply the normalized input plus beta. The normalization is x minus mean, divided by square root of variance plus epsilon. This transforms chaotic activations into stable, learnable representations.
                </aside>
            </section>

            <!-- SLIDE 6: Step 1 - Mean -->
            <section>
                <h2>Step 1: The Mean (μ)</h2>
                <p class="smaller">First, we need to find the average value of all the features in our input token.</p>
                <div class="formula-box">
                    $$ \mu = \frac{1}{C} \sum_{i=1}^{C} x_i $$
                </div>
                <div class="explanation">
                    <ul>
                        <li><span class="highlight">$x_i$</span>: This is the i-th feature of our single input token vector.</li>
                        <li><span class="highlight">$C$</span>: This is the total number of features, i.e., the embedding dimension.</li>
                        <li><span class="highlight">$\sum x_i$</span>: We sum up every single feature value in the vector.</li>
                        <li><span class="highlight">$\frac{1}{C} ...$</span>: We divide that sum by the number of features to get the mean.</li>
                    </ul>
                </div>
                
                <aside class="notes">
                    Step 1: compute the mean of all features in our token. Mu equals 1 over C times the sum of all x_i. For a typical transformer with C=4096, we're averaging 4096 numbers into one scalar. This gives us the center point of our data. For example, if features are [2, 4, 6, 8], our mean is 20 divided by 4 equals 5. This becomes our reference point for normalization.
                </aside>
            </section>

            <!-- SLIDE 7: Step 2 - Variance -->
            <section>
                <h2>Step 2: The Variance (σ²)</h2>
                <p class="smaller">Next, we calculate how much our features vary from the mean we just found.</p>
                <div class="formula-box">
                    $$ \sigma^2 = \frac{1}{C} \sum_{i=1}^{C} (x_i - \mu)^2 $$
                </div>
                 <div class="explanation">
                    <ul>
                        <li><span class="highlight">$(x_i - \mu)$</span>: For each feature, we find the difference between its value and the mean.</li>
                        <li><span class="highlight">$(...)^2$</span>: We square that difference. (This ensures the result is positive and penalizes larger deviations more).</li>
                        <li><span class="highlight">$\sum ...$</span>: We sum up all these squared differences.</li>
                        <li><span class="highlight">$\frac{1}{C} ...$</span>: We find the average of the squared differences, which is the variance.</li>
                    </ul>
                </div>
                
                <aside class="notes">
                    Step 2: measure how much features spread from the mean. Sigma squared equals 1 over C times the sum of x_i minus mu squared. Why squared? Two reasons: makes everything positive, and penalizes outliers more heavily. Continuing our example with mean 5: differences are [-3, -1, 1, 3], squared gives [9, 1, 1, 9], variance equals 5. High variance means chaotic activations, low variance means stable activations.
                </aside>
            </section>

            <!-- SLIDE 8: Step 3 - Normalization -->
            <section>
                <h2>Step 3: Normalization</h2>
                <p class="smaller">Now we use the mean and variance to create a clean, standardized vector.</p>
                <div class="formula-box">
                    $$ \hat{x}_i = \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}} $$
                </div>
                <div class="explanation">
                    <ul>
                        <li><span class="highlight">$x_i - \mu$</span>: We center the data around zero.</li>
                        <li><span class="highlight">$\sqrt{\sigma^2 + \epsilon}$</span>: This is the standard deviation (rstd). The tiny <span class="highlight">$\epsilon$</span> (epsilon) is added for numerical stability to prevent division by zero if the variance is zero.</li>
                        <li>The result, <span class="highlight">$\hat{x}$</span>, is a new vector where the features have a mean of 0 and a variance of 1.</li>
                    </ul>
                </div>
                
                <aside class="notes">
                    Step 3: the heart of LayerNorm. X-hat equals x_i minus mu, divided by square root of sigma squared plus epsilon. Step by step: subtract mean to center at zero, divide by standard deviation to normalize spread. The tiny epsilon prevents division by zero - usually 1e-5. Result: every token now has mean zero, variance one. This is the standardization that makes learning stable.
                </aside>
            </section>

            <!-- SLIDE 9: Step 4 & Dimensions -->
            <section>
                <h2>Step 4 & Dimensions</h2>
                <p class="smaller">Finally, we scale and shift the normalized vector using learned parameters.</p>
                <div class="formula-box">
                    $$ y = \gamma \odot \hat{x} + \beta $$
                </div>
                <p class="smaller" style="margin-top: 30px;">Here are the dimensions of each tensor for a <span class="highlight">single token</span> (batch size = 1):</p>
                <table class="dim-table">
                    <thead>
                        <tr><th>Variable</th><th>Symbol</th><th>Dimension</th><th>Description</th></tr>
                    </thead>
                    <tbody>
                        <tr><td>Input</td><td>$x$</td><td>$[C]$</td><td>The 1D vector for one token.</td></tr>
                        <tr><td>Mean</td><td>$\mu$</td><td>Scalar</td><td>A single float, the average of x.</td></tr>
                        <tr><td>Variance</td><td>$\sigma^2$</td><td>Scalar</td><td>A single float, the variance of x.</td></tr>
                        <tr><td>Gamma</td><td>$\gamma$</td><td>$[C]$</td><td>A learned scaling parameter vector.</td></tr>
                        <tr><td>Beta</td><td>$\beta$</td><td>$[C]$</td><td>A learned shifting parameter vector.</td></tr>
                        <tr><td>Output</td><td>$y$</td><td>$[C]$</td><td>The final 1D output vector.</td></tr>
                    </tbody>
                </table>
                
                <aside class="notes">
                    Final step: let the network learn optimal scale and shift. Y equals gamma element-wise multiply x-hat plus beta. Gamma and beta are learned parameters - the network discovers best values during training. Note the dimensions for batch size 1: Input x is just a C-dimensional vector, mean and variance are scalars, gamma and beta are C-dimensional learned parameters. This B=1 simplification is crucial for HPC optimization - no complex batch indexing, just simple vector operations.
                </aside>
            </section>

            <!-- SLIDE 10: Revolution Timeline -->
            <section data-section="revolution">
                <div class="section-progress">Part III: Revolution</div>
                <h2>The LayerNorm Revolution: BERT vs GPT-2</h2>
                <p class="smaller">How moving LayerNorm by 2 lines of code enabled the modern AI era</p>
                
                <div class="timeline">
                    <div class="timeline-item">
                        <h4 style="color: var(--c-bert);">2018: BERT's Post-LN</h4>
                        <p class="smaller">LayerNorm applied AFTER the residual connection</p>
                        <code>output = LayerNorm(x + F(x))</code>
                    </div>
                    <div class="timeline-item">
                        <h4 style="color: var(--c-gpt);">2019: GPT-2's Pre-LN Breakthrough</h4>
                        <p class="smaller">LayerNorm applied BEFORE the function</p>
                        <code>output = x + F(LayerNorm(x))</code>
                    </div>
                </div>
                
                <aside class="notes">
                    Let me tell you the story of how 2 lines of code changed AI forever. 2018: BERT uses Post-LayerNorm - normalize AFTER the function. 2019: GPT-2 makes the breakthrough - normalize BEFORE the function. This tiny change enabled training of massive models like GPT-3. The difference: BERT normalizes x plus F of x, GPT-2 does x plus F of LayerNorm x. Moving LayerNorm from after to before unlocked the modern transformer era.
                </aside>
            </section>

            <!-- SLIDE 11: BERT Post-LN -->
            <section data-section="revolution">
                <h2>BERT's Approach: Post-LN (Normalizing After)</h2>
                <p class="smaller">The original approach normalized <span class="highlight">after</span> adding the residual connection.</p>
                
                <div class="two-col">
                    <div>
                        <svg class="viz-svg" viewBox="0 0 400 400">
                            <!-- BERT Post-LN diagram -->
                            <text x="200" y="30" class="svg-label" fill="var(--c-bert)">BERT-style Post-LN</text>
                            <text x="200" y="70" class="svg-text">Input (x)</text>
                            <!-- Function path -->
                            <rect x="150" y="115" width="100" height="50" class="svg-box" stroke="var(--c-purple)"/>
                            <text x="200" y="145" class="svg-text">F(x)</text>
                            <!-- Residual connection -->
                            <circle cx="200" cy="195" r="12" fill="var(--c-orange)"/>
                            <text x="200" y="198" class="svg-text">+</text>
                            <!-- LayerNorm after residual -->
                            <rect x="125" y="215" width="150" height="50" class="svg-box" stroke="var(--c-green)"/>
                            <text x="200" y="245" class="svg-text">LayerNorm</text>
                            <text x="200" y="295" class="svg-text">Output</text>
                        </svg>
                    </div>
                    <div>
                        <h4>The Equation: <span class="bert-color">`y = LN(x + F(x))`</span></h4>
                        <p class="smaller">Problem: The residual connection `x` gets normalized along with `F(x)`, creating gradient flow issues.</p>
                        
                        <h4 class="fragment">Training Challenges:</h4>
                        <ul class="smaller fragment">
                            <li>Gradients must flow through LayerNorm</li>
                            <li>Residual path is not "clean"</li>
                            <li>Harder to train very deep networks</li>
                            <li>BERT was limited to ~24 layers</li>
                        </ul>
                    </div>
                </div>
                
                <aside class="notes">
                    BERT's approach: y equals LayerNorm of x plus F of x. The residual connection x gets added BEFORE normalization. See how the residual path goes through LayerNorm? This creates instability - gradients have to flow through the normalization operation. This made deep networks hard to train. BERT was limited to 24 layers because of this architectural choice. The problem is that the clean gradient highway gets corrupted by the normalization.
                </aside>
            </section>

            <!-- SLIDE 12: GPT-2 Pre-LN -->
            <section data-section="revolution">
                <h2>GPT-2's Innovation: Pre-LN (Normalizing Before)</h2>
                <p class="smaller">The breakthrough: normalize <span class="highlight">before</span> the function, keeping the residual path clean.</p>
                
                <div class="two-col">
                    <div>
                        <svg class="viz-svg" viewBox="0 0 400 400">
                            <!-- GPT-2 Pre-LN diagram -->
                            <text x="200" y="30" class="svg-label" fill="var(--c-gpt)">GPT-2 style Pre-LN</text>
                            <text x="200" y="70" class="svg-text">Input (x)</text>
                            <!-- LayerNorm before function -->
                            <rect x="125" y="115" width="150" height="50" class="svg-box" stroke="var(--c-green)"/>
                            <text x="200" y="145" class="svg-text">LayerNorm</text>
                            <!-- Function -->
                            <rect x="150" y="195" width="100" height="50" class="svg-box" stroke="var(--c-purple)"/>
                            <text x="200" y="225" class="svg-text">F(LN(x))</text>
                            <!-- Clean residual -->
                            <circle cx="200" cy="315" r="12" fill="var(--c-orange)"/>
                            <text x="200" y="318" class="svg-text">+</text>
                            <text x="200" y="385" class="svg-text">Output</text>
                            <text x="45" y="200" class="svg-desc" fill="var(--c-green)">Clean Path!</text>
                        </svg>
                    </div>
                    <div>
                        <h4>The Equation: <span class="gpt-color">`y = x + F(LN(x))`</span></h4>
                        <p class="smaller">Solution: The residual path `x` stays untouched, providing a clean gradient highway.</p>
                        
                        <h4 class="fragment">Training Benefits:</h4>
                        <ul class="smaller fragment">
                            <li>Clean gradient flow through residuals</li>
                            <li>Much more stable training</li>
                            <li>Can train very deep networks (96+ layers)</li>
                            <li>Enabled GPT-3, ChatGPT, modern LLMs</li>
                        </ul>
                    </div>
                </div>
                
                <aside class="notes">
                    GPT-2's breakthrough: y equals x plus F of LayerNorm x. Notice LayerNorm happens BEFORE the function. The residual connection x stays completely clean! This provides a clean gradient highway from output directly to input. Benefits: stable training, can go much deeper, enabled GPT-3's 96 layers and ChatGPT's scale. This architectural change was the key unlock for modern large language models.
                </aside>
            </section>

            <!-- SLIDE 13: Gradient Flow Comparison -->
            <section data-section="revolution">
                <h2>Why Pre-LN Works: Gradient Flow Analysis</h2>
                <p class="smaller">The key insight: Pre-LN provides uninterrupted gradient highways for stable training.</p>
                
                <div class="two-col">
                    <div>
                        <h4 style="color: var(--c-red);">Post-LN: Gradient Explosion</h4>
                        <svg class="viz-svg" viewBox="0 0 200 400">
                            <!-- Gradient explosion visualization -->
                            <text x="100" y="340" class="svg-desc">Layer 1</text>
                            <text x="100" y="260" class="svg-desc">Layer 10</text>
                            <text x="100" y="180" class="svg-desc">Layer 20</text>
                            <text x="100" y="100" class="svg-desc">Layer 30</text>
                            
                            <path d="M100 320 V 80" stroke="var(--c-red)" stroke-width="8" opacity="0.8"/>
                            <text x="100" y="60" class="svg-text" fill="var(--c-red)">EXPLODES!</text>
                        </svg>
                    </div>
                    <div>
                        <h4 style="color: var(--c-green);">Pre-LN: Stable Gradients</h4>
                        <svg class="viz-svg" viewBox="0 0 200 400">
                            <!-- Stable gradient visualization -->
                            <text x="100" y="340" class="svg-desc">Layer 1</text>
                            <text x="100" y="260" class="svg-desc">Layer 50</text>
                            <text x="100" y="180" class="svg-desc">Layer 96</text>
                            <text x="100" y="100" class="svg-desc">Layer 150</text>
                            
                            <path d="M100 320 V 80" stroke="var(--c-green)" stroke-width="4"/>
                            <text x="100" y="60" class="svg-text" fill="var(--c-green)">Stable!</text>
                        </svg>
                    </div>
                </div>
                
                <aside class="notes">
                    Here's why Pre-LN works: gradient flow analysis. Left side shows Post-LN - gradients explode as they flow backward through layers. Each normalization amplifies the gradient, creating instability. Right side shows Pre-LN - gradients stay stable through the clean residual path. The magic is that uninterrupted highway from output to input. This is why modern transformers can train with hundreds of layers while BERT was limited to 24.
                </aside>
            </section>

            <!-- SLIDE 14: The Code Difference -->
            <section data-section="revolution">
                <h2>The Two Lines That Changed Everything</h2>
                <p class="smaller">The architectural revolution that enabled modern LLMs was literally a 2-line code change.</p>
                
                <div class="code-comparison">
                    <div class="code-block" style="border-left: 4px solid var(--c-red);">
                        <h4 style="color: var(--c-red);">BERT: Post-LN</h4>
                        <pre><code>def transformer_layer(x):
    # Attention block
    attn_out = attention(x)
    x = layer_norm(x + attn_out)  # ❌
    
    # FFN block  
    ffn_out = ffn(x)
    x = layer_norm(x + ffn_out)   # ❌
    return x</code></pre>
                    </div>
                    
                    <div class="code-block" style="border-left: 4px solid var(--c-green);">
                        <h4 style="color: var(--c-green);">GPT-2: Pre-LN</h4>
                        <pre><code>def transformer_layer(x):
    # Attention block
    attn_out = attention(layer_norm(x))  # ✅
    x = x + attn_out
    
    # FFN block
    ffn_out = ffn(layer_norm(x))         # ✅  
    x = x + ffn_out
    return x</code></pre>
                    </div>
                </div>
                
                <div class="fragment" style="margin-top: 30px; text-align: center;">
                    <p class="highlight">Moving `layer_norm()` from line 4 to line 2 enabled GPT-3, ChatGPT, and the entire modern LLM era.</p>
                </div>
                
                <aside class="notes">
                    Here's the actual code difference - just 2 lines! BERT: layer_norm of x plus attention output. GPT-2: x plus attention of layer_norm x. Moving layer_norm from after the addition to before the function call changed everything. This tiny architectural modification enabled GPT-3, ChatGPT, and the entire modern LLM era. Sometimes the most profound advances come from the simplest changes.
                </aside>
            </section>

            <!-- SLIDE 15: Memory Layout Implementation -->
            <section data-section="implementation">
                <div class="section-progress">Part IV: Implementation</div>
               <div class="bg-white p-6 rounded-2xl shadow-lg w-full" style="transform: scale(0.9);">
        <svg viewBox="0 0 1400 850" xmlns="http://www.w3.org/2000/svg" class="w-full">
            <!-- Memory layout to execution visualization -->
            <text x="700" y="40" text-anchor="middle" class="title" style="color:white;">End-to-End LayerNorm: Memory Layout to Core Execution</text>
            
            <!-- DRAM -->
            <rect x="50" y="80" width="300" height="730" rx="15" fill="url(#grad-dram)"/>
            <text x="200" y="110" text-anchor="middle" class="label" fill="white">Main Memory (DRAM)</text>
            
            <!-- Memory layout sections -->
            <rect x="70" y="275" width="260" height="180" rx="5" fill="#fef08a"/>
            <text x="200" y="295" text-anchor="middle" class="small-label">L->layer_input_offset</text>
            
            <rect x="70" y="490" width="260" height="40" rx="5" fill="#f9a8d4"/>
            <text x="200" y="515" text-anchor="middle" class="small-label">L->ln1_weight_offset (gamma)</text>
            
            <rect x="70" y="580" width="260" height="40" rx="5" fill="#fff"/>
            <text x="200" y="605" text-anchor="middle" class="small-label">L->ln1_mean_offset</text>
            
            <rect x="70" y="625" width="260" height="40" rx="5" fill="#fff"/>
            <text x="200" y="650" text-anchor="middle" class="small-label">L->ln1_rstd_offset</text>
            
            <!-- CPU with cores -->
            <rect x="400" y="80" width="990" height="730" rx="15" fill="url(#grad-cpu)"/>
            <text x="875" y="110" text-anchor="middle" class="label" fill="white">CPU</text>
            
            <!-- L3 Cache -->
            <rect x="420" y="140" width="910" height="60" rx="10" fill="url(#grad-cache-l3)"/>
            <text x="875" y="175" text-anchor="middle" class="label" fill="white">Shared L3 Cache</text>
            
            <!-- Core 0 detailed -->
            <rect x="430" y="220" width="500" height="570" rx="10" fill="white" fill-opacity="0.6"/>
            <text x="680" y="245" text-anchor="middle" class="label">Core 0</text>
            
            <!-- L2, L1 caches -->
            <rect x="585" y="260" width="190" height="40" rx="5" fill="url(#grad-cache-l2)"/>
            <text x="680" y="285" text-anchor="middle" class="small-label">L2 Cache</text>
            
            <rect x="610" y="310" width="140" height="40" rx="5" fill="url(#grad-cache-l1)"/>
            <text x="680" y="335" text-anchor="middle" class="small-label">L1 Cache</text>
            
            <!-- Execution unit with 3 passes -->
            <rect x="450" y="370" width="460" height="400" rx="5" fill="url(#grad-exec)"/>
            <text x="680" y="395" text-anchor="middle" class="label" fill="white">Execution Unit</text>
        </svg>
    </div>
                
                <aside class="notes">
                    Now let's see how LayerNorm actually runs on modern hardware. This shows the complete journey from memory to execution. Data starts in DRAM with our aligned memory layout - notice we store mean and rstd for fast backward pass. Key additions: ln1_mean_offset and ln1_rstd_offset for each layer. Data flows through cache hierarchy: L3 shared, L2 private, L1 closest to cores. Finally reaches execution units with AVX-512 doing 16-float operations per cycle. Three passes: compute mean, compute variance, normalize and scale.
                </aside>
            </section>

            <!-- SLIDE 16: SIMD Code Implementation -->
            <section data-section="implementation">
                <h2>SIMD Optimization</h2>
                <div style="text-align: left;">
                    <pre class="smaller" style="height:650px;">
<code data-line-numbers="27-44|45-74|75-100">// High-performance LayerNorm with AVX-512 optimization
void layernorm_forward_rolled_slice(const float *__restrict input_slice_base,
                                    const float *__restrict gamma,
                                    const float *__restrict beta,
                                    float *__restrict output_slice_base,
                                    float *__restrict mean_cache_slice,
                                    float *__restrict rstd_cache_slice,
                                    int num_tokens_in_slice,
                                    int d_model,
                                    int aligned_embed_dim,
                                    float eps)
{
    for (int t = 0; t < num_tokens_in_slice; ++t)
    {
        const float *in_ptr_token = input_slice_base + t * aligned_embed_dim;
        float *out_ptr_token = output_slice_base + t * aligned_embed_dim;

        // Pass 1: Compute mean (AVX-512, rolled)
        __m512 acc_sum_vec = _mm512_setzero_ps();
        int j = 0;
        for (; j <= d_model - 16; j += 16)
        {
            _mm_prefetch((const char *)(in_ptr_token + j + 128), _MM_HINT_T0);
            __m512 v = _mm512_load_ps(in_ptr_token + j);
            acc_sum_vec = _mm512_add_ps(acc_sum_vec, v);
        }
        float mean = _mm512_reduce_add_ps(acc_sum_vec);
        for (; j < d_model; ++j) mean += in_ptr_token[j];
        mean /= (float)d_model;

        // Pass 2: Compute variance (AVX-512, rolled)
        __m512 acc_var_vec = _mm512_setzero_ps();
        __m512 mean_vec = _mm512_set1_ps(mean);
        j = 0;
        for (; j <= d_model - 16; j += 16)
        {
            __m512 v = _mm512_load_ps(in_ptr_token + j);
            __m512 diff = _mm512_sub_ps(v, mean_vec);
            acc_var_vec = _mm512_fmadd_ps(diff, diff, acc_var_vec);
        }
        float var = _mm512_reduce_add_ps(acc_var_vec);
        for (; j < d_model; ++j) {
            float diff = in_ptr_token[j] - mean;
            var += diff * diff;
        }
        var = var / (float)d_model + eps;
        float inv_std = (float)(1.0 / sqrt((double)var));

        // Store mean and rstd for backward pass
        mean_cache_slice[t] = mean;
        rstd_cache_slice[t] = inv_std;

        // Pass 3: Normalize, scale, shift (AVX-512, rolled)
        __m512 inv_std_vec = _mm512_set1_ps(inv_std);
        j = 0;
        for (; j <= d_model - 16; j += 16)
        {
            __m512 v = _mm512_load_ps(in_ptr_token + j);
            __m512 g = _mm512_load_ps(gamma + j);
            __m512 b = _mm512_load_ps(beta + j);
            __m512 n = _mm512_mul_ps(_mm512_sub_ps(v, mean_vec), inv_std_vec);
            __m512 o = _mm512_fmadd_ps(n, g, b);
            _mm512_store_ps(out_ptr_token + j, o);
        }
        for (; j < d_model; ++j) {
            float normed = (in_ptr_token[j] - mean) * inv_std;
            out_ptr_token[j] = normed * gamma[j] + beta[j];
        }
    }
}</code></pre>
                </div>
                <div class="smaller" style="margin-top: 20px;">
                    <p><span class="highlight">Performance gains:</span> 8-12x speedup over naive implementation</p>
                    <p><span class="highlight">Memory bandwidth:</span> ~80% of theoretical peak</p>
                    <p><span class="highlight">Cache efficiency:</span> 90%+ L1 hit rate</p>
                </div>
                
                <aside class="notes">
                    Here's production-level LayerNorm with AVX-512 optimization. Three passes highlighted: Pass 1 computes mean using _mm512_add_ps for 16-way parallelism. Notice prefetch hints telling CPU what data we need next. Pass 2 computes variance using FMA - fused multiply-add does diff times diff plus accumulator in single instruction. Pass 3 normalizes, scales, and shifts in vectorized operations. Key: we store mean and rstd for fast backward pass. Performance: 8-12x speedup over naive, 80% of memory bandwidth, 90%+ L1 hit rate.
                </aside>
            </section>

            <!-- SLIDE 17: B=1 Optimization Benefits -->
            <section data-section="implementation">
                <h2>B=1 Optimization Advantages</h2>
                <p class="smaller">With batch size 1, LayerNorm becomes dramatically simpler and faster.</p>
                
                <div class="two-col">
                    <div>
                        <h4 style="color: var(--c-green);">Memory Simplifications</h4>
                        <ul class="smaller">
                            <li>Input: [T×C] instead of [B×T×C]</li>
                            <li>Mean: [T] instead of [B×T] (2KB for T=512)</li>
                            <li>Rstd: [T] instead of [B×T] (2KB for T=512)</li>
                            <li>32× less memory bandwidth!</li>
                        </ul>
                        
                        <h4 style="color: var(--c-blue);">Access Pattern Benefits</h4>
                        <ul class="smaller">
                            <li>Pure sequential memory access</li>
                            <li>Perfect cache line utilization</li>
                            <li>Hardware prefetcher friendly</li>
                            <li>No complex batch indexing</li>
                        </ul>
                    </div>
                    <div>
                        <h4 style="color: var(--c-orange);">Performance Impact</h4>
                        <div class="math-insight">
                            <p class="smaller"><strong>Memory Transfer Time:</strong></p>
                            <p class="smaller">B=32: 540MB @ 200GB/s = 2.7ms</p>
                            <p class="smaller">B=1: 16.8MB @ 200GB/s = 0.084ms</p>
                            <p class="smaller"><strong>Compute Time:</strong> ~0.5ms</p>
                            <p class="smaller highlight"><strong>Result:</strong> Now compute-bound!</p>
                        </div>
                        
                        <h4 style="color: var(--c-purple);">Implementation Benefits</h4>
                        <ul class="smaller">
                            <li>6 streams → 4 streams</li>
                            <li>Gamma/Beta read once, broadcast</li>
                            <li>Linear address arithmetic only</li>
                            <li>Entire working set fits in L3</li>
                        </ul>
                    </div>
                </div>
                
                <aside class="notes">
                    With batch size 1, LayerNorm becomes dramatically simpler. Memory simplifications: Input is just T×C instead of B×T×C, mean and rstd are T-dimensional vectors instead of B×T matrices. This gives us 32× less memory bandwidth! Access patterns become purely sequential - hardware prefetcher loves this. Performance impact is huge: memory transfer drops from 2.7ms to 0.084ms while compute stays at 0.5ms. Now we're compute-bound instead of memory-bound! Implementation benefits: fewer streams needed, simpler indexing, entire working set fits in L3 cache.
                </aside>
            </section>

            <!-- SLIDE 18: Complete Performance Analysis -->
            <section data-section="implementation">
                <h2>Complete Performance Analysis</h2>
                <p class="smaller">Understanding the full performance picture from memory bandwidth to compute throughput.</p>
                
                <div class="three-col">
                    <div>
                        <h4 style="color: var(--c-red);">Memory Bottlenecks</h4>
                        <ul class="smaller">
                            <li>DRAM: 200 GB/s theoretical</li>
                            <li>L3: 1 TB/s aggregate</li>
                            <li>L2: 500 GB/s per core</li>
                            <li>L1: 2 TB/s per core</li>
                            <li>Prefetch distance critical</li>
                        </ul>
                    </div>
                    <div>
                        <h4 style="color: var(--c-green);">Compute Capabilities</h4>
                        <ul class="smaller">
                            <li>AVX-512: 16 floats/cycle</li>
                            <li>FMA: 32 FLOPS/cycle</li>
                            <li>Peak: ~2 TFLOPS/s</li>
                            <li>Reality: Limited by memory</li>
                            <li>B=1: Achieves compute bound</li>
                        </ul>
                    </div>
                    <div>
                        <h4 style="color: var(--c-blue);">Optimization Strategy</h4>
                        <ul class="smaller">
                            <li>Store mean/rstd for backprop</li>
                            <li>Manual prefetch streams</li>
                            <li>Cache-aligned memory layout</li>
                            <li>SIMD vectorization</li>
                            <li>B=1 for inference</li>
                        </ul>
                    </div>
                </div>
                
                <div class="math-insight">
                    <h4>🎯 Key Insight</h4>
                    <p class="smaller">LayerNorm optimization demonstrates the critical importance of understanding the full stack - from mathematical foundations to hardware limitations. The 32× memory reduction with B=1 transforms a memory-bound operation into a compute-bound one, enabling full utilization of modern CPU capabilities.</p>
                </div>
                
                <aside class="notes">
                    Let's understand the complete performance picture. Memory hierarchy: DRAM at 200GB/s, L3 at 1TB/s, L2 at 500GB/s per core, L1 at 2TB/s per core. Compute capabilities: AVX-512 processes 16 floats per cycle, FMA gives 32 FLOPS per cycle, theoretical peak around 2 TFLOPS. But reality: usually memory-bound. With B=1, we achieve compute-bound performance! Optimization strategy: store mean/rstd for fast backprop, use manual prefetch, cache-aligned layout, SIMD vectorization. The key insight: understanding full stack from math to hardware enables transformative optimizations.
                </aside>
            </section>

            <!-- SLIDE 19: Conclusion -->
            <section data-section="conclusion">
                <h2>The Lasting Impact</h2>
                <p class="smaller">LayerNorm represents a perfect example of how deep understanding across multiple layers creates transformative technology.</p>
                
                <div class="three-col" style="margin-top: 40px;">
                    <div style="text-align: center; padding: 20px;">
                        <div style="font-size: 50px; color: var(--c-blue);">🧮</div>
                        <h4>Mathematical Insight</h4>
                        <p class="smaller">Understanding internal covariate shift led to the LayerNorm formula</p>
                    </div>
                    <div style="text-align: center; padding: 20px;">
                        <div style="font-size: 50px; color: var(--c-orange);">⚡</div>
                        <h4>Architectural Innovation</h4>
                        <p class="smaller">Moving LayerNorm by 2 lines enabled deep network training</p>
                    </div>
                    <div style="text-align: center; padding: 20px;">
                        <div style="font-size: 50px; color: var(--c-green);">🔧</div>
                        <h4>Implementation Mastery</h4>
                        <p class="smaller">SIMD optimization achieves near-hardware-limit performance</p>
                    </div>
                </div>
                
                <div class="fragment" style="margin-top: 40px; text-align: center;">
                    <p class="highlight larger">This is the kind of full-stack thinking that drives real AI progress.</p>
                </div>
                
                <aside class="notes">
                    LayerNorm perfectly demonstrates how deep understanding across multiple layers creates breakthrough technology. Mathematical insight: understanding internal covariate shift led to the normalization formula. Architectural innovation: moving LayerNorm by 2 lines enabled deep network training and unlocked the modern LLM era. Implementation mastery: SIMD optimization achieves near-hardware-limit performance through careful memory management and vectorization. This is the kind of full-stack thinking - from mathematical foundations to hardware optimization - that drives real AI progress. Thank you for your attention. Questions?
                </aside>
            </section>

        </div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/dist/reveal.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/plugin/math/math.js"></script>
    <script src="../reveal.js/plugin/highlight/highlight.js"></script>
    <script src="../reveal.js/plugin/zoom/zoom.js"></script>
    <script src="../reveal.js/plugin/notes/notes.js"></script>
    <script>
        Reveal.initialize({
            controls: true,
            progress: true,
            center: true,
            hash: true,
            width: 1600,
            height: 900,
            transition: 'slide',
            plugins: [RevealZoom, RevealMath.MathJax2, RevealHighlight, RevealNotes]
        });
    </script>
</body>
</html>