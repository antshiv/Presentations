<!doctype html>
<html lang="en">

<head>
	<meta charset="utf-8">

	<title>Memory planning of transformer models</title>

	<meta name="description" content="A framework for easily creating beautiful presentations using HTML">
	<meta name="author" content="Hakim El Hattab">

	<meta name="apple-mobile-web-app-capable" content="yes">
	<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

	<meta name="viewport" content="width=device-width, initial-scale=1.0">

	<link rel="stylesheet" href="../reveal.js/dist/reset.css">
	<link rel="stylesheet" href="../reveal.js/dist/reveal.css">
	<link rel="stylesheet" href="../reveal.js/dist/theme/black.css" id="theme">

	<!-- Theme used for syntax highlighting of code -->
	<link rel="stylesheet" href="../reveal.js/plugin/highlight/monokai.css">
	<style>
		.container {
			position: relative;
			display: flex;
			flex-direction: column;
			align-items: center;
			justify-content: space-evenly;
			border-radius: 9px;
		}

		#svg-stage {
			width: 60%;
			max-width: 750px;
			overflow: visible;
		}

		.gradient {
			display: inline-block !important;
			color: transparent !important;
			background: -webkit-linear-gradient(16deg, #4285f4 0%, #9b72cb 9%, #d96570 20%, #d96570 24%, #9b72cb 35%, #4285f4 44%, #9b72cb 50%, #d96570 56%, #2a2a2a 75%, #2a2a2a 100%);
			background: linear-gradient(74deg, #4285f4 0%, #9b72cb 9%, #d96570 20%, #d96570 24%, #9b72cb 35%, #4285f4 44%, #9b72cb 50%, #d96570 56%, #2a2a2a 75%, #2a2a2a 100%) !important;
			background: linear-gradient(74deg, #0066ff 0%, #00ccff 25%, #0099ff 50%, #66d9ef 75%, #2a2a2a 100%) !important;
			background: linear-gradient(74deg, #6c5ce7 0%, #a29bfe 25%, #74b9ff 50%, #0984e3 75%, #2a2a2a 100%) !important;
			background: linear-gradient(74deg, #ff7675 0%, #fd79a8 25%, #fdcb6e 50%, #e17055 75%, #2a2a2a 100%) !important;
			background: linear-gradient(74deg, #fdcb6e 0%, #f39c12 25%, #3498db 50%, #2980b9 75%, #2a2a2a 100%) !important;
			background: linear-gradient(74deg, #00b894 0%, #00cec9 25%, #74b9ff 50%, #0984e3 75%, #2a2a2a 100%) !important;
			background: linear-gradient(74deg, #4285f4 0%, #9b72cb 9%, #d9a765 20%, #d96570 24%, #72cb77 35%, #4285f4 44%, #9b72cb 50%, #65d9c0 56%, #2a2a2a 75%, #2a2a2a 100%) !important;
			background-size: 400% 100% !important;
			background-clip: text !important;
			-webkit-background-clip: text !important;
			-webkit-text-fill-color: transparent !important;
		}

		.slide-background {
			background-image: url("../assets/topleft.svg"), url("../assets/bottomright.svg") !important;
			background-position: top left, bottom right !important;
			background-size: 720px auto, 640px auto !important;
			background-repeat: no-repeat !important;
			background-color: transparent !important;
			position: absolute;
			width: 100%;
			height: 100%;
			opacity: 0.5;
		}
	</style>
</head>

<body>

	<div class="reveal">

		<!-- Any section element inside of this container is displayed as a slide -->
		<div class="slides">
			<section>
				<aside class="notes">
					<p>
						"What if I told you that CPUs aren't inherently slow for AI workloads - they're just running
						software that completely ignores how the hardware actually works? Today, I'm going to show you
						how CPUs can actually be faster than GPUs for transformer inference when you properly optimize
						software for the hardware."
						[Pause for impact]
						"And before you say 'but GPUs are just better for AI' - I'm going to prove that we apply
						completely different optimization standards. We write hyper-optimized software that carefully
						exploits every aspect of GPU hardware architecture, but then write software for CPUs that
						ignores cache hierarchy, memory layout, and every optimization principle we know. Then we
						conclude that 'CPUs are slow.'"
						[Transition to next section]
						"Today we're going to challenge this double standard, look at real code examples, and show what
						happens when you apply the same level of hardware-aware optimization to CPU software that we
						routinely apply to GPU software."
					</p>
				</aside>
				<h1>LLM
					<span class="gradient">memory Layout</span>
				</h1>
				<p>
					<small>Created by <a href="http://shivasnotes.com">Anthony Shivakumar</a> and <a
							href="https://antshiv.com">ANTSHIV ROBOTICS</a></small>
				</p>
			</section>
			<section>
				<aside class="notes">
					[Screen: AlexNet paper - zoom in on the highlighted GPU text]
					"Let's start with where this all began. Here's the AlexNet paper from 2012 - the paper that
					basically launched the modern deep learning revolution. Look at this highlighted text: 'very
					efficient GPU implementation.' This single phrase changed everything."
					[Pause to let that sink in]
					"This paper set the tone for the entire industry - everyone assumed GPUs were the only way forward
					for deep learning. But here's the thing - what if that assumption was based on fundamentally unfair
					comparisons?"

					[Screen: Stay on AlexNet paper, pointing to the GPU mention]
					"Here's what actually happened: The AlexNet team wrote highly optimized CUDA code that carefully
					managed GPU memory hierarchy, used efficient convolution libraries, and obsessed over every detail
					of GPU performance. Then they compared it to... basic CPU implementations that ignored every single
					optimization principle we know about computer architecture."

					[Continue on AlexNet paper or transition to code examples]
					"And this pattern has continued for over a decade. When developers write GPU code, they obsess over
					coalesced memory access, shared memory optimization, warp divergence minimization, and memory bank
					conflicts. But the same developers write CPU code that completely ignores cache locality, creates
					memory fragmentation, causes constant TLB misses, and then conclude 'CPUs are just slow.'"

					[Screen: Emphasize the contradiction]
					"This is the fundamental unfairness I want to expose today. We're comparing hyper-optimized GPU
					software versus completely unoptimized CPU software - and then drawing conclusions about hardware
					capabilities. What if we applied the same level of optimization rigor to both platforms?"
				</aside>
				<img src="assets/alex_net.png" alt="AlexNet Memory Layout"
					style="width: 100%; max-width: 800px; border-radius: 10px;">
			</section>
			<section data-auto-animate="">
				<aside class="notes">
					<p>
						(Transition from the AlexNet slide, where you just asked: "What if we applied the same level of
						optimization rigor to both platforms?")
					</p>
					<p>
						So, to answer that exact question, let's start with a well-known and highly-respected baseline.
						Many of you will recognize Andrej Karpathy, a legend in the AI community for his work at OpenAI
						and Tesla, and for his incredible educational projects.
					</p>
					<p><strong>(PAUSE)</strong></p>
					<p>
						This slide shows a snippet from his train_gpt2.c project—a brilliant, from-scratch
						implementation of GPT-2 training in pure C. What you're looking at is the GPT2Config struct,
						which is the architectural blueprint for the entire model.
					</p>
					<p>
						<em>(Gesture towards the code on the screen)</em>
					</p>
					<p>
						This defines everything from the sequence length to the vocabulary size to the number of layers
						and heads. It's good, clean C code that follows best practices.
					</p>
					<p><strong>(PAUSE FOR EMPHASIS)</strong></p>
					<p>
						But as we'll see, 'good software engineering' and 'hardware-optimized' are two very different
						things.
					</p>
					<p>
						(Click to transition to the next slide) Now, let's look at how the actual model parameters and
						activations are structured in this same file. That's where we'll see the problem firsthand.
					</p>
				</aside>
				<h2>Karapathy <span class="gradient">Memory Layout</span></h2>
				<pre data-id="code-animation"><code class="c hljs" data-trim contenteditable style="font-size: 18px;">
				typedef struct {
					int max_seq_len; // max sequence length, e.g. 1024
					int vocab_size; // vocab size, e.g. 50257
					int padded_vocab_size; // padded to e.g. %128==0, 50304
					int num_layers; // number of layers, e.g. 12
					int num_heads; // number of heads in attention, e.g. 12
					int channels; // number of channels, e.g. 768
				} GPT2Config;
				</code></pre>
				<p><a style="font-size:24px;"
						href="https://github.com/karpathy/llm.c/blob/master/train_gpt2.c#L524">Source: train_gpt2.c</a>
				</p>
			</section>
			<section data-auto-animate>
				<aside class="notes">
					<p>
						Now at first glance, this ActivationTensors struct might look like it’s doing 23 separate memory
						allocations – one for each tensor. And it's tempting to assume that this means memory is badly
						fragmented.
					</p>
					<p>
						But let’s not oversimplify. If you dive into Karpathy’s train_gpt2.c, you’ll see something much
						smarter happening.
					</p>
					<p>
						<em>(CUE: Gesture or zoom into struct definitions)</em>
					</p>
					<p>
						Each of these float* pointers is actually referencing an offset inside a single large contiguous
						block of memory — one for all the parameters, and one for all the activations. This is already a
						solid optimization.
					</p>
					<p>
						By using offsets into a contiguous region, Karpathy avoids the worst-case scenario of fragmented
						memory with dozens of separate mallocs. So yes, this code is partially optimized, and that
						deserves recognition.
					</p>
					<p>
						<em>(CUE: Build anticipation for what's still missing)</em>
					</p>
					<p>
						But here's the catch — and this is where we take things to the next level.
					</p>
					<p>
						During the forward pass, the CPU still needs to constantly jump between two distinct blocks: one
						for parameters, and another for activations.
					</p>
					<p>
						That jump – between reading from one big region and writing to another – creates discontinuity
						in memory access.
					</p>
					<p>
						Even though each block is individually contiguous, they're not laid out in the order of
						execution. The result? Constant cache evictions, TLB misses, and disrupted prefetching.
					</p>
					<p>
						<em>(CUE: Introduce your solution)</em>
					</p>
					<p>
						In our cache-optimal approach, we solve this by fusing all these regions into one unified memory
						block for the entire forward pass.
					</p>
					<p>
						We no longer group memory by 'what it is' — parameters vs activations — we group by <strong>how
							it's used</strong>, in the order the CPU needs it.
					</p>
					<p>
						That means when the CPU finishes one operation, the next input is already in the next cache
						line. Not in another megabyte-sized block somewhere else.
					</p>
					<p>
						<strong>(CUE: Pause for emphasis before transition)</strong>
					</p>
					<p>
						This shift — from separation to true sequence — is the core reason why we get 95%+ cache hit
						rates, minimal TLB overhead, and up to 100x faster inference on CPUs.
					</p>
					<p>
						Let’s go to the next slide and look at exactly how this unified layout works.
					</p>
				</aside>
				<h2>Karapathy <span class="gradient">Memory Layout contd ..</span></h2>
				<pre data-id="code-animation"><code class="c hljs" data-trim contenteditable style="font-size: 18px;">
				// the parameters of the model
				#define NUM_PARAMETER_TENSORS 16
				typedef struct {
					float* wte; // (V, C)
					float* wpe; // (maxT, C)
					float* ln1w; // (L, C)
					float* ln1b; // (L, C)
					float* qkvw; // (L, 3*C, C)
					float* qkvb; // (L, 3*C)
					float* attprojw; // (L, C, C)
					float* attprojb; // (L, C)
					float* ln2w; // (L, C)
					float* ln2b; // (L, C)
					float* fcw; // (L, 4*C, C)
					float* fcb; // (L, 4*C)
					float* fcprojw; // (L, C, 4*C)
					float* fcprojb; // (L, C)
					float* lnfw; // (C)
					float* lnfb; // (C)
				} ParameterTensors;	

				#define NUM_ACTIVATION_TENSORS 23
				typedef struct {
					float* encoded; // (B, T, C)
					float* ln1; // (L, B, T, C)
					float* ln1_mean; // (L, B, T)
					float* ln1_rstd; // (L, B, T)
					float* qkv; // (L, B, T, 3*C)
					float* atty; // (L, B, T, C)
					float* preatt; // (L, B, NH, T, T)
					float* att; // (L, B, NH, T, T)
					float* attproj; // (L, B, T, C)
					float* residual2; // (L, B, T, C)
					float* ln2; // (L, B, T, C)
					float* ln2_mean; // (L, B, T)
					float* ln2_rstd; // (L, B, T)
					float* fch; // (L, B, T, 4*C)
					float* fch_gelu; // (L, B, T, 4*C)
					float* fcproj; // (L, B, T, C)
					float* residual3; // (L, B, T, C)
					float* lnf; // (B, T, C)
					float* lnf_mean; // (B, T)
					float* lnf_rstd; // (B, T)
					float* logits; // (B, T, V)
					float* probs; // (B, T, V)
					float* losses; // (B, T)
				} ActivationTensors;

				
				</code></pre>
				<p><a style="font-size:24px;"
						href="https://github.com/karpathy/llm.c/blob/master/train_gpt2.c#L524">Source: train_gpt2.c</a>
				</p>
			</section>
			<section data-auto-animate>
				<aside class="notes">
					<p>
						Now here's my proposed solution—and this is where we take CPU optimization to the same level we
						apply to GPU programming. Instead of thinking about memory in terms of data structures, we think
						about it in terms of execution flow.
					</p>
					<p>
						<em>(CUE: Point to the key concept in the code)</em>
					</p>
					<p>
						The core insight is right here in the comments: 'Offsets into contiguous memory (execution order
						layout)'. We're not just making data contiguous—we're arranging it in the exact order the CPU
						will access it during computation.
					</p>
					<p>
						<em>(CUE: Point to the TransformerModel structure)</em>
					</p>
					<p>
						Look at this TransformerModel structure. Everything—and I mean EVERYTHING—goes into a single
						contiguous memory block. Token embeddings, position embeddings, all layer weights, all
						intermediate activations, all temporary results. One malloc call, one contiguous chunk of
						memory.
					</p>
					<p>
						<em>(CUE: Emphasize the execution order concept)</em>
					</p>
					<p>
						But here's the revolutionary part: the memory layout follows execution order. When we finish
						computing token embeddings, the position embeddings are right there in the next cache line. When
						we finish the embedding combination, Layer 0's weights are immediately adjacent. When Layer 0
						outputs its result, Layer 1's input space is right there waiting.
					</p>
					<p>
						<em>(CUE: Point to TrulyOptimalLayer)</em>
					</p>
					<p>
						Each layer becomes a metadata structure with offsets into the unified memory block. No more
						jumping between scattered allocations. No more cache misses between related computations. The
						CPU's cache prefetcher can actually predict what we're going to need next because our memory
						access pattern is perfectly sequential.
					</p>
					<p>
						<em>(CUE: Connect back to GPU comparison)</em>
					</p>
					<p>
						This is exactly the principle behind GPU coalesced memory access—except now we're applying it to
						CPU cache hierarchy. Same optimization philosophy, different hardware target.
					</p>
				</aside>
				<h2><span class="gradient">Proposed Structure:</span></h2>
				<pre data-id="code-animation"><code class="c hljs" data-trim contenteditable style="font-size: 18px;">
// ============================================================================
// CACHE-OPTIMAL TRANSFORMER IMPLEMENTATION
// Breaking the GPU Myth with Better Memory Layout
// ============================================================================

typedef struct {
    float* token_embedding;     // [vocab_size, embed_dim]
    float* position_embedding;  // [context_window, embed_dim]
} EmbeddingBlock;

typedef struct {
    // Offsets into contiguous memory (execution order layout)
    size_t token_emb_offset;           // Token embeddings start
    size_t pos_emb_offset;             // Position embeddings start
    size_t embedded_input_offset;      // Combined embedding result
    
    size_t ln1_weight_offset;
    size_t ln1_bias_offset;
    size_t layer_input_offset;
    size_t ln1_output_offset;
    
    size_t qkv_weight_offset;
    size_t qkv_bias_offset;
    size_t qkv_output_offset;
    
    size_t proj_weight_offset;
    size_t proj_bias_offset;
    size_t attention_output_offset;
    size_t residual1_output_offset;
    
    size_t ln2_weight_offset;
    size_t ln2_bias_offset;
    size_t ln2_output_offset;
    
    size_t fc1_weight_offset;
    size_t fc1_bias_offset;
    size_t fc2_weight_offset;
    size_t fc2_bias_offset;
    size_t mlp_output_offset;
    size_t residual2_output_offset;
} TrulyOptimalLayer;

typedef struct {
    // Model configuration
    int num_layers;
    int vocab_size;
    int embed_dim;
    int context_window;
    int num_heads;
    int head_dim;
    
    // Single contiguous memory block per batch (EVERYTHING!)
    float* memory_base;                // [token_emb + pos_emb + embedded_input + layers]
    size_t memory_size;                // Total memory allocated per batch
    size_t layer_stride;               // Memory per layer (in floats)
    
    // Offsets within contiguous memory for perfect locality
    size_t token_emb_offset;           // Token embeddings start
    size_t pos_emb_offset;             // Position embeddings start
    size_t embedded_input_offset;      // Combined embedding result
    size_t layers_start_offset;        // Where layer processing begins
    
    // Per-batch contiguous processing memory
    TrulyOptimalLayer* layers;         // Array of layer metadata
    
    // Final output layer
    float* final_ln_weight;
    float* final_ln_bias;
} TransformerModel;



				</code></pre>
			</section>
			<section data-auto-animate="">
				<aside class="notes">
					<p>
						Now let's see this execution-order principle in action. This
						<code>calculate_batch_memory_size</code> function is where we determine exactly how much memory
						we need for our cache-optimal layout.
					</p>
					<p>
						<em>(CUE: Point to the variable declarations)</em>
					</p>
					<p>
						We start with our basic dimensions - V for vocab size, C for embedding dimension, T for context
						length. But notice how we're thinking about memory allocation differently now.
					</p>
					<p>
						<em>(CUE: Point to the embeddings section)</em>
					</p>
					<p>
						First, we calculate space for token embeddings and position embeddings—but the key insight is in
						that comment: 'now part of contiguous block!'. These aren't separate allocations anymore;
						they're part of our unified memory layout.
					</p>
					<p>
						<em>(CUE: Point to embedded_input)</em>
					</p>
					<p>
						Then we allocate space for the embedded input—the result of combining token and position
						embeddings. This space is placed right after the embeddings in memory, so when we finish the
						embedding computation, the result goes exactly where the next operation expects to find it.
					</p>
					<p>
						<em>(CUE: Point to the single_layer calculation)</em>
					</p>
					<p>
						Here's where it gets interesting. For each layer, we calculate memory for every single
						intermediate result in execution order: LayerNorm weights and bias, then the space for LayerNorm
						input and output, then QKV weights and bias, then QKV output space, then attention projection,
						and so on.
					</p>
					<p>
						<em>(CUE: Emphasize the key insight)</em>
					</p>
					<p>
						The crucial point is that we're not just calculating how much memory we need—we're calculating
						it in the exact order the CPU will access it. When LayerNorm finishes, the QKV computation's
						input is right there in the next cache line.
					</p>
					<p>
						<em>(CUE: Point to the final return)</em>
					</p>
					<p>
						And finally, we return one number—the total size for everything in perfect sequential order. One
						allocation, perfect cache behavior.
					</p>
				</aside>
				<h2><span class="gradient">Memory Allocation</span></h2>
				<pre data-id="code-animation"><code class="c hljs" data-trim data-line-numbers="|1, 4,13-14|17|22-24" contenteditable style="font-size: 18px;">
// ============================================================================
// CACHE-OPTIMAL MEMORY ALLOCATION
// ============================================================================

size_t calculate_batch_memory_size(int vocab_size, int embed_dim, int context_len, int num_layers) {
    int V = vocab_size;
    int C = embed_dim;
    int T = context_len;
    
    // Token and position embeddings (now part of contiguous block!)
    size_t token_embeddings = V * C;
    size_t pos_embeddings = T * C;
    
    // Embedded input (result of token + position embedding)
    size_t embedded_input = T * C;
    
    // All layers 
    size_t single_layer = 
        // LayerNorm 1
        C + C +                          // weights + bias
        T * C + T * C +                  // input + output
        
        // QKV Attention  
        3 * C * C + 3 * C +              // weights + bias
        T * 3 * C +                      // qkv output
        
        // Attention Projection
        C * C + C +                      // weights + bias  
        T * C + T * C +                  // attention output + residual1
        
        // LayerNorm 2
        C + C +                          // weights + bias
        T * C +                          // output
        
        // MLP
        4 * C * C + 4 * C +              // fc1 weights + bias
        C * 4 * C + C +                  // fc2 weights + bias
        T * C + T * C;                   // mlp output + residual2
    
    size_t all_layers = single_layer * num_layers;
    
    // Everything in one contiguous block for perfect cache behavior!
    return token_embeddings + pos_embeddings + embedded_input + all_layers;
}
				</code></pre>
			</section>
			<Section data-auto-animate="">
				<aside class="notes">
					<p>
						Now here's where the magic happens—the actual allocation function that implements our
						cache-optimal design. This is where we go from theory to practice.
					</p>
					<p>
						<em>(CUE: Point to the function signature and setup)</em>
					</p>
					<p>
						We start by setting up our model configuration, then we get to the heart of it—that comment says
						it all: 'SINGLE CONTIGUOUS ALLOCATION FOR EVERYTHING!'
					</p>
					<p>
						<em>(CUE: Point to the allocation section)</em>
					</p>
					<p>
						Look at this: <code>aligned_alloc(64, model->memory_size)</code>. One function call. One
						allocation. Everything the transformer needs—all weights, all activations, all intermediate
						results—lives in this single contiguous block. And notice we're using 64-byte alignment, which
						is exactly one cache line on modern CPUs.
					</p>
					<p>
						<em>(CUE: Point to the offset calculation section)</em>
					</p>
					<p>
						But here's the crucial part—how we set up the offsets. We start at offset 0 with token
						embeddings, then immediately after that come position embeddings, then immediately after that
						comes the space for embedded input. No gaps, no fragmentation, perfect sequential layout.
					</p>
					<p>
						<em>(CUE: Point to the layer setup)</em>
					</p>
					<p>
						Each layer gets its own metadata structure that points into specific offsets within our unified
						block. When we call <code>setup_layer_pointers</code>, we're creating a roadmap that ensures
						every computation knows exactly where to find its inputs and where to place its outputs.
					</p>
					<p>
						<em>(CUE: Point to the printf statements)</em>
					</p>
					<p>
						And look at this output—this isn't just debugging, it's proof of concept. When you run this
						code, you can see exactly how your memory is laid out: Token embeddings flow into position
						embeddings, which flow into embedded input, which flows into Layer 0, and so on.
					</p>
					<p>
						<em>(CUE: Emphasize the key insight)</em>
					</p>
					<p>
						The result? Zero memory fragmentation, perfect sequential access patterns, and that critical
						transition from embeddings to Layer 0 happens with zero cache misses. This is the difference
						between fighting the hardware and working with it.
					</p>
				</aside>
				<h2><span class="gradient">Memory Allocation contd ..</span></h2>
				<pre data-id="code-animation"><code class="c hljs" data-trim data-line-numbers="|1, 4,13-14|17|22-24" contenteditable style="font-size: 18px;">

void allocate_transformer_with_embeddings(TransformerModel* model, int num_layers,
                                          int vocab_size, int embed_dim, int context_len) {
    model->num_layers = num_layers;
    model->vocab_size = vocab_size;
    model->embed_dim = embed_dim;
    model->context_window = context_len;
    
    // ========================================================================
    // SINGLE CONTIGUOUS ALLOCATION FOR EVERYTHING!
    // ========================================================================
    
    size_t batch_memory_floats = calculate_batch_memory_size(vocab_size, embed_dim, context_len, num_layers);
    model->memory_size = batch_memory_floats * sizeof(float);
    
    // ONE allocation for everything - no fragmentation!
    model->memory_base = aligned_alloc(64, model->memory_size);
    if (!model->memory_base) {
        fprintf(stderr, "Failed to allocate %zu MB for transformer\n", 
                model->memory_size / (1024 * 1024));
        exit(1);
    }
    
    // ========================================================================
    // SET UP OFFSETS FOR PERFECT SEQUENTIAL ACCESS
    // ========================================================================
    
    size_t offset = 0;
    
    // Token embeddings come first
    model->token_emb_offset = offset;
    offset += vocab_size * embed_dim;
    
    // Position embeddings immediately after
    model->pos_emb_offset = offset;
    offset += context_len * embed_dim;
    
    // Embedded input immediately after position embeddings
    model->embedded_input_offset = offset;
    size_t embedded_input_size = context_len * embed_dim;
    offset += embedded_input_size;
    
    // Layer processing starts immediately after embedded input
    size_t layer_size_floats = calculate_layer_memory_size(embed_dim, context_len);
    model->layers_start_offset = offset;
    model->layer_stride = layer_size_floats;
    
    // ========================================================================
    // SET UP LAYER METADATA POINTING INTO CONTIGUOUS BLOCK
    // ========================================================================
    
    model->layers = malloc(sizeof(TrulyOptimalLayer) * num_layers);
    
    for (int i = 0; i < num_layers; i++) {
        size_t layer_offset = model->layers_start_offset + i * layer_size_floats;
        setup_layer_pointers(&model->layers[i], layer_offset, embed_dim, context_len);
    }
    
    // ========================================================================
    // DISPLAY CACHE-OPTIMAL MEMORY LAYOUT
    // ========================================================================
    
    printf("=== CACHE-OPTIMAL MEMORY LAYOUT ===\n");
    printf("Total per batch: %zu MB\n", model->memory_size / (1024 * 1024));
    printf("\nMemory layout breakdown:\n");
    printf("  • Token Embeddings     : %zu MB\n", (vocab_size * embed_dim * sizeof(float)) / (1024 * 1024));
    printf("  • Position Embeddings  : %zu KB\n", (context_len * embed_dim * sizeof(float)) / 1024);
    printf("  • Embedded Input       : %zu KB\n", (embedded_input_size * sizeof(float)) / 1024);
    printf("  • All Layers           : %zu MB\n", (layer_size_floats * num_layers * sizeof(float)) / (1024 * 1024));
    
    printf("\n🎯 PERFECT SEQUENTIAL LAYOUT:\n");
    printf("   [TokenEmb] → [PosEmb] → [EmbeddedInput] → [Layer0] → [Layer1] → ... → [Layer%d]\n", num_layers - 1);
    printf("   ↑                                                                               ↑\n");
    printf("   0x%p                                              Single allocation\n", model->memory_base);
    
    printf("\n✅ Cache-optimal design achieved:\n");
    printf("   • ZERO memory fragmentation\n");
    printf("   • Perfect sequential access patterns\n");
    printf("   • Embedding → Layer 0 transition: ZERO cache misses\n");
    printf("   • Expected speedup: 30-50%% over fragmented approaches\n");
}
</code></pre>
			</Section>
			<section style="transform: scale(1.4);">
				<aside class="notes">
					<p><strong>(CUE):</strong> <em>Transition from the previous slide, where you just detailed the
							unified allocation function.</em></p>

					<p><strong>YOU:</strong> "So, let's put it all together. This slide is a side-by-side comparison
						that visually summarizes the entire philosophy we've been discussing: the 'Performance Killer'
						versus the 'HPC Optimized' approach."</p>

					<p><strong>(CUE):</strong> <em>Gesture to the left, "Performance Killer" column.</em></p>

					<p><strong>YOU:</strong> "On the left, we have the traditional 'Clean' separated structure. This is
						the standard approach, with separate data structures and memory blocks for model weights and
						activations. Now, to be fair, any software engineering textbook would actually approve of this
						design - it's modular, readable, and follows good object-oriented principles."</p>

					<p><strong>(CUE):</strong> <em>Point to the memory layout diagram showing the gap</em></p>

					<p><strong>YOU:</strong> "But look what happens in memory: Model Weights are in one location,
						there's a gap, then Activations are somewhere completely different. As we've discussed, this
						forces the CPU to constantly jump between these regions during inference - and that gap
						represents cache misses."</p>

					<p><strong>(CUE):</strong> <em>Point to the performance impact statistics</em></p>

					<p><strong>YOU:</strong> "The numbers don't lie: 60-80% cache miss rate, which means most of your
						data is coming from main memory at 200+ cycle penalties instead of L1 cache at 1-3 cycles. Your
						inference time becomes 5-10x slower than optimal."</p>

					<p><strong>(CUE):</strong> <em>Gesture to the right side, "HPC Optimized" column</em></p>

					<p><strong>YOU:</strong> "Now look at the unified approach on the right. We've abandoned 'clean'
						separation for hardware reality. Everything lives in one contiguous block, accessed by offsets
						into unified memory - exactly like what GPU programmers do with coalesced memory access."</p>

					<p><strong>(CUE):</strong> <em>Point to the green sequential layout</em></p>

					<p><strong>YOU:</strong> "The memory layout tells the whole story: Weights flow seamlessly into
						Activations, which flow into Layer0, Layer1, and so on. Perfect sequential access that works
						with the CPU cache system instead of against it."</p>

					<p><strong>(CUE):</strong> <em>Point to the performance benefits</em></p>

					<p><strong>YOU:</strong> "And the result? 95%+ cache hit rate and 6-10x faster inference. This is
						the exact same hardware-versus-abstraction tradeoff that GPU programmers make every day - except
						now we're applying it to CPU optimization."</p>
				</aside>
				<svg id="a5430479-7bef-4db9-a0b9-c8773248ba2c" data-name="Layer 1" xmlns="http://www.w3.org/2000/svg"
					xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 1400 596.66">
					<defs>
						<pattern id="b5e40e4b-ccce-4730-b3a6-13421bd6368f" data-name="cacheHitPattern" width="10"
							height="10" patternTransform="matrix(1, 0, 0, -1, 9, 1089)" patternUnits="userSpaceOnUse"
							viewBox="0 0 10 10">
							<rect width="10" height="10" style="fill:none" />
							<rect width="10" height="10" style="fill:#2ed573" />
							<rect y="8" width="10" height="2" style="fill:#26de81" />
						</pattern>
					</defs><text transform="translate(340.31 40)"
						style="isolation:isolate;font-size:28px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">The
						Hidden Performance Killer: Separated<tspan x="561.71" y="0"
							style="letter-spacing:-0.037109375em"> </tspan>
						<tspan x="568.45" y="0">Activations</tspan>
					</text><text transform="translate(520.03 65)"
						style="isolation:isolate;font-size:16px;fill:#f39c12;font-family:ArialMT, Arial">Why &quot;Clean
						Code&quot; Can Destroy HPC Performance</text><text transform="translate(116.56 110)"
						style="isolation:isolate;font-size:20px;fill:#e74c3c;font-family:Arial-BoldMT, Arial;font-weight:700">&quot;Clean&quot;
						Separated Structure (Performance Killer)</text>
					<path
						d="M58,130H642a8.25,8.25,0,0,1,8,8.49V333.87a8.25,8.25,0,0,1-8,8.49H58a8.25,8.25,0,0,1-8-8.49V138.49A8.25,8.25,0,0,1,58,130Z"
						style="fill:#2c2c54;stroke:#e74c3c;stroke-width:2px" /><text transform="translate(60 155)"
						style="isolation:isolate;font-size:14px;fill:#f39c12;font-family:Arial-BoldMT, Arial;font-weight:700">typedef
						struct {</text><text transform="translate(70 175)"
						style="isolation:isolate;font-size:12px;fill:#3498db;font-family:ArialMT, Arial">float* weights;
						// Model parameters</text><text transform="translate(70 190)"
						style="isolation:isolate;font-size:12px;fill:#3498db;font-family:ArialMT, Arial">float*
						biases;</text><text transform="translate(70 205)"
						style="isolation:isolate;font-size:12px;fill:#3498db;font-family:ArialMT, Arial">float*
						layer_norms;</text><text transform="translate(60 225)"
						style="isolation:isolate;font-size:14px;fill:#f39c12;font-family:Arial-BoldMT, Arial;font-weight:700">}
						ModelParams;</text><text transform="translate(60 250)"
						style="isolation:isolate;font-size:14px;fill:#f39c12;font-family:Arial-BoldMT, Arial;font-weight:700">typedef
						struct {</text><text transform="translate(70 270)"
						style="isolation:isolate;font-size:12px;fill:#e74c3c;font-family:ArialMT, Arial">float*
						input_activations; // Separate allocation!</text><text transform="translate(70 285)"
						style="isolation:isolate;font-size:12px;fill:#e74c3c;font-family:ArialMT, Arial">float*
						hidden_states;</text><text transform="translate(70 300)"
						style="isolation:isolate;font-size:12px;fill:#e74c3c;font-family:ArialMT, Arial">float*
						attention_scores;</text><text transform="translate(70 315)"
						style="isolation:isolate;font-size:12px;fill:#e74c3c;font-family:ArialMT, Arial">float*
						output_activations;</text><text transform="translate(60 332)"
						style="isolation:isolate;font-size:14px;fill:#f39c12;font-family:Arial-BoldMT, Arial;font-weight:700">}
						<tspan x="5.45" y="0" style="letter-spacing:-0.037109375em"> </tspan>
						<tspan x="8.82" y="0">ActivationBuffers;</tspan>
					</text><text transform="translate(266.88 367)"
						style="isolation:isolate;font-size:16px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">Memory
						Layout</text>
					<rect x="50" y="380" width="200" height="40" rx="4" style="fill:#3498db" /><text
						transform="translate(107.78 405)"
						style="isolation:isolate;font-size:12px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">Model
						<tspan x="38" y="0" style="letter-spacing:-0.01806640625em">W</tspan>
						<tspan x="49.11" y="0">eights</tspan>
					</text>
					<rect x="270" y="380" width="100" height="40" rx="4"
						style="fill:#34495e;stroke:#7f8c8d;stroke-width:2px;stroke-dasharray:5,5" /><text
						transform="translate(291.1 405)"
						style="isolation:isolate;font-size:10px;fill:#7f8c8d;font-family:ArialMT, Arial">Memory
						Gap</text>
					<rect x="390" y="380" width="200" height="40" rx="4" style="fill:#e74c3c" /><text
						transform="translate(457.66 405)"
						style="isolation:isolate;font-size:12px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">Activations</text>
					<path d="M250,400q70,50,140,0"
						style="fill:none;stroke:#ff4757;stroke-width:4px;stroke-dasharray:8,4" /><text
						transform="translate(273.72 470)"
						style="isolation:isolate;font-size:14px;fill:#ff4757;font-family:Arial-BoldMT, Arial;font-weight:700">CACHE
						MISS!</text>
					<path
						d="M58,490H642c4.42,0,8,3.95,8,8.83v70.62c0,4.87-3.58,8.83-8,8.83H58c-4.42,0-8-4-8-8.83V498.83C50,494,53.58,490,58,490Z"
						style="fill:#e74c3c;stroke:#c0392b;stroke-width:2px" /><text transform="translate(60 515)"
						style="isolation:isolate;font-size:16px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">Performance
						Impact:</text><text transform="translate(70 535)"
						style="isolation:isolate;font-size:14px;fill:#fff;font-family:ArialMT, Arial">❌ Cache miss rate:
						60-80% (constant memory jumping)</text><text transform="translate(70 550)"
						style="isolation:isolate;font-size:14px;fill:#fff;font-family:ArialMT, Arial">❌ Memory
						bandwidth: <tspan x="140.44" y="0" style="letter-spacing:-0.037109375em">W</tspan>
						<tspan x="153.14" y="0">asted on cache line fills</tspan>
					</text><text transform="translate(70 565)"
						style="isolation:isolate;font-size:14px;fill:#fff;font-family:ArialMT, Arial">❌ Inference time:
						5-10x slower than optimal</text><text transform="translate(887.22 110)"
						style="isolation:isolate;font-size:20px;fill:#2ed573;font-family:Arial-BoldMT, Arial;font-weight:700">Unified
						Structure (HPC Optimized)</text>
					<rect x="750" y="130" width="600" height="200" rx="8"
						style="fill:#2c2c54;stroke:#2ed573;stroke-width:2px" /><text transform="translate(760 155)"
						style="isolation:isolate;font-size:14px;fill:#f39c12;font-family:Arial-BoldMT, Arial;font-weight:700">typedef
						struct {</text><text transform="translate(770 175)"
						style="isolation:isolate;font-size:12px;fill:#2ed573;font-family:ArialMT, Arial">float*
						unified_memory; // ONE allocation for<tspan x="230.1" y="0"
							style="letter-spacing:-0.05517578125em"> </tspan>
						<tspan x="232.78" y="0">AL</tspan>
						<tspan x="247.45" y="0" style="letter-spacing:-0.037109375em">L</tspan>
						<tspan x="253.68" y="0" xml:space="preserve"> data</tspan>
					</text><text transform="translate(770 190)"
						style="isolation:isolate;font-size:12px;fill:#26de81;font-family:ArialMT, Arial">size_t
						weights_o<tspan x="88.72" y="0" style="letter-spacing:-0.01806640625em">f</tspan>
						<tspan x="91.83" y="0">fset; // </tspan>
						<tspan x="127.85" y="0" style="letter-spacing:-0.01806640625em">W</tspan>
						<tspan x="138.96" y="0">eights start here</tspan>
					</text><text transform="translate(770 205)"
						style="isolation:isolate;font-size:12px;fill:#26de81;font-family:ArialMT, Arial">size_t
						activations_o<tspan x="104.72" y="0" style="letter-spacing:-0.01806640625em">f</tspan>
						<tspan x="107.84" y="0">fset; //</tspan>
						<tspan x="140.52" y="0" style="letter-spacing:-0.05517578125em"> </tspan>
						<tspan x="143.19" y="0">Activations right after</tspan>
					</text><text transform="translate(770 220)"
						style="isolation:isolate;font-size:12px;fill:#26de81;font-family:ArialMT, Arial">size_t layer0_o
						<tspan x="80.71" y="0" style="letter-spacing:-0.01806640625em">f</tspan>
						<tspan x="83.83" y="0">fset; // Layer 0 processing space</tspan>
					</text><text transform="translate(770 235)"
						style="isolation:isolate;font-size:12px;fill:#26de81;font-family:ArialMT, Arial">size_t layer1_o
						<tspan x="80.71" y="0" style="letter-spacing:-0.01806640625em">f</tspan>
						<tspan x="83.83" y="0">fset; // Layer 1 processing space</tspan>
					</text><text transform="translate(770 250)"
						style="isolation:isolate;font-size:12px;fill:#26de81;font-family:ArialMT, Arial">size_t output_o
						<tspan x="81.39" y="0" style="letter-spacing:-0.01806640625em">f</tspan>
						<tspan x="84.51" y="0">fset; // Final output space</tspan>
					</text><text transform="translate(770 265)"
						style="isolation:isolate;font-size:12px;fill:#3498db;font-family:ArialMT, Arial">// Everything
						flows sequentially!</text><text transform="translate(760 280)"
						style="isolation:isolate;font-size:14px;fill:#f39c12;font-family:Arial-BoldMT, Arial;font-weight:700">}
						Optimized<tspan x="77.01" y="0" style="letter-spacing:-0.05517578125em">T</tspan>
						<tspan x="84.79" y="0">ransformer;</tspan>
					</text><text transform="translate(760 300)"
						style="isolation:isolate;font-size:12px;fill:#95a5a6;font-family:ArialMT, Arial">// No separate
						activation struct needed!</text><text transform="translate(760 315)"
						style="isolation:isolate;font-size:12px;fill:#95a5a6;font-family:ArialMT, Arial">// Everything
						accessed by o<tspan x="146.74" y="0" style="letter-spacing:-0.01806640625em">f</tspan>
						<tspan x="149.86" y="0">fset into unified_memory</tspan>
					</text><text transform="translate(990.88 360)"
						style="isolation:isolate;font-size:16px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">Memory
						Layout</text>
					<rect x="750" y="380" width="600" height="40" rx="4"
						style="stroke:#2ed573;stroke-width:3px;fill:url(#b5e40e4b-ccce-4730-b3a6-13421bd6368f)" /><text
						transform="translate(846 391)"
						style="isolation:isolate;font-size:9px;fill:#fff;font-family:ArialMT, Arial">
						<tspan style="letter-spacing:-0.01806640625em">W</tspan>
						<tspan x="8.33" y="0">eights</tspan>
					</text><text transform="translate(946 391)"
						style="isolation:isolate;font-size:9px;fill:#fff;font-family:ArialMT, Arial">Act0</text><text
						transform="translate(996 391)"
						style="isolation:isolate;font-size:9px;fill:#fff;font-family:ArialMT, Arial">Layer0</text><text
						transform="translate(1046 391)"
						style="isolation:isolate;font-size:9px;fill:#fff;font-family:ArialMT, Arial">Layer1</text><text
						transform="translate(1096 391)"
						style="isolation:isolate;font-size:9px;fill:#fff;font-family:ArialMT, Arial">LayerN</text><text
						transform="translate(1146 391)"
						style="isolation:isolate;font-size:9px;fill:#fff;font-family:ArialMT, Arial">Output</text><text
						transform="translate(1196 391)"
						style="isolation:isolate;font-size:9px;fill:#fff;font-family:ArialMT, Arial">
						<tspan style="letter-spacing:-0.11083984375em">T</tspan>
						<tspan x="4.5" y="0">emp</tspan>
					</text><text transform="translate(1246 391)"
						style="isolation:isolate;font-size:9px;fill:#fff;font-family:ArialMT, Arial">Results</text><text
						transform="translate(975.19 410)"
						style="isolation:isolate;font-size:12px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">Perfect
						Sequential<tspan x="104.7" y="0" style="letter-spacing:-0.037109375em"> </tspan>
						<tspan x="107.59" y="0">Access</tspan>
					</text><text transform="translate(965.6 460)"
						style="isolation:isolate;font-size:14px;fill:#2ed573;font-family:Arial-BoldMT, Arial;font-weight:700">CACHE
						HIT EVE<tspan x="108.12" y="0" style="letter-spacing:-0.037109375em">R</tspan>
						<tspan x="117.71" y="0" style="letter-spacing:-0.01806640625em">Y</tspan>
						<tspan x="126.8" y="0" xml:space="preserve"> TIME!</tspan>
					</text>
					<path
						d="M758,490h584c4.42,0,8,3.95,8,8.83v70.62c0,4.87-3.58,8.83-8,8.83H758c-4.42,0-8-4-8-8.83V498.83C750,494,753.58,490,758,490Z"
						style="fill:#27ae60;stroke:#229954;stroke-width:2px" /><text transform="translate(760 515)"
						style="isolation:isolate;font-size:16px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">Performance
						Benefits:</text><text transform="translate(770 535)"
						style="isolation:isolate;font-size:14px;fill:#fff;font-family:ArialMT, Arial">✅ Cache hit rate:
						95%+ (sequential access pattern)</text><text transform="translate(770 550)"
						style="isolation:isolate;font-size:14px;fill:#fff;font-family:ArialMT, Arial">✅ Memory
						bandwidth: Perfectly utilized</text><text transform="translate(770 565)"
						style="isolation:isolate;font-size:14px;fill:#fff;font-family:ArialMT, Arial">✅ Inference time:
						6-10x faster than separated</text>
					<line x1="750" y1="440" x2="1331.89" y2="440"
						style="fill:none;stroke:#2ed573;stroke-miterlimit:10;stroke-width:3px" />
					<path
						d="M1350,440c-8.52,3.16-19.09,8.55-25.64,14.27l5.16-14.27-5.16-14.26C1330.91,431.45,1341.48,436.84,1350,440Z"
						style="fill:#2ed573" />
				</svg>
			</section>
			<section style="transform: scale(1.3);">
				<aside class="notes">
					<p><strong>(CUE):</strong> <em>Transition from the previous summary slide.</em></p>

					<p><strong>YOU:</strong> "So we've seen the high-level difference between the two approaches. This
						slide dives into the low-level mechanics of *why* separation kills performance. It all comes
						down to the critical trade-off between how we organize code versus how the hardware actually
						performs."</p>

					<p><strong>(CUE):</strong> <em>Point to the orange header with the key insight</em></p>

					<p><strong>YOU:</strong> "And here's the fundamental tension: Separated activations look clean,
						follow OOP principles, and are easy to understand. Unified memory violates 'clean code'
						principles but delivers HPC-level performance. In HPC inference, performance absolutely trumps
						code aesthetics."</p>

					<p><strong>(CUE):</strong> <em>Point to the left side - The Cache Miss Cascade</em></p>

					<p><strong>YOU:</strong> "Here's the step-by-step breakdown of what happens with that 'clean'
						separated approach: CPU loads a weight, fills a 64-byte cache line with weight data, then needs
						activation data from a completely different location - cache miss! Evict the weights, load
						activations. Need the next weight? Cache miss again! Constant thrashing between memory regions."
					</p>

					<p><strong>(CUE):</strong> <em>Point to the result section on the left</em></p>

					<p><strong>YOU:</strong> "The memory controller can't predict this chaotic access pattern. We're
						fighting against every optimization the hardware is trying to do for us."</p>

					<p><strong>(CUE):</strong> <em>Point to the right side - The Sequential Solution</em></p>

					<p><strong>YOU:</strong> "Now contrast that with sequential access: CPU loads weight, cache line
						includes weight plus prefetches next activation, next access hits cache, prefetcher predicts
						pattern and loads ahead. Every access works with the hardware."</p>

					<p><strong>(CUE):</strong> <em>Point to the green result section</em></p>

					<p><strong>YOU:</strong> "Perfect cache utilization and optimal memory controller behavior. This is
						the exact same trade-off GPU programmers make every day - they choose hardware optimization over
						clean abstractions. It's time we applied the same thinking to CPU code."</p>
				</aside>
				<svg id="ea249576-18d5-4863-8b95-96a43074f05e" data-name="Layer 1" xmlns="http://www.w3.org/2000/svg"
					viewBox="0 0 1400 462.64">
					<rect x="17.63" y="13" width="1370.35" height="137.03" rx="13.7"
						style="fill:#34495e;stroke:#f39c12;stroke-width:3px" /><text transform="translate(391.24 47.26)"
						style="isolation:isolate;font-size:22.839115142822266px;fill:#f39c12;font-family:Arial-BoldMT, Arial;font-weight:700">The
						Critical <tspan x="130.72" y="0" style="letter-spacing:-0.0551796292618659em">T</tspan>
						<tspan x="143.41" y="0" style="letter-spacing:-0.000021379166703551296em">rade-off: Code
							Organization vs Performance</tspan>
					</text>
					<g class="fragment" class="fragment" style="isolation:isolate"><text
							transform="translate(40.47 81.51)"
							style="isolation:isolate;font-size:18.271291732788086px;fill:#e74c3c;font-family:Arial-BoldMT, Arial;font-weight:700">Separated
							<tspan x="88.35" y="0" style="letter-spacing:-0.037092855005089555em"> </tspan>
							<tspan x="92.75" y="0">Activations:</tspan>
						</text><text transform="translate(237.79 81.51)"
							style="font-size:18.271291732788086px;fill:#fff;font-family:ArialMT, Arial">
							<tspan xml:space="preserve"> Looks clean, follows OO</tspan>
							<tspan x="202.09" y="0" style="letter-spacing:-0.018092120200609244em">P</tspan>
							<tspan x="213.95" y="0" xml:space="preserve"
								style="letter-spacing:0.000026723958937384405em"> principles, easy to understand</tspan>
						</text></g>
					<g class="fragment" class="fragment" style="isolation:isolate"><text
							transform="translate(40.47 104.35)"
							style="isolation:isolate;font-size:18.271291732788086px;fill:#2ed573;font-family:Arial-BoldMT, Arial;font-weight:700">Unified
							Memory:</text><text transform="translate(183.61 104.35)"
							style="font-size:18.271291732788086px;fill:#fff;font-family:ArialMT, Arial">
							<tspan x="5.08" y="0" style="letter-spacing:-0.01806539624167186em">V</tspan>
							<tspan x="16.93" y="0">iolates &quot;clean code&quot; but delivers HPC-level performance
							</tspan>
						</text></g><text transform="translate(460.64 132.9)"
						style="isolation:isolate;font-size:18.271291732788086px;fill:#f39c12;font-family:Arial-BoldMT, Arial;font-weight:700">In
						HPC inference: Performance trumps code aesthetics!</text><text
						transform="translate(616.96 188.41)"
						style="isolation:isolate;font-size:21.44890785217285px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">Why
						Separation Kills Performance</text>
					<rect x="17.63" y="212.25" width="655.38" height="238.32" rx="9.53"
						style="fill:#2c3e50;stroke:#e74c3c;stroke-width:2px" /><text transform="translate(29.55 242.04)"
						style="isolation:isolate;font-size:19.06569480895996px;fill:#e74c3c;font-family:Arial-BoldMT, Arial;font-weight:700">The
						Cache Miss Cascade:</text><text transform="translate(41.46 271.83)"
						style="isolation:isolate;font-size:16.682483673095703px;fill:#fff;font-family:ArialMT, Arial">1.
						CPU loads weight from memory</text><text transform="translate(41.46 295.66)"
						style="isolation:isolate;font-size:16.682483673095703px;fill:#fff;font-family:ArialMT, Arial">2.
						Cache line filled with weights (64 bytes)</text><text transform="translate(41.46 319.49)"
						style="isolation:isolate;font-size:16.682483673095703px;fill:#fff;font-family:ArialMT, Arial">3.
						CPU needs activation from di<tspan x="234.59" y="0"
							style="letter-spacing:-0.018059033484076812em">f</tspan>
						<tspan x="238.92" y="0" style="letter-spacing:0.00002926909802929791em">ferent location</tspan>
					</text><text transform="translate(41.46 343.32)"
						style="isolation:isolate;font-size:16.682483673095703px;fill:#fff;font-family:ArialMT, Arial">4.
						Cache miss! Evict weight data, load activations</text><text transform="translate(41.46 367.15)"
						style="isolation:isolate;font-size:16.682483673095703px;fill:#fff;font-family:ArialMT, Arial">5.
						Need next weight? Cache miss again!</text><text transform="translate(41.46 396.95)"
						style="isolation:isolate;font-size:16.682483673095703px;fill:#ff4757;font-family:Arial-BoldMT, Arial;font-weight:700">Result:
						Thrashing between memory regions</text><text transform="translate(41.46 420.78)"
						style="isolation:isolate;font-size:16.682483673095703px;fill:#ff4757;font-family:Arial-BoldMT, Arial;font-weight:700">Memory
						controller can&apos;t predict access pattern</text>
					<rect x="732.6" y="212.25" width="655.38" height="238.32" rx="9.53"
						style="fill:#2c3e50;stroke:#2ed573;stroke-width:2px" /><text
						transform="translate(744.51 242.04)"
						style="isolation:isolate;font-size:19.06569480895996px;fill:#2ed573;font-family:Arial-BoldMT, Arial;font-weight:700">The
						Sequential Solution:</text><text transform="translate(756.43 271.83)"
						style="isolation:isolate;font-size:16.682483673095703px;fill:#fff;font-family:ArialMT, Arial">1.
						CPU loads weight from unified memory</text><text transform="translate(756.43 295.66)"
						style="isolation:isolate;font-size:16.682483673095703px;fill:#fff;font-family:ArialMT, Arial">2.
						Cache line includes weight + next activation</text><text transform="translate(756.43 319.49)"
						style="isolation:isolate;font-size:16.682483673095703px;fill:#fff;font-family:ArialMT, Arial">3.
						Next data access hits cache (already loaded!)</text><text transform="translate(756.43 343.32)"
						style="isolation:isolate;font-size:16.682483673095703px;fill:#fff;font-family:ArialMT, Arial">4.
						Prefetcher predicts pattern, loads ahead</text><text transform="translate(756.43 367.15)"
						style="isolation:isolate;font-size:16.682483673095703px;fill:#fff;font-family:ArialMT, Arial">5.
						Every access hits cache or prefetch bu<tspan x="305.07" y="0"
							style="letter-spacing:-0.018088302582106107em">f</tspan>
						<tspan x="309.4" y="0">fer</tspan>
					</text><text transform="translate(756.43 396.95)"
						style="isolation:isolate;font-size:16.682483673095703px;fill:#26de81;font-family:Arial-BoldMT, Arial;font-weight:700">Result:
						Perfect cache utilization</text><text transform="translate(756.43 420.78)"
						style="isolation:isolate;font-size:16.682483673095703px;fill:#26de81;font-family:Arial-BoldMT, Arial;font-weight:700">Memory
						controller optimizes burst access</text>
				</svg>
			</section>

			<section style="transform: scale(1.4);">
				<aside class="notes">
					<p><strong>(CUE):</strong> <em>Transition from the previous slide that explained the general cache
							miss mechanism.</em></p>

					<p><strong>YOU:</strong> "We've seen how separated memory kills performance between major
						components, but the problem is actually even deeper. It happens at every single step *inside*
						each transformer layer. This slide shows that complete data flow chain."</p>

					<p><strong>(CUE):</strong> <em>Point to the orange header</em></p>

					<p><strong>YOU:</strong> "The data flow is LayerNorm → QKV → Attention → LayerNorm → MLP. Each arrow
						represents a critical handoff where one operation's output becomes the next operation's input."
					</p>

					<p><strong>(CUE):</strong> <em>Gesture to the left, "Traditional" column.</em></p>

					<p><strong>YOU:</strong> "On the left is the 'Cache Nightmare' of a traditional layout. Every
						intermediate result—the output from LayerNorm, the output of the QKV projection, the attention
						output—is written to a new, separate buffer somewhere else in memory. Every single arrow here
						represents a jump to a new memory location, which means a potential cache miss."</p>

					<p><strong>YOU:</strong> "The execution trace shows this clearly: LayerNorm writes its output, and
						the QKV step has to fetch it from a different location, causing a miss. The attention step then
						has to fetch the QKV output from yet another location, causing another miss. The result is a
						60-80% cache miss rate on every single intermediate step."</p>

					<p><strong>(CUE):</strong> <em>Gesture to the right, "Optimized" column.</em></p>

					<p><strong>YOU:</strong> "Now on the right is our unified 'Cache Paradise' layout. We've
						pre-allocated one contiguous memory block for all the intermediate results of the layer,
						arranged in the exact order of execution."</p>

					<p><strong>YOU:</strong> "Look at the perfect flow: LayerNorm writes its output to a specific
						offset. The QKV step reads its input from that *exact same offset*—it's an immediate cache hit
						because that data is already hot in the cache. It then writes its own output to the next
						sequential spot, where the attention step is waiting to read it."</p>

					<p><strong>YOU:</strong> "The data flows like a perfect pipeline with zero gaps. This allows the
						hardware prefetcher to work perfectly, giving us a 95%+ cache hit rate throughout the entire
						layer."</p>

					<p><strong>(CUE):</strong> <em>Deliver the final takeaway.</em></p>

					<p><strong>YOU:</strong> "This shows that true optimization isn't just about the big picture; it's
						about meticulously planning for every intermediate dependency. And this is exactly the level of
						detail that GPU programmers think about constantly - making sure every memory access is
						coalesced and predictable."</p>
				</aside>
				<svg id="b00a9b77-3646-4264-b2ab-5601f43891cb" data-name="Layer 1" xmlns="http://www.w3.org/2000/svg"
					xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 1500 504.7">
					<defs>
						<linearGradient id="ffb502f4-7a93-43a6-acb8-f89603b2b1dc" x1="248.77" y1="-248.81" x2="1251.23"
							y2="753.51" gradientUnits="userSpaceOnUse">
							<stop offset="0" stop-color="#0a0a0a" />
							<stop offset="1" stop-color="#1a1a2e" />
						</linearGradient>
						<linearGradient id="a6962ff7-4df7-430a-a944-b4ad326030a5" x1="-412.77" y1="-338.93" x2="-411.77"
							y2="-338.93" gradientTransform="translate(33072 10323) scale(80 30)"
							gradientUnits="userSpaceOnUse">
							<stop offset="0" stop-color="#3498db" />
							<stop offset="1" stop-color="#2980b9" />
						</linearGradient>
						<linearGradient id="e87f3ae6-84dd-4c6a-8c1d-fde0bd9fb83f" x1="-412.77" y1="-338.93" x2="-411.77"
							y2="-338.93" gradientTransform="translate(33182 10383) scale(80 30)"
							xlink:href="#a6962ff7-4df7-430a-a944-b4ad326030a5" />
						<linearGradient id="adf20954-a65f-4702-8c4d-50f070a27c18" x1="-412.77" y1="-338.93" x2="-411.77"
							y2="-338.93" gradientTransform="translate(33302 10323) scale(80 30)"
							gradientUnits="userSpaceOnUse">
							<stop offset="0" stop-color="#9b59b6" />
							<stop offset="1" stop-color="#8e44ad" />
						</linearGradient>
						<linearGradient id="a5b4e0e5-c7d2-4144-9e69-a28fd23ff7a0" x1="-412.77" y1="-338.93" x2="-411.77"
							y2="-338.93" gradientTransform="translate(33422 10383) scale(80 30)"
							gradientUnits="userSpaceOnUse">
							<stop offset="0" stop-color="#e74c3c" />
							<stop offset="1" stop-color="#c0392b" />
						</linearGradient>
						<linearGradient id="b4d0fb8a-aa7f-4163-8fdd-388faf918df1" x1="-412.77" y1="-338.93" x2="-411.77"
							y2="-338.93" gradientTransform="translate(33542 10323) scale(80 30)"
							xlink:href="#a6962ff7-4df7-430a-a944-b4ad326030a5" />
						<linearGradient id="b935af7c-a560-480f-8026-480107357428" x1="-412.77" y1="-338.93" x2="-411.77"
							y2="-338.93" gradientTransform="translate(33662 10383) scale(80 30)"
							gradientUnits="userSpaceOnUse">
							<stop offset="0" stop-color="#f39c12" />
							<stop offset="1" stop-color="#e67e22" />
						</linearGradient>
						<pattern id="e3ccaf04-cb62-4788-a207-5e07129013fb" data-name="SVGID 0000004198962464737954979"
							width="10" height="10" patternTransform="translate(18 17792.14)"
							patternUnits="userSpaceOnUse" viewBox="0 0 10 10">
							<rect width="10" height="10" style="fill:none" />
							<rect width="10" height="10" style="fill:none" />
							<rect width="10" height="10" style="fill:#2ed573" />
							<path d="M0,0H10M0,5H10M0,10H10" style="stroke:#26de81" />
						</pattern>
					</defs>
					<rect width="1500" height="504.7" style="fill:url(#ffb502f4-7a93-43a6-acb8-f89603b2b1dc)" /><text
						transform="translate(295.64 40)"
						style="isolation:isolate;font-size:28px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">Intra-Layer
						Cache Dependencies: Every Intermediate Output Matters</text><text
						transform="translate(440.7 65)"
						style="isolation:isolate;font-size:16px;fill:#f39c12;font-family:ArialMT, Arial">LayerNorm → QKV
						→<tspan x="158.26" y="0" style="letter-spacing:-0.05517578125em"> </tspan>
						<tspan x="161.82" y="0">Attention → LayerNorm → MLP:</tspan>
						<tspan x="391.23" y="0" style="letter-spacing:-0.01806640625em"> </tspan>
						<tspan x="395.39" y="0">The Complete Data Flow Chain</tspan>
					</text>
					<g class="fragment" class="fragment" id="a1b2c93e-b7a1-47cc-a1f3-e0c62c42a8ae"
						data-name="separated">
						<rect x="114.2" y="92.8" width="521.6" height="24.7" style="fill:none" /><text
							transform="translate(114.2 110)"
							style="isolation:isolate;font-size:20px;fill:#e74c3c;font-family:Arial-BoldMT, Arial;font-weight:700">
							<tspan style="letter-spacing:-0.05517578125em">T</tspan>
							<tspan x="11.11" y="0">raditional: Separated</tspan>
							<tspan x="210.05" y="0" style="letter-spacing:-0.037109375em"> </tspan>
							<tspan x="214.86" y="0">Activation Buffers (Cache </tspan>
						</text>
						<rect x="77.8" y="151.4" width="24.4" height="12.3" style="fill:none" />
						<rect x="76.9" y="174.1" width="26.2" height="9.5" style="fill:none" />
						<path d="M54,140h72a4,4,0,0,1,4,4v22a4,4,0,0,1-4,4H54a4,4,0,0,1-4-4V144A4,4,0,0,1,54,140Z"
							style="fill:url(#a6962ff7-4df7-430a-a944-b4ad326030a5)" /><text
							transform="translate(77.78 160)"
							style="isolation:isolate;font-size:10px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">Input</text><text
							transform="translate(76.88 181)"
							style="isolation:isolate;font-size:8px;fill:#95a5a6;font-family:ArialMT, Arial">0x1000</text>
						<rect x="180.6" y="211.4" width="38.9" height="12.3" style="fill:none" />
						<rect x="186.9" y="234.1" width="26.2" height="9.5" style="fill:none" />
						<path d="M164,200h72a4,4,0,0,1,4,4v22a4,4,0,0,1-4,4H164a4,4,0,0,1-4-4V204A4,4,0,0,1,164,200Z"
							style="fill:url(#e87f3ae6-84dd-4c6a-8c1d-fde0bd9fb83f)" /><text
							transform="translate(180.56 220)"
							style="isolation:isolate;font-size:10px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">LN1
							Out</text><text transform="translate(186.88 241)"
							style="isolation:isolate;font-size:8px;fill:#95a5a6;font-family:ArialMT, Arial">0x5000</text>
						<rect x="299.2" y="151.4" width="41.7" height="12.3" style="fill:none" />
						<rect x="306.9" y="174.1" width="26.2" height="9.5" style="fill:none" />
						<path d="M284,140h72a4,4,0,0,1,4,4v22a4,4,0,0,1-4,4H284a4,4,0,0,1-4-4V144A4,4,0,0,1,284,140Z"
							style="fill:url(#adf20954-a65f-4702-8c4d-50f070a27c18)" /><text
							transform="translate(299.17 160)"
							style="isolation:isolate;font-size:10px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">QKV
							Out</text><text transform="translate(306.88 181)"
							style="isolation:isolate;font-size:8px;fill:#95a5a6;font-family:ArialMT, Arial">0x8000</text>
						<rect x="420" y="211.4" width="40" height="12.3" style="fill:none" />
						<rect x="426.2" y="234.1" width="27.6" height="9.5" style="fill:none" />
						<path d="M404,200h72a4,4,0,0,1,4,4v22a4,4,0,0,1-4,4H404a4,4,0,0,1-4-4V204A4,4,0,0,1,404,200Z"
							style="fill:url(#a5b4e0e5-c7d2-4144-9e69-a28fd23ff7a0)" /><text
							transform="translate(420.01 220)"
							style="isolation:isolate;font-size:10px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">Attn
							Out</text><text transform="translate(426.21 241)"
							style="isolation:isolate;font-size:8px;fill:#95a5a6;font-family:ArialMT, Arial">0xC000</text>
						<rect x="540.6" y="151.4" width="38.9" height="12.3" style="fill:none" />
						<rect x="546.7" y="174.1" width="26.7" height="9.5" style="fill:none" />
						<path d="M524,140h72a4,4,0,0,1,4,4v22a4,4,0,0,1-4,4H524a4,4,0,0,1-4-4V144A4,4,0,0,1,524,140Z"
							style="fill:url(#b4d0fb8a-aa7f-4163-8fdd-388faf918df1)" /><text
							transform="translate(540.56 160)"
							style="isolation:isolate;font-size:10px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">LN2
							Out</text><text transform="translate(546.66 181)"
							style="isolation:isolate;font-size:8px;fill:#95a5a6;font-family:ArialMT, Arial">0xF000</text>
						<rect x="659.5" y="211.4" width="40.9" height="12.3" style="fill:none" />
						<rect x="664.7" y="234.1" width="30.7" height="9.5" style="fill:none" />
						<path d="M644,200h72a4,4,0,0,1,4,4v22a4,4,0,0,1-4,4H644a4,4,0,0,1-4-4V204A4,4,0,0,1,644,200Z"
							style="fill:url(#b935af7c-a560-480f-8026-480107357428)" /><text
							transform="translate(669.07 220)"
							style="isolation:isolate;font-size:10px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">ML
							<tspan x="14.44" y="0" style="letter-spacing:-0.01806640625em">P</tspan>
							<tspan x="20.93" y="0"> </tspan>
						</text><text transform="translate(664.65 241)"
							style="isolation:isolate;font-size:8px;fill:#95a5a6;font-family:ArialMT, Arial">0x12000</text>
						<rect x="209.4" y="249.7" width="331.2" height="14.8" style="fill:none" /><text
							transform="translate(209.41 260)"
							style="isolation:isolate;font-size:12px;fill:#ff4757;font-family:Arial-BoldMT, Arial;font-weight:700">Every
							arrow = Cache miss! Data scattered across memory</text>
						<path d="M58,280H692a8,8,0,0,1,8,8V472a8,8,0,0,1-8,8H58a8,8,0,0,1-8-8V288A8,8,0,0,1,58,280Z"
							style="fill:#2c2c54;stroke:#e74c3c;stroke-width:2px" />
						<rect x="60" y="291.3" width="290.8" height="19.8" style="fill:none" />
						<g class="fragment" class="fragment" style="isolation:isolate"><text
								transform="translate(60 305)"
								style="isolation:isolate;font-size:16px;fill:#e74c3c;font-family:Arial-BoldMT, Arial;font-weight:700">Execution
								<tspan x="80.91" y="0" style="letter-spacing:-0.05517578125em">T</tspan>
								<tspan x="89.8" y="0">race: Cache Miss Cascade</tspan>
							</text></g>
						<rect x="70" y="313.8" width="127.8" height="16" style="fill:none" /><text
							transform="translate(70 325)"
							style="isolation:isolate;font-size:13px;fill:#3498db;font-family:Arial-BoldMT, Arial;font-weight:700">1.
							LayerNorm(input):</text>
						<rect x="80" y="329.7" width="277" height="14.2" style="fill:none" /><text
							transform="translate(80 340)"
							style="isolation:isolate;font-size:12px;fill:#fff;font-family:ArialMT, Arial">• Read input
							from 0x1000 → Cache loads input data</text>
						<rect x="80" y="344.7" width="300.6" height="14.2" style="fill:none" /><text
							transform="translate(80 355)"
							style="isolation:isolate;font-size:12px;fill:#fff;font-family:ArialMT, Arial">• <tspan
								x="7.54" y="0" style="letter-spacing:-0.01806640625em">W</tspan>
							<tspan x="18.64" y="0">rite LN1 output to 0x5000 → Di</tspan>
							<tspan x="183.4" y="0" style="letter-spacing:-0.01806640625em">f</tspan>
							<tspan x="186.52" y="0">ferent memory </tspan>
						</text>
						<rect x="70" y="363.8" width="127.8" height="16" style="fill:none" /><text
							transform="translate(70 375)"
							style="isolation:isolate;font-size:13px;fill:#9b59b6;font-family:Arial-BoldMT, Arial;font-weight:700">2.
							QKV(LN1_output):</text>
						<rect x="80" y="379.7" width="336.3" height="14.2" style="fill:none" /><text
							transform="translate(80 390)"
							style="isolation:isolate;font-size:12px;fill:#ff4757;font-family:ArialMT, Arial">• Read LN1
							output from 0x5000 → CACHE MISS! (evicts input)</text>
						<rect x="80" y="394.7" width="307.9" height="14.2" style="fill:none" />
						<g class="fragment" class="fragment" style="isolation:isolate"><text
								transform="translate(80 405)"
								style="isolation:isolate;font-size:12px;fill:#fff;font-family:ArialMT, Arial">• <tspan
									x="7.54" y="0" style="letter-spacing:-0.01806640625em">W</tspan>
								<tspan x="18.64" y="0">rite QKV output to 0x8000 →</tspan>
								<tspan x="172.07" y="0" style="letter-spacing:-0.05517578125em"> </tspan>
								<tspan x="174.74" y="0">Another di</tspan>
								<tspan x="229.44" y="0" style="letter-spacing:-0.01806640625em">f</tspan>
								<tspan x="232.56" y="0">ferent loca</tspan>
							</text><text transform="translate(369.7 405)"
								style="isolation:isolate;font-size:12px;fill:#fff;font-family:ArialMT, Arial">-</text>
						</g>
						<rect x="70" y="413.8" width="159.8" height="16" style="fill:none" />
						<g class="fragment" class="fragment" style="isolation:isolate"><text
								transform="translate(70 425)"
								style="isolation:isolate;font-size:13px;fill:#e74c3c;font-family:Arial-BoldMT, Arial;font-weight:700">3.
								<tspan x="10.84" y="0" style="letter-spacing:-0.037109375em"> </tspan>
								<tspan x="13.97" y="0">Attention(QKV_out</tspan>
							</text><text transform="translate(201.4 425)"
								style="isolation:isolate;font-size:13px;fill:#e74c3c;font-family:Arial-BoldMT, Arial;font-weight:700">-</text>
						</g>
						<rect x="80" y="429.7" width="325.7" height="14.2" style="fill:none" /><text
							transform="translate(80 440)"
							style="isolation:isolate;font-size:12px;fill:#ff4757;font-family:ArialMT, Arial">• Read QKV
							from 0x8000 → CACHE MISS! (evicts LN1 data)</text>
						<rect x="80" y="444.7" width="266.8" height="14.2" style="fill:none" />
						<g class="fragment" class="fragment" style="isolation:isolate"><text
								transform="translate(80 455)"
								style="isolation:isolate;font-size:12px;fill:#fff;font-family:ArialMT, Arial">• <tspan
									x="7.54" y="0" style="letter-spacing:-0.01806640625em">W</tspan>
								<tspan x="18.64" y="0">rite attention to 0xC000 →</tspan>
								<tspan x="158.06" y="0" style="letter-spacing:-0.01806640625em"> </tspan>
								<tspan x="161.17" y="0" style="letter-spacing:-0.091796875em">Y</tspan>
								<tspan x="168.08" y="0">et another loca</tspan>
							</text><text transform="translate(329 455)"
								style="isolation:isolate;font-size:12px;fill:#fff;font-family:ArialMT, Arial">-</text>
						</g>
						<rect x="70" y="459.7" width="346.8" height="14.8" style="fill:none" /><text
							transform="translate(70 470)"
							style="isolation:isolate;font-size:12px;fill:#ff4757;font-family:Arial-BoldMT, Arial;font-weight:700">Result:
							60-80% cache miss rate on EVE<tspan x="224.1" y="0" style="letter-spacing:-0.037109375em">R
							</tspan>
							<tspan x="232.32" y="0" style="letter-spacing:-0.01806640625em">Y</tspan>
							<tspan x="240.11" y="0" xml:space="preserve"> intermediate </tspan>
						</text>
						<line x1="133.4" y1="170" x2="150" y2="186.6"
							style="fill:none;stroke:#ff4757;stroke-miterlimit:10;stroke-width:2px" />
						<polygon points="160 196.6 140.2 188.3 149.3 185.9 151.7 176.8 160 196.6"
							style="fill:#ff4757" />
						<line x1="245.1" y1="194.4" x2="261.7" y2="177.8"
							style="fill:none;stroke:#ff4757;stroke-miterlimit:10;stroke-width:2px" />
						<polygon points="271.7 167.8 263.4 187.6 261 178.5 251.9 176.1 271.7 167.8"
							style="fill:#ff4757" />
						<line x1="370.8" y1="170" x2="387.5" y2="186.6"
							style="fill:none;stroke:#ff4757;stroke-miterlimit:10;stroke-width:2px" />
						<polygon points="397.5 196.6 377.7 188.3 386.7 185.9 389.2 176.8 397.5 196.6"
							style="fill:#ff4757" />
						<line x1="608.1" y1="170" x2="624.8" y2="186.6"
							style="fill:none;stroke:#ff4757;stroke-miterlimit:10;stroke-width:2px" />
						<polygon points="634.8 196.6 614.9 188.3 624 185.9 626.4 176.8 634.8 196.6"
							style="fill:#ff4757" />
						<line x1="482.5" y1="194.4" x2="499.2" y2="177.8"
							style="fill:none;stroke:#ff4757;stroke-miterlimit:10;stroke-width:2px" />
						<polygon points="509.2 167.8 500.8 187.6 498.4 178.5 489.3 176.1 509.2 167.8"
							style="fill:#ff4757" />
					</g>
					<g class="fragment" class="fragment" id="ac9fc892-754b-40c0-8aa6-297b90226a3d" data-name="optmized">
						<text transform="translate(876.06 110)"
							style="isolation:isolate;font-size:20px;fill:#2ed573;font-family:Arial-BoldMT, Arial;font-weight:700">Optimized:
							Unified Memory Layout (Cache Paradise)</text>
						<path d="M806,140h638a6,6,0,0,1,6,6v28a6,6,0,0,1-6,6H806a6,6,0,0,1-6-6V146A6,6,0,0,1,806,140Z"
							style="stroke:#2ed573;stroke-width:3px;fill:url(#e3ccaf04-cb62-4788-a207-5e07129013fb)" />
						<text transform="translate(825 135)"
							style="isolation:isolate;font-size:9px;fill:#fff;font-family:ArialMT, Arial">Input</text><text
							transform="translate(875 135)"
							style="isolation:isolate;font-size:9px;fill:#fff;font-family:ArialMT, Arial">LN1</text><text
							transform="translate(925 135)"
							style="isolation:isolate;font-size:9px;fill:#fff;font-family:ArialMT, Arial">QKV</text><text
							transform="translate(975 135)"
							style="isolation:isolate;font-size:9px;fill:#fff;font-family:ArialMT, Arial">Attn</text><text
							transform="translate(1025 135)"
							style="isolation:isolate;font-size:9px;fill:#fff;font-family:ArialMT, Arial">LN2</text><text
							transform="translate(1075 135)"
							style="isolation:isolate;font-size:9px;fill:#fff;font-family:ArialMT, Arial">ML<tspan
								x="12.5" y="0" style="letter-spacing:-0.01806640625em">P</tspan></text><text
							transform="translate(1125 135)"
							style="isolation:isolate;font-size:9px;fill:#fff;font-family:ArialMT, Arial">Resid</text><text
							transform="translate(1175 135)"
							style="isolation:isolate;font-size:9px;fill:#fff;font-family:ArialMT, Arial">Output</text><text
							transform="translate(825 195)"
							style="isolation:isolate;font-size:9px;fill:#fff;font-family:ArialMT, Arial">0x1000</text><text
							transform="translate(875 195)"
							style="isolation:isolate;font-size:9px;fill:#fff;font-family:ArialMT, Arial">+64B</text><text
							transform="translate(925 195)"
							style="isolation:isolate;font-size:9px;fill:#fff;font-family:ArialMT, Arial">+128B</text><text
							transform="translate(975 195)"
							style="isolation:isolate;font-size:9px;fill:#fff;font-family:ArialMT, Arial">+256B</text><text
							transform="translate(1025 195)"
							style="isolation:isolate;font-size:9px;fill:#fff;font-family:ArialMT, Arial">+384B</text><text
							transform="translate(1075 195)"
							style="isolation:isolate;font-size:9px;fill:#fff;font-family:ArialMT, Arial">+448B</text><text
							transform="translate(1125 195)"
							style="isolation:isolate;font-size:9px;fill:#fff;font-family:ArialMT, Arial">+576B</text><text
							transform="translate(1175 195)"
							style="isolation:isolate;font-size:9px;fill:#fff;font-family:ArialMT, Arial">+640B</text><text
							transform="translate(909.14 230)"
							style="isolation:isolate;font-size:14px;fill:#2ed573;font-family:Arial-BoldMT, Arial;font-weight:700">Perfect
							Sequential Flow: Every Output → Next Input (Zero Gaps!)</text>
						<path d="M808,280h634a8,8,0,0,1,8,8V472a8,8,0,0,1-8,8H808a8,8,0,0,1-8-8V288A8,8,0,0,1,808,280Z"
							style="fill:#2c2c54;stroke:#2ed573;stroke-width:2px" /><text transform="translate(810 305)"
							style="isolation:isolate;font-size:16px;fill:#2ed573;font-family:Arial-BoldMT, Arial;font-weight:700">Execution
							<tspan x="80.91" y="0" style="letter-spacing:-0.05517578125em">T</tspan>
							<tspan x="89.8" y="0">race: Perfect Cache Flow</tspan>
						</text><text transform="translate(820 325)"
							style="isolation:isolate;font-size:13px;fill:#3498db;font-family:Arial-BoldMT, Arial;font-weight:700">1.
							LayerNorm(input):</text><text transform="translate(830 340)"
							style="isolation:isolate;font-size:12px;fill:#fff;font-family:ArialMT, Arial">• Read input
							from o<tspan x="102.92" y="0" style="letter-spacing:-0.01806640625em">f</tspan>
							<tspan x="106.04" y="0">fset 0 → Cache loads 64-byte line</tspan>
						</text><text transform="translate(830 355)"
							style="isolation:isolate;font-size:12px;fill:#26de81;font-family:ArialMT, Arial">• <tspan
								x="7.54" y="0" style="letter-spacing:-0.01806640625em">W</tspan>
							<tspan x="18.64" y="0">rite LN1 output to o</tspan>
							<tspan x="120.71" y="0" style="letter-spacing:-0.01806640625em">f</tspan>
							<tspan x="123.83" y="0">fset+64B → Next sequential location!</tspan>
						</text><text transform="translate(820 375)"
							style="isolation:isolate;font-size:13px;fill:#9b59b6;font-family:Arial-BoldMT, Arial;font-weight:700">2.
							QKV(LN1_output):</text><text transform="translate(830 390)"
							style="isolation:isolate;font-size:12px;fill:#26de81;font-family:ArialMT, Arial">• Read LN1
							from o<tspan x="98.91" y="0" style="letter-spacing:-0.01806640625em">f</tspan>
							<tspan x="102.03" y="0">fset+64B → CACHE HIT! (already loaded)</tspan>
						</text><text transform="translate(830 405)"
							style="isolation:isolate;font-size:12px;fill:#26de81;font-family:ArialMT, Arial">• <tspan
								x="7.54" y="0" style="letter-spacing:-0.01806640625em">W</tspan>
							<tspan x="18.64" y="0">rite QKV to o</tspan>
							<tspan x="87.34" y="0" style="letter-spacing:-0.01806640625em">f</tspan>
							<tspan x="90.46" y="0">fset+128B → Sequential, prefetcher ready</tspan>
						</text><text transform="translate(820 425)"
							style="isolation:isolate;font-size:13px;fill:#e74c3c;font-family:Arial-BoldMT, Arial;font-weight:700">3.
							<tspan x="10.84" y="0" style="letter-spacing:-0.037109375em"> </tspan>
							<tspan x="13.97" y="0">Attention(QKV_output):</tspan>
						</text><text transform="translate(830 440)"
							style="isolation:isolate;font-size:12px;fill:#26de81;font-family:ArialMT, Arial">• Read QKV
							from o<tspan x="102.24" y="0" style="letter-spacing:-0.01806640625em">f</tspan>
							<tspan x="105.36" y="0">fset+128B → CACHE HIT! (prefetched)</tspan>
						</text><text transform="translate(830 455)"
							style="isolation:isolate;font-size:12px;fill:#26de81;font-family:ArialMT, Arial">• <tspan
								x="7.54" y="0" style="letter-spacing:-0.01806640625em">W</tspan>
							<tspan x="18.64" y="0">rite attention to o</tspan>
							<tspan x="108.04" y="0" style="letter-spacing:-0.01806640625em">f</tspan>
							<tspan x="111.15" y="0">fset+256B → Continues perfect flow</tspan>
						</text><text transform="translate(820 470)"
							style="isolation:isolate;font-size:12px;fill:#2ed573;font-family:Arial-BoldMT, Arial;font-weight:700">Result:
							95%+ cache hit rate - data flows like a perfect pipeline!</text>
						<line x1="800" y1="210" x2="1431" y2="210"
							style="fill:none;stroke:#2ed573;stroke-miterlimit:10;stroke-width:4px" />
						<path
							d="M1455.2,210c-11.4,4.2-25.5,11.4-34.2,19l6.9-19-6.9-19C1429.7,198.6,1443.8,205.8,1455.2,210Z"
							style="fill:#2ed573" />
					</g>
				</svg>
			</section>
			<section style="transform: scale(1.3);">
				<aside class="notes">
					<p><strong>(CUE):</strong> <em>Transition from the previous slide, which showed a simplified
							intra-layer comparison.</em></p>

					<p><strong>YOU:</strong> "So, let's zoom out and look at the complete picture for an entire
						transformer layer. This diagram at the top shows all the data dependencies in sequential order,
						from the initial layer input, through LayerNorm, self-attention, the MLP, and the final residual
						connection to the layer's output."</p>

					<p><strong>(CUE):</strong> <em>Point to the flow diagram and emphasize the arrows</em></p>

					<p><strong>YOU:</strong> "Count those arrows - there are ten handoffs between eleven operations. And
						here's the critical insight: every arrow represents a data dependency that must hit cache for
						optimal performance."</p>

					<p><strong>(CUE):</strong> <em>Point to the implementation code section</em></p>

					<p><strong>YOU:</strong> "Here's how we implement this in practice: Every intermediate result is
						placed in sequential memory. Look at this pattern - ln1_output equals layer_input plus
						input_size, qkv_output equals ln1_output plus ln1_size. Each output is exactly where the next
						computation expects to find it."</p>

					<p><strong>(CUE):</strong> <em>Point to the green comment in the code</em></p>

					<p><strong>YOU:</strong> "The comment captures the essence: 'Perfect data pipeline: Each output is
						exactly where next computation expects it!' This transforms every potential cache miss into a
						guaranteed cache hit."</p>

					<p><strong>(CUE):</strong> <em>Point to the performance comparison at the bottom</em></p>

					<p><strong>YOU:</strong> "And the numbers are dramatic: Traditional approach gives us 10+ cache
						misses per layer times 12 layers equals 120+ cache misses per forward pass. Our approach? 1-2
						cache misses per layer times 12 layers equals roughly 15 cache misses total."</p>

					<p><strong>(CUE):</strong> <em>Deliver the impact</em></p>

					<p><strong>YOU:</strong> "That's an 8x reduction in cache misses, which translates directly to
						massive performance improvements. This is the difference between designing software for the
						hardware versus against it."</p>
				</aside>
				<svg id="a8fbe899-3e39-4c2f-9c64-7f5ed7a343b7" data-name="Layer 1" xmlns="http://www.w3.org/2000/svg"
					viewBox="0 0 1500 572.94">
					<g class="fragment" class="fragment" id="a204047d-714f-4d06-be4f-cf30789b0c6b"
						data-name="performance">
						<path
							d="M34.1,424.54H1465.9a13.51,13.51,0,0,1,13.5,13.5v107.7a13.51,13.51,0,0,1-13.5,13.5H34.1a13.51,13.51,0,0,1-13.5-13.5V438A13.57,13.57,0,0,1,34.1,424.54Z"
							style="fill:#2c3e50;stroke:#f39c12;stroke-width:3px" /><text
							transform="translate(543 458.2)"
							style="isolation:isolate;font-size:22.441200256347656px;fill:#f39c12;font-family:Arial-BoldMT, Arial;font-weight:700">The
							Complete Performance Revolution</text><text transform="translate(386.29 491.87)"
							style="isolation:isolate;font-size:17.952999114990234px;fill:#e74c3c;font-family:Arial-BoldMT, Arial;font-weight:700">
							<tspan style="letter-spacing:-0.05518424247137489em">T</tspan>
							<tspan x="9.98" y="0">raditional:</tspan>
						</text><text transform="translate(483.05 491.87)"
							style="isolation:isolate;font-size:17.952999114990234px;fill:#fff;font-family:ArialMT, Arial">
							<tspan xml:space="preserve"> 10+ cache misses per layer × 12 layers = 120+ cache misses per
								forward pass</tspan>
						</text><text transform="translate(424.98 519.92)"
							style="isolation:isolate;font-size:17.952999114990234px;fill:#2ed573;font-family:Arial-BoldMT, Arial;font-weight:700">
							<tspan style="letter-spacing:-0.0742226701352302em">Y</tspan>
							<tspan x="10.64" y="0">our</tspan>
							<tspan x="39.56" y="0" style="letter-spacing:-0.03709773619071235em"> </tspan>
							<tspan x="43.88" y="0">Approach:</tspan>
						</text><text transform="translate(558.63 519.92)"
							style="isolation:isolate;font-size:17.952999114990234px;fill:#fff;font-family:ArialMT, Arial">
							<tspan xml:space="preserve"> 1-2 cache misses per layer × 12 layers = ~15 cache misses total
							</tspan>
						</text>
					</g>
					<g class="fragment" class="fragment" id="b962d89d-6378-4710-ae71-5ae241a08498"
						data-name="data_flow">
						<rect x="20.7" y="11.94" width="1458.7" height="201.97" rx="13.46"
							style="fill:#34495e;stroke:#f39c12;stroke-width:3px" /><text
							transform="translate(502.51 45.6)"
							style="isolation:isolate;font-size:22.441539764404297px;fill:#f39c12;font-family:Arial-BoldMT, Arial;font-weight:700">Complete
							Intra-Layer Data Flow Dependencies</text>
						<rect x="43.14" y="68.04" width="78.55" height="44.88" rx="4.49" style="fill:#3498db" /><text
							transform="translate(49.15 96.09)"
							style="isolation:isolate;font-size:12.342846870422363px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">Layer
							Input</text>
						<rect x="175.55" y="68.04" width="78.55" height="44.88" rx="4.49" style="fill:#3498db" /><text
							transform="translate(178.8 96.09)"
							style="isolation:isolate;font-size:12.342846870422363px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">LayerNorm1</text>
						<rect x="307.95" y="68.04" width="78.55" height="44.88" rx="4.49" style="fill:#9b59b6" /><text
							transform="translate(320.14 96.09)"
							style="isolation:isolate;font-size:12.342846870422363px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">QKV
							Proj</text>
						<rect x="440.36" y="68.04" width="78.55" height="44.88" rx="4.49" style="fill:#e74c3c" /><text
							transform="translate(453.92 96.09)"
							style="isolation:isolate;font-size:12.342846870422363px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">Self-Attn</text>
						<rect x="572.76" y="68.04" width="78.55" height="44.88" rx="4.49" style="fill:#e74c3c" /><text
							transform="translate(585.98 96.09)"
							style="isolation:isolate;font-size:12.342846870422363px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">Attn
							Proj</text>
						<rect x="837.58" y="68.04" width="78.55" height="44.88" rx="4.49" style="fill:#3498db" /><text
							transform="translate(840.83 96.09)"
							style="isolation:isolate;font-size:12.342846870422363px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">LayerNorm2</text>
						<rect x="969.98" y="68.04" width="78.55" height="44.88" rx="4.49" style="fill:#f39c12" /><text
							transform="translate(982.97 96.09)"
							style="isolation:isolate;font-size:12.342846870422363px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">ML
							<tspan x="17.82" y="0" style="letter-spacing:-0.018039294527226104em">P</tspan>
							<tspan x="25.83" y="0" xml:space="preserve"
								style="letter-spacing:-0.00007911971283871099em"> FC1</tspan>
						</text>
						<rect x="1102.39" y="68.04" width="78.55" height="44.88" rx="4.49" style="fill:#f39c12" /><text
							transform="translate(1115.37 96.09)"
							style="isolation:isolate;font-size:12.342846870422363px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">ML
							<tspan x="17.82" y="0" style="letter-spacing:-0.018039294527226104em">P</tspan>
							<tspan x="25.83" y="0" xml:space="preserve"
								style="letter-spacing:-0.00007911971283871099em"> FC2</tspan>
						</text>
						<rect x="1367.2" y="68.04" width="78.55" height="44.88" rx="4.49" style="fill:#27ae60" /><text
							transform="translate(1377.66 96.09)"
							style="isolation:isolate;font-size:12.342846870422363px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">Layer
							Out</text>
						<polygon points="133.07 78.7 166.73 90.48 133.07 102.26 133.07 78.7" style="fill:#2ed573" />
						<polygon points="265.32 78.7 298.98 90.48 265.32 102.26 265.32 78.7" style="fill:#2ed573" />
						<polygon points="397.56 78.7 431.23 90.48 397.56 102.26 397.56 78.7" style="fill:#2ed573" />
						<polygon points="529.81 78.7 563.48 90.48 529.81 102.26 529.81 78.7" style="fill:#2ed573" />
						<polygon points="662.06 78.7 695.72 90.48 662.06 102.26 662.06 78.7" style="fill:#2ed573" />
						<polygon points="794.31 78.7 827.97 90.48 794.31 102.26 794.31 78.7" style="fill:#2ed573" />
						<polygon points="926.56 78.7 960.22 90.48 926.56 102.26 926.56 78.7" style="fill:#2ed573" />
						<polygon points="1058.8 78.7 1092.47 90.48 1058.8 102.26 1058.8 78.7" style="fill:#2ed573" />
						<polygon points="1191.05 78.7 1224.71 90.48 1191.05 102.26 1191.05 78.7" style="fill:#2ed573" />
						<polygon points="1323.3 78.7 1356.96 90.48 1323.3 102.26 1323.3 78.7" style="fill:#2ed573" />
						<path d="M82.41,112.92q0,33.66,679.22,33.66,0-33.66-52.83-56.1"
							style="fill:none;stroke:#95a5a6;stroke-width:2px;stroke-dasharray:3,3" />
						<path d="M741.05,121.83q0,56.12,594.29,56.11,0-56.11-57.78-78.55"
							style="fill:none;stroke:#95a5a6;stroke-width:2px;stroke-dasharray:3,3" /><text
							transform="translate(513.46 135.36)"
							style="isolation:isolate;font-size:15.709077835083008px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">Every
							arrow represents a data dependency that must hit cache!</text><text
							transform="translate(454.93 191.47)"
							style="isolation:isolate;font-size:15.709077835083008px;fill:#2ed573;font-family:ArialMT, Arial">In
							unified memory:<tspan x="129.22" y="0" style="letter-spacing:-0.055171871184851146em">
							</tspan>
							<tspan x="132.71" y="0">All these intermediate results are sequential → Perfect cache flow
							</tspan>
						</text>
						<rect x="705.17" y="68.04" width="78.55" height="44.88" rx="4.49" style="fill:#3a3a3a" /><text
							transform="translate(715.29 96.09)"
							style="isolation:isolate;font-size:12.342846870422363px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">Residual1</text>
						<rect x="1234.79" y="68.04" width="78.55" height="44.88" rx="4.49" style="fill:#3a3a3a" /><text
							transform="translate(1244.91 96.09)"
							style="isolation:isolate;font-size:12.342846870422363px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">Residual2</text>
					</g>
					<g class="fragment" class="fragment" id="bbb1b8f9-9e79-4c22-9509-212bc5ff3699"
						data-name="implementation">
						<path
							d="M31.9,245H1468.1a11.18,11.18,0,0,1,11.2,11.2v134.6a11.18,11.18,0,0,1-11.2,11.2H31.9a11.18,11.18,0,0,1-11.2-11.2V256.24A11.18,11.18,0,0,1,31.9,245Z"
							style="fill:#1e1e1e;stroke:#3498db;stroke-width:2px" /><text
							transform="translate(31.88 273.06)"
							style="isolation:isolate;font-size:15.708900451660156px;fill:#2ed573;font-family:Arial-BoldMT, Arial;font-weight:700">Implementation:
							Every Intermediate Result in Sequential Memory</text><text transform="translate(43.1 295.5)"
							style="isolation:isolate;font-size:13.464699745178223px;fill:#3498db;font-family:ArialMT, Arial">//
							<tspan x="7.48" y="0" style="letter-spacing:-0.05515724786332172em"> </tspan>
							<tspan x="10.48" y="0">All intermediate activations in execution order</tspan>
						</text><text transform="translate(43.1 312.34)"
							style="isolation:isolate;font-size:13.464699745178223px;fill:#ffa502;font-family:ArialMT, Arial">float*
							layer_input = unified_memory + layer_o<tspan x="270.94" y="0"
								style="letter-spacing:-0.018023111234760615em">f</tspan>
							<tspan x="274.44" y="0">fset;</tspan>
						</text><text transform="translate(43.1 329.17)"
							style="isolation:isolate;font-size:13.464699745178223px;fill:#ffa502;font-family:ArialMT, Arial">float*
							ln1_output = layer_input + input_size; // Right after input</text><text
							transform="translate(43.1 346)"
							style="isolation:isolate;font-size:13.464699745178223px;fill:#ffa502;font-family:ArialMT, Arial">float*
							qkv_output = ln1_output + ln1_size; // Right after LN1</text><text
							transform="translate(43.1 362.83)"
							style="isolation:isolate;font-size:13.464699745178223px;fill:#ffa502;font-family:ArialMT, Arial">float*
							attention_out = qkv_output + qkv_size; // Right after QKV</text><text
							transform="translate(43.1 379.66)"
							style="isolation:isolate;font-size:13.464699745178223px;fill:#ffa502;font-family:ArialMT, Arial">float*
							residual1_out = attention_out + attn_size; // Right after attention</text><text
							transform="translate(43.1 395.87)"
							style="isolation:isolate;font-size:12px;fill:#ff6b6b;font-family:ArialMT, Arial">// Perfect
							data pipeline: Each output is exactly where next computation expects it!</text>
					</g>
				</svg>
			</section>
			<section style="transform: scale(1.3);">
				<aside class="notes">
					<p><strong>(CUE):</strong> <em>Transition from the previous slide on intra-layer dependencies.</em>
					</p>

					<p><strong>YOU:</strong> "So, we've solved the cache dependency problem *inside* a layer. But what
						happens when we move from Layer 0 to Layer 1? This is where the output of one layer becomes the
						input for the next, and this dependency determines the overall cache performance of the model."
					</p>

					<p><strong>(CUE):</strong> <em>Point to the orange subtitle</em></p>

					<p><strong>YOU:</strong> "The subtitle captures it perfectly: 'Why Output → Input Dependencies
						Determine Cache Performance.' This is the fundamental insight."</p>

					<p><strong>(CUE):</strong> <em>Gesture to the left, "Fragmented Activations" column.</em></p>

					<p><strong>YOU:</strong> "On the left, with a fragmented design, the output of each layer is written
						to a completely separate memory location. Look at these addresses: 0x1000, 0x5000, 0x8000 -
						scattered across memory with gaps of other data in between. When Layer 1 needs the output from
						Layer 0, it has to go and fetch it from a different address, causing a cache miss. This creates
						a 'Cache Miss Chain Reaction'."</p>

					<p><strong>YOU:</strong> "As the execution trace shows, Layer 2 then needs to fetch the output from
						Layer 1, causing another miss. The result is constant cache thrashing, where the data needed for
						the next step is always in a different location and has to be slowly loaded from main memory."
					</p>

					<p><strong>(CUE):</strong> <em>Gesture to the right, "Contiguous Activations" column.</em></p>

					<p><strong>YOU:</strong> "Now, compare that to our contiguous approach on the right. We create a
						'Perfect Cache Chain.' How? We design the memory layout so that the output buffer for Layer 0
						*is* the input buffer for Layer 1. They are the same physical memory location."</p>

					<p><strong>YOU:</strong> "The execution trace shows the benefit: when Layer 1 starts, the data it
						needs is already hot in the cache from the previous step. It's a perfect, sequential cache hit.
						The output of Layer N flows seamlessly to the input of Layer N+1, achieving perfect locality."
					</p>

					<p><strong>(CUE):</strong> <em>Deliver the final takeaway for the slide.</em></p>

					<p><strong>YOU:</strong> "The core insight is this: the physical location of a layer's output must
						equal the physical location of the next layer's input. By engineering our layout this way, we
						transform the expensive transition between layers from a cache bottleneck into a high-speed data
						flow. This is how we maintain a 95%+ cache hit rate across the entire model, not just within a
						single component."</p>
				</aside>
				<svg id="b893edfe-7ca4-4dd9-aa56-9cadd3858075" data-name="Layer 1" xmlns="http://www.w3.org/2000/svg"
					xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 1400 575.19">
					<defs>
						<pattern id="f289b82c-6e54-4286-b7bc-4c80e9a2bf33" data-name="cacheMissPattern" width="12"
							height="12" patternTransform="matrix(1, 0, 0, -1, 7, 1070)" patternUnits="userSpaceOnUse"
							viewBox="0 0 12 12">
							<rect width="12" height="12" style="fill:none" />
							<rect width="12" height="12" style="fill:#e74c3c" />
							<path d="M0,0,12,12M6-6,18,6M-6,6,6,18" style="stroke:#c0392b;stroke-width:2px" />
						</pattern>
						<pattern id="e6b7c308-0263-4b1d-920c-6bf74c58a957" data-name="cacheHitPattern" width="12"
							height="12" patternTransform="matrix(1, 0, 0, -1, 7, 1070)" patternUnits="userSpaceOnUse"
							viewBox="0 0 12 12">
							<rect width="12" height="12" style="fill:none" />
							<rect width="12" height="12" style="fill:#2ed573" />
							<circle cx="6" cy="6" r="2" style="fill:#26de81" />
						</pattern>
					</defs><text transform="translate(249.5 40)"
						style="isolation:isolate;font-size:28px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">Sequential
						Processing: Cache Dependencies in <tspan x="639.53" y="0"
							style="letter-spacing:-0.05517578125em">T</tspan>
						<tspan x="655.09" y="0">ransformer Layers</tspan>
					</text><text transform="translate(461.66 65)"
						style="isolation:isolate;font-size:16px;fill:#f39c12;font-family:ArialMT, Arial">Why Output →
						Input Dependencies Determine Cache Performance</text><text transform="translate(99.76 110)"
						style="isolation:isolate;font-size:20px;fill:#e74c3c;font-family:Arial-BoldMT, Arial;font-weight:700">Fragmented
						<tspan x="114.46" y="0" style="letter-spacing:-0.037109375em"> </tspan>
						<tspan x="119.28" y="0">Activations: Cache Miss Chain Reaction</tspan>
					</text>
					<rect x="50" y="140" width="120" height="40" rx="4" style="fill:#3498db" /><text
						transform="translate(67.66 165)"
						style="isolation:isolate;font-size:12px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">Layer
						0 Output</text><text transform="translate(72.76 190)"
						style="isolation:isolate;font-size:10px;fill:#95a5a6;font-family:ArialMT, Arial">Memory:
						0x1000</text>
					<rect x="200" y="140" width="80" height="40" rx="4"
						style="fill:#34495e;stroke:#7f8c8d;stroke-width:2px;stroke-dasharray:3,3" /><text
						transform="translate(215.54 165)"
						style="isolation:isolate;font-size:10px;fill:#7f8c8d;font-family:ArialMT, Arial">Other
						Data</text>
					<rect x="310" y="140" width="120" height="40" rx="4" style="fill:#9b59b6" /><text
						transform="translate(327.66 165)"
						style="isolation:isolate;font-size:12px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">Layer
						1 Output</text><text transform="translate(332.76 190)"
						style="isolation:isolate;font-size:10px;fill:#95a5a6;font-family:ArialMT, Arial">Memory:
						0x5000</text>
					<rect x="460" y="140" width="80" height="40" rx="4"
						style="fill:#34495e;stroke:#7f8c8d;stroke-width:2px;stroke-dasharray:3,3" /><text
						transform="translate(475.54 165)"
						style="isolation:isolate;font-size:10px;fill:#7f8c8d;font-family:ArialMT, Arial">Other
						Data</text>
					<rect x="570" y="140" width="120" height="40" rx="4" style="fill:#e74c3c" /><text
						transform="translate(587.66 165)"
						style="isolation:isolate;font-size:12px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">Layer
						2 Output</text><text transform="translate(592.76 190)"
						style="isolation:isolate;font-size:10px;fill:#95a5a6;font-family:ArialMT, Arial">Memory:
						0x8000</text><text transform="translate(266.86 220)"
						style="isolation:isolate;font-size:16px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">Processing
						Sequence</text>
					<rect x="50" y="240" width="600" height="60" rx="6"
						style="fill:#2c3e50;stroke:#3498db;stroke-width:2px" /><text transform="translate(60 260)"
						style="isolation:isolate;font-size:14px;fill:#3498db;font-family:Arial-BoldMT, Arial;font-weight:700">Step
						1: Process Layer 0</text><text transform="translate(70 280)"
						style="isolation:isolate;font-size:12px;fill:#fff;font-family:ArialMT, Arial">✅ Cache loads
						Layer 0 output (0x1000)</text><text transform="translate(70 295)"
						style="isolation:isolate;font-size:12px;fill:#26de81;font-family:ArialMT, Arial">✅ Cache hit:
						Processing Layer 0 data</text>
					<rect x="50" y="310" width="600" height="80" rx="6"
						style="fill:#2c3e50;stroke:#f39c12;stroke-width:2px" /><text transform="translate(60 330)"
						style="isolation:isolate;font-size:14px;fill:#f39c12;font-family:Arial-BoldMT, Arial;font-weight:700">Step
						2: Layer 1 needs Layer 0 output as input</text><text transform="translate(70 350)"
						style="isolation:isolate;font-size:12px;fill:#fff;font-family:ArialMT, Arial">❌ Layer 1
						processing space at di<tspan x="175.1" y="0" style="letter-spacing:-0.01806640625em">f</tspan>
						<tspan x="178.21" y="0">ferent memory location</tspan>
					</text><text transform="translate(70 365)"
						style="isolation:isolate;font-size:12px;fill:#e74c3c;font-family:ArialMT, Arial">❌ Cache miss:
						Must load from 0x5000 (evicts Layer 0 data!)</text>
					<rect x="50" y="400" width="600" height="80" rx="6"
						style="fill:#2c3e50;stroke:#e74c3c;stroke-width:2px" /><text transform="translate(60 420)"
						style="isolation:isolate;font-size:14px;fill:#e74c3c;font-family:Arial-BoldMT, Arial;font-weight:700">Step
						3: Layer 2 needs Layer 1 output</text><text transform="translate(70 440)"
						style="isolation:isolate;font-size:12px;fill:#fff;font-family:ArialMT, Arial">❌ Must reload
						Layer 1 output from memory (0x5000)</text><text transform="translate(70 455)"
						style="isolation:isolate;font-size:12px;fill:#e74c3c;font-family:ArialMT, Arial">❌ Cache miss
						again: Load from 0x8000</text>
					<rect x="50" y="490" width="600" height="60" rx="6"
						style="stroke:#c0392b;stroke-width:3px;fill:url(#f289b82c-6e54-4286-b7bc-4c80e9a2bf33)" /><text
						transform="translate(260.65 515)"
						style="isolation:isolate;font-size:16px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">Cache
						State: Thrashing</text><text transform="translate(195.15 535)"
						style="isolation:isolate;font-size:14px;fill:#fff;font-family:ArialMT, Arial">60-80% miss rate:
						Data needed is always evicted!</text><text transform="translate(834.79 110)"
						style="isolation:isolate;font-size:20px;fill:#2ed573;font-family:Arial-BoldMT, Arial;font-weight:700">Contiguous
						<tspan x="111.08" y="0" style="letter-spacing:-0.037109375em"> </tspan>
						<tspan x="115.9" y="0">Activations: Perfect Cache Chain</tspan>
					</text>
					<rect x="750" y="140" width="600" height="40" rx="4"
						style="stroke:#2ed573;stroke-width:3px;fill:url(#e6b7c308-0263-4b1d-920c-6bf74c58a957)" /><text
						transform="translate(800 155)"
						style="isolation:isolate;font-size:10px;fill:#fff;font-family:ArialMT, Arial">L0 In</text><text
						transform="translate(850 155)"
						style="isolation:isolate;font-size:10px;fill:#fff;font-family:ArialMT, Arial">L0 Out</text><text
						transform="translate(900 155)"
						style="isolation:isolate;font-size:10px;fill:#fff;font-family:ArialMT, Arial">L1 In</text><text
						transform="translate(950 155)"
						style="isolation:isolate;font-size:10px;fill:#fff;font-family:ArialMT, Arial">L1 Out</text><text
						transform="translate(1000 155)"
						style="isolation:isolate;font-size:10px;fill:#fff;font-family:ArialMT, Arial">L2 In</text><text
						transform="translate(1050 155)"
						style="isolation:isolate;font-size:10px;fill:#fff;font-family:ArialMT, Arial">L2 Out</text><text
						transform="translate(1100 155)"
						style="isolation:isolate;font-size:10px;fill:#fff;font-family:ArialMT, Arial">L3 In</text><text
						transform="translate(1150 155)"
						style="isolation:isolate;font-size:10px;fill:#fff;font-family:ArialMT, Arial">L3 Out</text><text
						transform="translate(1200 155)"
						style="isolation:isolate;font-size:10px;fill:#fff;font-family:ArialMT, Arial">Final</text><text
						transform="translate(945.09 192)"
						style="isolation:isolate;font-size:10px;fill:#95a5a6;font-family:ArialMT, Arial">Memory: 0x1000
						→ 0x1000 + size (contiguous)</text><text transform="translate(966.86 220)"
						style="isolation:isolate;font-size:16px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">Processing
						Sequence</text>
					<rect x="750" y="240" width="600" height="60" rx="6"
						style="fill:#2c3e50;stroke:#2ed573;stroke-width:2px" /><text transform="translate(760 260)"
						style="isolation:isolate;font-size:14px;fill:#2ed573;font-family:Arial-BoldMT, Arial;font-weight:700">Step
						1: Process Layer 0</text><text transform="translate(770 280)"
						style="isolation:isolate;font-size:12px;fill:#fff;font-family:ArialMT, Arial">✅ Cache loads
						Layer 0 data from o<tspan x="186.43" y="0" style="letter-spacing:-0.01806640625em">f</tspan>
						<tspan x="189.55" y="0">fset 0</tspan>
					</text><text transform="translate(770 295)"
						style="isolation:isolate;font-size:12px;fill:#26de81;font-family:ArialMT, Arial">✅ Prefetcher
						detects pattern, loads Layer 1 input (next 64 bytes)</text>
					<rect x="750" y="310" width="600" height="80" rx="6"
						style="fill:#2c3e50;stroke:#26de81;stroke-width:2px" /><text transform="translate(760 330)"
						style="isolation:isolate;font-size:14px;fill:#26de81;font-family:Arial-BoldMT, Arial;font-weight:700">Step
						2: Layer 1 processes Layer 0 output</text><text transform="translate(770 350)"
						style="isolation:isolate;font-size:12px;fill:#fff;font-family:ArialMT, Arial">✅ Layer 0 output →
						Layer 1 input (sequential, cache hit!)</text><text transform="translate(770 365)"
						style="isolation:isolate;font-size:12px;fill:#26de81;font-family:ArialMT, Arial">✅ Layer 1
						output written to next sequential location</text>
					<rect x="750" y="400" width="600" height="80" rx="6"
						style="fill:#2c3e50;stroke:#2ed573;stroke-width:2px" /><text transform="translate(760 420)"
						style="isolation:isolate;font-size:14px;fill:#2ed573;font-family:Arial-BoldMT, Arial;font-weight:700">Step
						3: Every subsequent layer</text><text transform="translate(770 440)"
						style="isolation:isolate;font-size:12px;fill:#fff;font-family:ArialMT, Arial">✅ Layer N output →
						Layer N+1 input (perfect locality)</text><text transform="translate(770 455)"
						style="isolation:isolate;font-size:12px;fill:#26de81;font-family:ArialMT, Arial">✅ Cache
						prefetcher stays ahead of computation</text>
					<rect x="750" y="490" width="600" height="60" rx="6"
						style="stroke:#229954;stroke-width:3px;fill:url(#e6b7c308-0263-4b1d-920c-6bf74c58a957)" /><text
						transform="translate(969.54 515)"
						style="isolation:isolate;font-size:16px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">Cache
						State: Optimal</text><text transform="translate(895.33 535)"
						style="isolation:isolate;font-size:14px;fill:#fff;font-family:ArialMT, Arial">95%+ hit rate:
						Data flows perfectly through cache!</text>
				</svg>
			</section>
			<section style="transform: scale(1.3);">
				<aside class="notes">
					<p><strong>(CUE):</strong> <em>Transition from the previous slide, which introduced the concept of
							the inter-layer cache chain.</em></p>

					<p><strong>YOU:</strong> "So let's put some concrete numbers and access patterns to that concept.
						The core insight is simple: for maximum performance, the location where a layer writes its
						output must be the same location where the next layer reads its input."</p>

					<p><strong>(CUE):</strong> <em>Point to the header insight</em></p>

					<p><strong>YOU:</strong> "This slide crystallizes that principle: 'Output Location = Next Input
						Location.' In fragmented memory, each layer's output is in the 'wrong place' for the next layer.
						In contiguous memory, it's exactly where the next layer expects it."</p>

					<p><strong>(CUE):</strong> <em>Point to the left side - Fragmented Memory Access</em></p>

					<p><strong>YOU:</strong> "Here's the step-by-step breakdown of what goes wrong: Layer 0 reads
						weights from 0x1000, writes output to 0x3000. Layer 1 needs that data as input - cache miss! Has
						to load from 0x3000. Then Layer 1 needs its weights from 0x6000 - another cache miss! Every
						operation fights the cache system."</p>

					<p><strong>(CUE):</strong> <em>Point to the cache efficiency number on the left</em></p>

					<p><strong>YOU:</strong> "Result: 20-40% cache efficiency with constant misses. The CPU spends more
						time waiting for memory than computing."</p>

					<p><strong>(CUE):</strong> <em>Point to the right side - Contiguous Memory Access</em></p>

					<p><strong>YOU:</strong> "Compare that to the contiguous approach: Layer 0 reads weights from
						offset+0, writes output to offset+X. Layer 1 reads input from that same offset+X - cache hit!
						The data is already hot in cache. Sequential access pattern means 95%+ cache efficiency."</p>

					<p><strong>(CUE):</strong> <em>Point to the Real-World Performance Impact</em></p>

					<p><strong>YOU:</strong> "And here are the concrete cycle counts: Fragmented approach means each
						layer stalls 200+ cycles waiting for main memory. Contiguous approach flows at 1-3 cycles per
						access from L1 cache. That's a 100x difference in memory access speed."</p>

					<p><strong>(CUE):</strong> <em>Conclude with impact</em></p>

					<p><strong>YOU:</strong> "This transforms the CPU from a constantly stalled processor into one
						that's actually computing. This is how we make CPUs competitive with GPUs."</p>
				</aside>
				<svg id="afae237a-a74d-445a-9c12-bf463d1b8971" data-name="Layer 1" xmlns="http://www.w3.org/2000/svg"
					viewBox="0 0 1400 551.49">
					<rect x="16.55" y="14.84" width="1371.97" height="114.33" rx="13.72"
						style="fill:#34495e;stroke:#f39c12;stroke-width:3px" /><text transform="translate(338.91 49.14)"
						style="isolation:isolate;font-size:22.86620330810547px;fill:#f39c12;font-family:Arial-BoldMT, Arial;font-weight:700">The
						Memory Layout Insight: Output Location = Next Input Location</text><text
						transform="translate(364.86 77.73)"
						style="isolation:isolate;font-size:18.2929630279541px;fill:#fff;font-family:ArialMT, Arial">In
						fragmented memory: Each layer&apos;s output is in the &quot;wrong place&quot; for the next
						layer</text><text transform="translate(336.79 100.59)"
						style="isolation:isolate;font-size:18.2929630279541px;fill:#fff;font-family:ArialMT, Arial">In
						contiguous memory: Each layer&apos;s output is exactly where the next layer expects its
						input</text><text transform="translate(628.6 165.97)"
						style="isolation:isolate;font-size:21.343000411987305px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">Memory
						<tspan x="81.84" y="0" style="letter-spacing:-0.037107818592140275em"> </tspan>
						<tspan x="86.98" y="0">Access Pattern</tspan>
						<tspan x="241.2" y="0" style="letter-spacing:-0.037107818592140275em"> </tspan>
						<tspan x="246.34" y="0">Analysis</tspan>
					</text>
					<rect x="24.95" y="189.69" width="652.15" height="189.72" rx="9.49"
						style="fill:#2c3e50;stroke:#e74c3c;stroke-width:2px" /><text transform="translate(36.8 219.33)"
						style="isolation:isolate;font-size:18.971555709838867px;fill:#e74c3c;font-family:Arial-BoldMT, Arial;font-weight:700">Fragmented
						Memory<tspan x="186.59" y="0" style="letter-spacing:-0.037113538460888836em"> </tspan>
						<tspan x="191.16" y="0">Access:</tspan>
					</text><text transform="translate(48.66 248.97)"
						style="isolation:isolate;font-size:15.414388656616211px;fill:#fff;font-family:ArialMT, Arial">Layer
						0: Read weights(0x1000) → <tspan x="238.19" y="0"
							style="letter-spacing:-0.018087554424698442em">W</tspan>
						<tspan x="252.46" y="0">rite output(0x3000)</tspan>
					</text><text transform="translate(48.66 272.69)"
						style="isolation:isolate;font-size:15.414388656616211px;fill:#fff;font-family:ArialMT, Arial">Layer
						1: Read input(0x3000) ❌ CACHE MISS!</text><text transform="translate(48.66 296.4)"
						style="isolation:isolate;font-size:15.414388656616211px;fill:#fff;font-family:ArialMT, Arial">Read
						weights(0x6000) ❌ CACHE MISS!</text><text transform="translate(48.66 320.11)"
						style="isolation:isolate;font-size:15.414388656616211px;fill:#fff;font-family:ArialMT, Arial;letter-spacing:-0.018055877446721737em">W
						<tspan x="14.27" y="0" style="letter-spacing:-0.0000316769779767048em">rite output(0x9000)
						</tspan>
					</text><text transform="translate(48.66 343.83)"
						style="isolation:isolate;font-size:15.414388656616211px;fill:#fff;font-family:ArialMT, Arial">Layer
						2: Read input(0x9000) ❌ CACHE MISS!</text><text transform="translate(48.66 361.61)"
						style="isolation:isolate;font-size:15.414388656616211px;fill:#ff4757;font-family:Arial-BoldMT, Arial;font-weight:700">Cache
						efficiency: 20-40% (constant misses)</text>
					<rect x="736.38" y="189.69" width="652.15" height="189.72" rx="9.49"
						style="fill:#2c3e50;stroke:#2ed573;stroke-width:2px" /><text
						transform="translate(748.24 219.33)"
						style="isolation:isolate;font-size:18.971555709838867px;fill:#2ed573;font-family:Arial-BoldMT, Arial;font-weight:700">Contiguous
						Memory<tspan x="183.39" y="0" style="letter-spacing:-0.037113538460888836em"> </tspan>
						<tspan x="187.96" y="0" style="letter-spacing:0.00005147508801787633em">Access:</tspan>
					</text><text transform="translate(760.09 248.97)"
						style="isolation:isolate;font-size:15.414388656616211px;fill:#fff;font-family:ArialMT, Arial">Layer
						0: Read weights(o<tspan x="167.08" y="0" style="letter-spacing:-0.018055877446721737em">f
						</tspan>
						<tspan x="171.09" y="0">fset+0) → </tspan>
						<tspan x="242.62" y="0" style="letter-spacing:-0.018055877446721737em">W</tspan>
						<tspan x="256.89" y="0">rite output(o</tspan>
						<tspan x="339.15" y="0" style="letter-spacing:-0.018055877446721737em">f</tspan>
						<tspan x="343.15" y="0">fset+X)</tspan>
					</text><text transform="translate(760.09 272.69)"
						style="isolation:isolate;font-size:15.414388656616211px;fill:#fff;font-family:ArialMT, Arial">Layer
						1: Read input(o<tspan x="148.24" y="0" style="letter-spacing:-0.018055877446721737em">f</tspan>
						<tspan x="152.25" y="0">fset+X) ✅ CACHE HIT!</tspan>
					</text><text transform="translate(760.09 296.4)"
						style="isolation:isolate;font-size:15.414388656616211px;fill:#fff;font-family:ArialMT, Arial">Read
						weights(o<tspan x="107.1" y="0" style="letter-spacing:-0.018055877446721737em">f</tspan>
						<tspan x="111.11" y="0" style="letter-spacing:0.0000633539559534096em">fset+Y) ✅ CACHE HIT!
						</tspan>
					</text><text transform="translate(760.09 320.11)"
						style="isolation:isolate;font-size:15.414388656616211px;fill:#fff;font-family:ArialMT, Arial">
						<tspan style="letter-spacing:-0.018055877446721737em">W</tspan>
						<tspan x="14.27" y="0">rite output(o</tspan>
						<tspan x="96.53" y="0" style="letter-spacing:-0.018055877446721737em">f</tspan>
						<tspan x="100.53" y="0">fset+Z)</tspan>
					</text><text transform="translate(760.09 343.83)"
						style="isolation:isolate;font-size:15.414388656616211px;fill:#fff;font-family:ArialMT, Arial">Layer
						2: Read input(o<tspan x="148.24" y="0" style="letter-spacing:-0.018055877446721737em">f</tspan>
						<tspan x="152.25" y="0">fset+Z) ✅ CACHE HIT!</tspan>
					</text><text transform="translate(760.09 361.61)"
						style="isolation:isolate;font-size:15.414388656616211px;fill:#26de81;font-family:Arial-BoldMT, Arial;font-weight:700">Cache
						efficiency: 95%+ (perfect flow)</text>
					<rect x="16.55" y="403.57" width="1371.97" height="137.2" rx="11.43"
						style="fill:#2c3e50;stroke:#3498db;stroke-width:2px" /><text
						transform="translate(546.63 437.87)"
						style="isolation:isolate;font-size:20.5795841217041px;fill:#3498db;font-family:Arial-BoldMT, Arial;font-weight:700">Real-
						<tspan x="50.32" y="0" style="letter-spacing:-0.018055857156905025em">W</tspan>
						<tspan x="69.38" y="0">orld Performance Impact</tspan>
					</text>
					<g class="fragment" class="fragment" style="isolation:isolate"><text
							transform="translate(377.48 466.45)"
							style="isolation:isolate;font-size:18.2929630279541px;fill:#e74c3c;font-family:Arial-BoldMT, Arial;font-weight:700">Fragmented:</text><text
							transform="translate(488.27 466.45)"
							style="font-size:18.2929630279541px;fill:#fff;font-family:ArialMT, Arial">
							<tspan xml:space="preserve"> Each layer stalls waiting for data from main memory (200+
								cycles)</tspan>
						</text></g>
					<g class="fragment" class="fragment" style="isolation:isolate"><text
							transform="translate(398.6 489.32)"
							style="isolation:isolate;font-size:18.2929630279541px;fill:#2ed573;font-family:Arial-BoldMT, Arial;font-weight:700">Contiguous:</text><text
							transform="translate(506.3 489.32)"
							style="font-size:18.2929630279541px;fill:#fff;font-family:ArialMT, Arial">
							<tspan xml:space="preserve"> Data flows seamlessly from L1 cache (1-3 cycles per access)
							</tspan>
						</text></g>
				</svg>
			</section>
			<section style="transform: scale(1.3);">
				<aside class="notes">
					<p><strong>(CUE):</strong> <em>Transition from the previous slide on memory access patterns.</em>
					</p>

					<p><strong>YOU:</strong> "So far, we've focused on the data cache. But there's another,
						often-forgotten hardware cache that our sequential layout optimizes, and it acts as a hidden
						performance multiplier. I'm talking about the TLB, or Translation Lookaside Buffer."</p>

					<p><strong>YOU:</strong> "The TLB's job is simple: it's a small, fast cache that stores the
						translations from the virtual memory addresses your program uses to the actual physical
						addresses in RAM. A TLB hit is super fast—1 to 3 cycles. But a TLB miss is a performance
						disaster. The CPU has to stall for 100 to 300 cycles to manually look up the address in the main
						memory's page tables."</p>

					<p><strong>(CUE):</strong> <em>Point to the orange subtitle</em></p>

					<p><strong>YOU:</strong> "And here's the kicker: traditional memory layouts cause constant TLB
						misses, creating massive hidden overhead that most developers never even measure."</p>

					<p><strong>(CUE):</strong> <em>Point to the left side - Fragmented Memory</em></p>

					<p><strong>YOU:</strong> "Look at this fragmented layout: Weights at virtual address 0x10000000,
						Layer 0 activations way over at 0x50000000, Layer 1 at 0xA0000000. Each allocation uses
						completely different virtual memory pages scattered across the address space."</p>

					<p><strong>(CUE):</strong> <em>Point to the TLB state on the left</em></p>

					<p><strong>YOU:</strong> "The TLB only holds 64 entries typically, and they get filled with
						translations for all these scattered locations. When we access weights - TLB hit. But then Layer
						0 activations from a different virtual address - TLB miss! 200+ cycles of stalling for page
						table walks."</p>

					<p><strong>(CUE):</strong> <em>Point to the red result on the left</em></p>

					<p><strong>YOU:</strong> "40-60% TLB miss rate creates massive hidden overhead that's invisible to
						most profiling tools but absolutely destroys performance."</p>

					<p><strong>(CUE):</strong> <em>Point to the right side - Contiguous Memory</em></p>

					<p><strong>YOU:</strong> "Now contrast with our contiguous approach: Everything in one sequential
						block from 0x10000000 onwards. The TLB entries become sequential pages, the hardware prefetcher
						can predict the pattern, and we get 95%+ hit rate with near-zero translation overhead."</p>

					<p><strong>(CUE):</strong> <em>Conclude with the multiplier effect</em></p>

					<p><strong>YOU:</strong> "This TLB optimization alone can give you a 10x performance improvement,
						and it stacks with all the cache optimizations. It's a true performance multiplier that most CPU
						developers completely ignore."</p>
				</aside>
				<svg id="b77c4322-b78d-47af-88df-b39fbbbd5f28" data-name="Layer 1" xmlns="http://www.w3.org/2000/svg"
					xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 1500 659.45">
					<defs>
						<pattern id="b0443671-e352-4a40-9482-ae18e75ce3b4" data-name="tlbMissPattern" width="8"
							height="8" patternTransform="matrix(1, 0, 0, -1, 1, 1078)" patternUnits="userSpaceOnUse"
							viewBox="0 0 8 8">
							<rect width="8" height="8" style="fill:none" />
							<rect width="8" height="8" style="fill:#e74c3c" />
							<path d="M0,8,8,0M0,0,8,8" style="stroke:#c0392b" />
						</pattern>
						<pattern id="b27b2905-b6f7-49db-b1d0-f2cd18483031" data-name="tlbHitPattern" width="8"
							height="8" patternTransform="matrix(1, 0, 0, -1, 1, 1078)" patternUnits="userSpaceOnUse"
							viewBox="0 0 8 8">
							<rect width="8" height="8" style="fill:none" />
							<rect width="8" height="8" style="fill:#2ed573" />
							<circle cx="4" cy="4" r="2" style="fill:#26de81" />
						</pattern>
					</defs><text transform="translate(393.76 40)"
						style="isolation:isolate;font-size:28px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">TLB
						Optimization: The Hidden Performance Multiplier</text><text transform="translate(531.68 65)"
						style="isolation:isolate;font-size:16px;fill:#f39c12;font-family:ArialMT, Arial">How Unified
						Memory Layout Eliminates<tspan x="279.22" y="0" style="letter-spacing:-0.01806640625em">
						</tspan>
						<tspan x="283.38" y="0" style="letter-spacing:-0.037109375em">T</tspan>
						<tspan x="292.55" y="0">ranslation Overhead</tspan>
					</text>
					<rect x="50" y="90" width="1400" height="100" rx="10"
						style="fill:#34495e;stroke:#3498db;stroke-width:2px" /><text transform="translate(424.97 120)"
						style="isolation:isolate;font-size:18px;fill:#3498db;font-family:Arial-BoldMT, Arial;font-weight:700">TLB
						(<tspan x="45.98" y="0" style="letter-spacing:-0.05517578125em">T</tspan>
						<tspan x="55.99" y="0">ranslation Lookaside Buffer): The Often-Forgotten Performance Killer
						</tspan>
					</text>
					<g class="fragment" class="fragment" style="isolation:isolate"><text transform="translate(70 145)"
							style="isolation:isolate;font-size:14px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">What
							is TLB?</text><text transform="translate(159.43 145)"
							style="font-size:14px;fill:#fff;font-family:ArialMT, Arial">
							<tspan xml:space="preserve"> Hardware cache that stores virtual → physical address
								translations</tspan>
						</text></g>
					<g class="fragment" class="fragment" style="isolation:isolate"><text transform="translate(70 165)"
							style="isolation:isolate;font-size:14px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">TLB
							Miss Cost:</text><text transform="translate(171.89 165)"
							style="font-size:14px;fill:#fff;font-family:ArialMT, Arial">
							<tspan xml:space="preserve"> 100-300 cycles to walk page tables (vs 1-3 cycles for</tspan>
							<tspan x="331.49" y="0" style="letter-spacing:-0.01806640625em"> </tspan>
							<tspan x="335.13" y="0">TLB hit)</tspan>
						</text></g><text transform="translate(70 185)"
						style="isolation:isolate;font-size:14px;fill:#f39c12;font-family:Arial-BoldMT, Arial;font-weight:700">Problem:
						<tspan x="64.57" y="0" style="letter-spacing:-0.05517578125em">T</tspan>
						<tspan x="72.34" y="0">raditional memory layouts cause constant TLB misses → Hidden performance
							killer!</tspan>
					</text><text transform="translate(139.99 220)"
						style="isolation:isolate;font-size:20px;fill:#e74c3c;font-family:Arial-BoldMT, Arial;font-weight:700">
						<tspan style="letter-spacing:-0.05517578125em">T</tspan>
						<tspan x="11.11" y="0">raditional: Fragmented Memory (TLB Thrashing)</tspan>
					</text><text transform="translate(50 250)"
						style="isolation:isolate;font-size:14px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">
						<tspan style="letter-spacing:-0.01806640625em">V</tspan>
						<tspan x="9.08" y="0">irtual Memory Layout:</tspan>
					</text>
					<rect x="50" y="270" width="100" height="30" rx="4" style="fill:#3498db" /><text
						transform="translate(80.23 290)"
						style="isolation:isolate;font-size:11px;fill:#fff;font-family:ArialMT, Arial">
						<tspan style="letter-spacing:-0.01806640625em">W</tspan>
						<tspan x="10.18" y="0">eights</tspan>
					</text><text transform="translate(67.06 312)"
						style="isolation:isolate;font-size:9px;fill:#95a5a6;font-family:ArialMT, Arial">
						<tspan style="letter-spacing:-0.07421875em">V</tspan>
						<tspan x="5.33" y="0">A: 0x10000000</tspan>
					</text>
					<rect x="170" y="270" width="60" height="30" rx="4"
						style="fill:#34495e;stroke:#7f8c8d;stroke-dasharray:3,3" /><text
						transform="translate(191.49 290)"
						style="isolation:isolate;font-size:9px;fill:#7f8c8d;font-family:ArialMT, Arial">Gap</text>
					<rect x="250" y="270" width="100" height="30" rx="4" style="fill:#e74c3c" /><text
						transform="translate(269.73 290)"
						style="isolation:isolate;font-size:11px;fill:#fff;font-family:ArialMT, Arial">Layer 0<tspan
							x="36.69" y="0" style="letter-spacing:-0.05517578125em"> </tspan>
						<tspan x="39.14" y="0">Acts</tspan>
					</text><text transform="translate(267.06 312)"
						style="isolation:isolate;font-size:9px;fill:#95a5a6;font-family:ArialMT, Arial">
						<tspan style="letter-spacing:-0.07421875em">V</tspan>
						<tspan x="5.33" y="0">A: 0x50000000</tspan>
					</text>
					<rect x="370" y="270" width="60" height="30" rx="4"
						style="fill:#34495e;stroke:#7f8c8d;stroke-dasharray:3,3" /><text
						transform="translate(391.49 290)"
						style="isolation:isolate;font-size:9px;fill:#7f8c8d;font-family:ArialMT, Arial">Gap</text>
					<rect x="450" y="270" width="100" height="30" rx="4" style="fill:#9b59b6" /><text
						transform="translate(469.73 290)"
						style="isolation:isolate;font-size:11px;fill:#fff;font-family:ArialMT, Arial">Layer 1<tspan
							x="36.69" y="0" style="letter-spacing:-0.05517578125em"> </tspan>
						<tspan x="39.14" y="0">Acts</tspan>
					</text><text transform="translate(466.56 312)"
						style="isolation:isolate;font-size:9px;fill:#95a5a6;font-family:ArialMT, Arial">
						<tspan style="letter-spacing:-0.07421875em">V</tspan>
						<tspan x="5.33" y="0">A: 0xA0000000</tspan>
					</text>
					<rect x="570" y="270" width="60" height="30" rx="4"
						style="fill:#34495e;stroke:#7f8c8d;stroke-dasharray:3,3" /><text
						transform="translate(591.49 290)"
						style="isolation:isolate;font-size:9px;fill:#7f8c8d;font-family:ArialMT, Arial">Gap</text>
					<rect x="650" y="270" width="100" height="30" rx="4" style="fill:#f39c12" /><text
						transform="translate(675.55 290)"
						style="isolation:isolate;font-size:11px;fill:#fff;font-family:ArialMT, Arial">More<tspan
							x="25.06" y="0" style="letter-spacing:-0.05517578125em"> </tspan>
						<tspan x="27.51" y="0">Acts</tspan>
					</text><text transform="translate(666.81 312)"
						style="isolation:isolate;font-size:9px;fill:#95a5a6;font-family:ArialMT, Arial">
						<tspan style="letter-spacing:-0.07421875em">V</tspan>
						<tspan x="5.33" y="0">A: 0xF0000000</tspan>
					</text><text transform="translate(50 330)"
						style="isolation:isolate;font-size:14px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">TLB
						State (64 entries typical):</text>
					<rect x="50" y="340" width="700" height="80" rx="6"
						style="fill:#2c2c54;stroke:#e74c3c;stroke-width:2px" /><text transform="translate(60 360)"
						style="isolation:isolate;font-size:12px;fill:#e74c3c;font-family:Arial-BoldMT, Arial;font-weight:700">TLB
						Entries (<tspan x="74.67" y="0" style="letter-spacing:-0.01806640625em">V</tspan>
						<tspan x="82.46" y="0">irtual → Physical):</tspan>
					</text><text transform="translate(70 380)"
						style="isolation:isolate;font-size:11px;fill:#fff;font-family:ArialMT, Arial">Entry 1:
						0x10000000 → 0x2A4F1000 (<tspan x="187.73" y="0" style="letter-spacing:-0.01806640625em">W
						</tspan>
						<tspan x="197.91" y="0">eights page 1)</tspan>
					</text><text transform="translate(70 395)"
						style="isolation:isolate;font-size:11px;fill:#fff;font-family:ArialMT, Arial">Entry 2:
						0x10001000 → 0x7B2A3000 (<tspan x="188.35" y="0" style="letter-spacing:-0.01806640625em">W
						</tspan>
						<tspan x="198.53" y="0">eights page 2)</tspan>
					</text><text transform="translate(70 410)"
						style="isolation:isolate;font-size:11px;fill:#ff4757;font-family:ArialMT, Arial">... (60 more
						entries for scattered allocations)</text><text transform="translate(400 380)"
						style="isolation:isolate;font-size:11px;fill:#ff4757;font-family:ArialMT, Arial">Entry 62:
						0x50000000 → 0x1C5D2000 (Layer 0 acts)</text><text transform="translate(400 395)"
						style="isolation:isolate;font-size:11px;fill:#ff4757;font-family:ArialMT, Arial">Entry 63:
						0xA0000000 → 0x8E1F4000 (Layer 1 acts)</text><text transform="translate(400 410)"
						style="isolation:isolate;font-size:11px;fill:#ff4757;font-family:ArialMT, Arial">Entry 64:
						0xF0000000 → 0x9A7B6000 (More acts)</text><text transform="translate(50 440)"
						style="isolation:isolate;font-size:14px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">Processing
						Sequence:</text>
					<path
						d="M56,450H744c3.31,0,6,3,6,6.73v98.71c0,3.72-2.69,6.73-6,6.73H56c-3.31,0-6-3-6-6.73V456.73C50,453,52.69,450,56,450Z"
						style="fill:#2c2c54;stroke:#e74c3c;stroke-width:2px" /><text transform="translate(60 470)"
						style="isolation:isolate;font-size:13px;fill:#3498db;font-family:Arial-BoldMT, Arial;font-weight:700">1.
						<tspan x="10.84" y="0" style="letter-spacing:-0.037109375em"> </tspan>
						<tspan x="13.97" y="0">Access weights at 0x10000000:</tspan>
					</text><text transform="translate(70 485)"
						style="isolation:isolate;font-size:12px;fill:#26de81;font-family:ArialMT, Arial">✅<tspan x="9"
							y="0" style="letter-spacing:-0.01806640625em"> </tspan>
						<tspan x="12.12" y="0">TLB hit (entry 1) → Fast access</tspan>
					</text><text transform="translate(60 505)"
						style="isolation:isolate;font-size:13px;fill:#e74c3c;font-family:Arial-BoldMT, Arial;font-weight:700">2.
						<tspan x="10.84" y="0" style="letter-spacing:-0.037109375em"> </tspan>
						<tspan x="13.97" y="0">Access Layer 0 activations at 0x50000000:</tspan>
					</text><text transform="translate(70 520)"
						style="isolation:isolate;font-size:12px;fill:#ff4757;font-family:ArialMT, Arial">❌<tspan x="9"
							y="0" style="letter-spacing:-0.01806640625em"> </tspan>
						<tspan x="12.12" y="0">TLB miss! Must evict old entries, walk page tables (200+ cycles)</tspan>
					</text><text transform="translate(60 535)"
						style="isolation:isolate;font-size:13px;fill:#9b59b6;font-family:Arial-BoldMT, Arial;font-weight:700">3.
						<tspan x="10.84" y="0" style="letter-spacing:-0.037109375em"> </tspan>
						<tspan x="13.97" y="0">Access Layer 1 activations at 0xA0000000:</tspan>
					</text><text transform="translate(70 550)"
						style="isolation:isolate;font-size:12px;fill:#ff4757;font-family:ArialMT, Arial">❌<tspan x="9"
							y="0" style="letter-spacing:-0.05517578125em"> </tspan>
						<tspan x="11.67" y="0">Another</tspan>
						<tspan x="53.7" y="0" style="letter-spacing:-0.01806640625em"> </tspan>
						<tspan x="56.82" y="0">TLB miss! More page table walks...</tspan>
					</text>
					<rect x="50" y="580" width="700" height="60" rx="6"
						style="stroke:#c0392b;stroke-width:3px;fill:url(#b0443671-e352-4a40-9482-ae18e75ce3b4)" /><text
						transform="translate(199.94 605)"
						style="isolation:isolate;font-size:16px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">TLB
						Miss Rate: 40-60% → Massive hidden overhead!</text><text transform="translate(230.36 625)"
						style="isolation:isolate;font-size:14px;fill:#fff;font-family:ArialMT, Arial">Each miss = 200+
						cycles of stalling for page table walk</text><text transform="translate(914.99 220)"
						style="isolation:isolate;font-size:20px;fill:#2ed573;font-family:Arial-BoldMT, Arial;font-weight:700">Unified:
						Contiguous Memory (TLB Paradise)</text><text transform="translate(800 250)"
						style="isolation:isolate;font-size:14px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">
						<tspan style="letter-spacing:-0.01806640625em">V</tspan>
						<tspan x="9.08" y="0">irtual Memory Layout:</tspan>
					</text>
					<rect x="800" y="270" width="600" height="30" rx="4"
						style="stroke:#2ed573;stroke-width:3px;fill:url(#b27b2905-b6f7-49db-b1d0-f2cd18483031)" /><text
						transform="translate(850 285)"
						style="isolation:isolate;font-size:9px;fill:#fff;font-family:ArialMT, Arial">
						<tspan style="letter-spacing:-0.01806640625em">W</tspan>
						<tspan x="8.33" y="0">eights</tspan>
					</text><text transform="translate(920 285)"
						style="isolation:isolate;font-size:9px;fill:#fff;font-family:ArialMT, Arial">L0<tspan x="10.01"
							y="0" style="letter-spacing:-0.05517578125em"> </tspan>
						<tspan x="12.01" y="0">Acts</tspan>
					</text><text transform="translate(980 285)"
						style="isolation:isolate;font-size:9px;fill:#fff;font-family:ArialMT, Arial">L1<tspan x="10.01"
							y="0" style="letter-spacing:-0.05517578125em"> </tspan>
						<tspan x="12.01" y="0">Acts</tspan>
					</text><text transform="translate(1040 285)"
						style="isolation:isolate;font-size:9px;fill:#fff;font-family:ArialMT, Arial">L2<tspan x="10.01"
							y="0" style="letter-spacing:-0.05517578125em"> </tspan>
						<tspan x="12.01" y="0">Acts</tspan>
					</text><text transform="translate(1100 285)"
						style="isolation:isolate;font-size:9px;fill:#fff;font-family:ArialMT, Arial">L3<tspan x="10.01"
							y="0" style="letter-spacing:-0.05517578125em"> </tspan>
						<tspan x="12.01" y="0">Acts</tspan>
					</text><text transform="translate(1160 285)"
						style="isolation:isolate;font-size:9px;fill:#fff;font-family:ArialMT, Arial">Output</text><text
						transform="translate(1220 285)"
						style="isolation:isolate;font-size:9px;fill:#fff;font-family:ArialMT, Arial">
						<tspan style="letter-spacing:-0.11083984375em">T</tspan>
						<tspan x="4.5" y="0">emp</tspan>
					</text><text transform="translate(1280 285)"
						style="isolation:isolate;font-size:9px;fill:#fff;font-family:ArialMT, Arial">Grads</text><text
						transform="translate(1006.54 315)"
						style="isolation:isolate;font-size:9px;fill:#95a5a6;font-family:ArialMT, Arial">
						<tspan style="letter-spacing:-0.07421875em">V</tspan>
						<tspan x="5.33" y="0">A: 0x10000000 → 0x10FFFFFF (contiguous!)</tspan>
					</text><text transform="translate(800 330)"
						style="isolation:isolate;font-size:14px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">TLB
						State (Much simpler!):</text>
					<rect x="800" y="340" width="600" height="80" rx="6"
						style="fill:#2c2c54;stroke:#2ed573;stroke-width:2px" /><text transform="translate(810 360)"
						style="isolation:isolate;font-size:12px;fill:#2ed573;font-family:Arial-BoldMT, Arial;font-weight:700">TLB
						Entries (Sequential pages):</text><text transform="translate(820 380)"
						style="isolation:isolate;font-size:11px;fill:#fff;font-family:ArialMT, Arial">Entry 1:
						0x10000000 → 0x2A4F1000 (Page 1)</text><text transform="translate(820 395)"
						style="isolation:isolate;font-size:11px;fill:#fff;font-family:ArialMT, Arial">Entry 2:
						0x10001000 → 0x2A4F2000 (Page 2, sequential!)</text><text transform="translate(820 410)"
						style="isolation:isolate;font-size:11px;fill:#fff;font-family:ArialMT, Arial">Entry 3:
						0x10002000 → 0x2A4F3000 (Page 3, sequential!)</text><text transform="translate(1150 380)"
						style="isolation:isolate;font-size:11px;fill:#26de81;font-family:ArialMT, Arial">... (All pages
						in sequence)</text><text transform="translate(1150 395)"
						style="isolation:isolate;font-size:11px;fill:#26de81;font-family:ArialMT, Arial">Perfect<tspan
							x="34.85" y="0" style="letter-spacing:-0.01806640625em"> </tspan>
						<tspan x="37.71" y="0">TLB utilization!</tspan>
					</text><text transform="translate(1150 410)"
						style="isolation:isolate;font-size:11px;fill:#26de81;font-family:ArialMT, Arial">Hardware
						prefetcher helps</text><text transform="translate(800 440)"
						style="isolation:isolate;font-size:14px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">Processing
						Sequence:</text>
					<path
						d="M806,450h588c3.31,0,6,3,6,6.73v98.71c0,3.72-2.69,6.73-6,6.73H806c-3.31,0-6-3-6-6.73V456.73C800,453,802.69,450,806,450Z"
						style="fill:#2c2c54;stroke:#2ed573;stroke-width:2px" /><text transform="translate(810 470)"
						style="isolation:isolate;font-size:13px;fill:#3498db;font-family:Arial-BoldMT, Arial;font-weight:700">1.
						<tspan x="10.84" y="0" style="letter-spacing:-0.037109375em"> </tspan>
						<tspan x="13.97" y="0">Access weights at 0x10000000:</tspan>
					</text><text transform="translate(820 485)"
						style="isolation:isolate;font-size:12px;fill:#26de81;font-family:ArialMT, Arial">✅<tspan x="9"
							y="0" style="letter-spacing:-0.01806640625em"> </tspan>
						<tspan x="12.12" y="0">TLB hit → Fast access + prefetch loads next pages</tspan>
					</text><text transform="translate(810 505)"
						style="isolation:isolate;font-size:13px;fill:#e74c3c;font-family:Arial-BoldMT, Arial;font-weight:700">2.
						<tspan x="10.84" y="0" style="letter-spacing:-0.037109375em"> </tspan>
						<tspan x="13.97" y="0">Access Layer 0 activations at 0x10080000:</tspan>
					</text><text transform="translate(820 520)"
						style="isolation:isolate;font-size:12px;fill:#26de81;font-family:ArialMT, Arial">✅<tspan x="9"
							y="0" style="letter-spacing:-0.01806640625em"> </tspan>
						<tspan x="12.12" y="0">TLB hit! (Already loaded by prefetcher)</tspan>
					</text><text transform="translate(810 535)"
						style="isolation:isolate;font-size:13px;fill:#9b59b6;font-family:Arial-BoldMT, Arial;font-weight:700">3.
						<tspan x="10.84" y="0" style="letter-spacing:-0.037109375em"> </tspan>
						<tspan x="13.97" y="0">Access Layer 1 activations at 0x10100000:</tspan>
					</text><text transform="translate(820 550)"
						style="isolation:isolate;font-size:12px;fill:#26de81;font-family:ArialMT, Arial">✅<tspan x="9"
							y="0" style="letter-spacing:-0.01806640625em"> </tspan>
						<tspan x="12.12" y="0">TLB hit! Perfect sequential pattern</tspan>
					</text>
					<rect x="800" y="580" width="600" height="60" rx="6"
						style="stroke:#229954;stroke-width:3px;fill:url(#b27b2905-b6f7-49db-b1d0-f2cd18483031)" /><text
						transform="translate(893.51 605)"
						style="isolation:isolate;font-size:16px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">TLB
						Hit Rate: 95%+ → Near-zero translation overhead!</text><text transform="translate(973.55 625)"
						style="isolation:isolate;font-size:14px;fill:#fff;font-family:ArialMT, Arial">Page table walks
						become extremely rare</text>
				</svg>
			</section>
			<section style="transform: scale(1.3);">
				<aside class="notes">
					<p><strong>(CUE):</strong> <em>Transition from the previous slide showing TLB thrashing vs.
							optimization.</em></p>

					<p><strong>YOU:</strong> "Now let's dive into why TLB optimization creates such dramatic performance
						gains. This slide shows how all the hardware optimizations work together as a multiplier
						effect."</p>

					<p><strong>(CUE):</strong> <em>Point to the TLB Technical Details section</em></p>

					<p><strong>YOU:</strong> "First, the technical reality: L1 TLB typically holds 64-128 entries for
						4KB pages, L2 TLB holds 1024-2048 entries. When you miss, that's a 100-300 cycle penalty. Our
						approach maximizes TLB coverage, and with huge pages, we get even better utilization - 512x
						fewer TLB entries needed for the same memory space."</p>

					<p><strong>(CUE):</strong> <em>Point to the Performance Mathematics section</em></p>

					<p><strong>YOU:</strong> "Here's the math: Traditional approach with 50% TLB miss rate means 50%
						times 200 cycles equals 100 cycles average overhead per memory access. Our approach with 5% miss
						rate means only 10 cycles average overhead. That's a 10x reduction in TLB overhead alone!"</p>

					<p><strong>(CUE):</strong> <em>Emphasize the combined effect</em></p>

					<p><strong>YOU:</strong> "But here's where it gets really interesting: combined with cache gains,
						we're looking at 50-100x total speedup over naive implementations."</p>

					<p><strong>(CUE):</strong> <em>Point to the Hardware Synergy section</em></p>

					<p><strong>YOU:</strong> "The magic happens in hardware synergy: TLB prefetcher works with cache
						prefetcher, sequential pages create predictable patterns, NUMA-aware allocation helps TLB
						performance, and huge pages reduce TLB pressure. Every hardware optimization amplifies the
						others."</p>

					<p><strong>(CUE):</strong> <em>Point to the Implementation section</em></p>

					<p><strong>YOU:</strong> "Implementation is straightforward: use mmap with MAP_HUGETLB flag for 2MB
						pages instead of 4KB pages. Result: 512x fewer TLB entries needed, and perfect sequential access
						means the TLB prefetcher works optimally."</p>

					<p><strong>(CUE):</strong> <em>Point to the final orange box</em></p>

					<p><strong>YOU:</strong> "This brings us to the complete memory hierarchy optimization stack: Cache
						plus TLB plus Prefetch plus SIMD plus DSA - all working in perfect harmony. This is what happens
						when you design software that truly understands and exploits the underlying hardware
						architecture."</p>

					<p><strong>(CUE):</strong> <em>Conclude with the key insight</em></p>

					<p><strong>YOU:</strong> "This level of hardware-software co-design is exactly what GPU programmers
						do every day. It's time we brought this same rigor to CPU optimization."</p>
				</aside>

				<svg id="b4b4979b-3c94-485e-8f81-3bf638a36b5a" data-name="Layer 1" xmlns="http://www.w3.org/2000/svg"
					viewBox="0 0 1500 470.3">
					<rect x="11.77" y="19.77" width="1472.17" height="189.28" rx="12.62"
						style="fill:#1e1e1e;stroke:#f39c12;stroke-width:3px" /><text transform="translate(459.03 51.31)"
						style="isolation:isolate;font-size:21.031003952026367px;fill:#f39c12;font-family:Arial-BoldMT, Arial;font-weight:700">Why
						TLB Optimization Multiplies<tspan x="327.13" y="0"
							style="letter-spacing:-0.01806298992509094em"> </tspan>
						<tspan x="332.59" y="0" style="letter-spacing:-0.07420220539921676em">Y</tspan>
						<tspan x="345.06" y="0">our Performance Gains</tspan>
					</text><text transform="translate(32.8 82.86)"
						style="isolation:isolate;font-size:16.82480239868164px;fill:#3498db;font-family:Arial-BoldMT, Arial;font-weight:700">TLB
						<tspan x="37.38" y="0" style="letter-spacing:-0.0742370345816288em">T</tspan>
						<tspan x="46.41" y="0" style="letter-spacing:0.00002902151469180172em">echnical Details:</tspan>
					</text><text transform="translate(43.31 103.89)"
						style="isolation:isolate;font-size:13.670151710510254px;fill:#fff;font-family:ArialMT, Arial">•
						L1<tspan x="23.79" y="0" style="letter-spacing:-0.01803798790765549em"> </tspan>
						<tspan x="27.34" y="0">TLB: 64-128 entries (4KB pages)</tspan>
					</text><text transform="translate(43.31 119.66)"
						style="isolation:isolate;font-size:13.670151710510254px;fill:#fff;font-family:ArialMT, Arial">•
						L2<tspan x="23.79" y="0" style="letter-spacing:-0.01803798790765549em"> </tspan>
						<tspan x="27.34" y="0">TLB: 1024-2048 entries</tspan>
					</text><text transform="translate(43.31 135.44)"
						style="isolation:isolate;font-size:13.670151710510254px;fill:#fff;font-family:ArialMT, Arial">•
						Page size: 4KB (default) or 2MB (huge pages)</text><text transform="translate(43.31 151.21)"
						style="isolation:isolate;font-size:13.670151710510254px;fill:#fff;font-family:ArialMT, Arial">•
						Miss penalty: 100-300 cycles</text><text transform="translate(43.31 166.98)"
						style="isolation:isolate;font-size:13.670151710510254px;fill:#26de81;font-family:Arial-BoldMT, Arial;font-weight:700">•
						<tspan x="4.79" y="0" style="letter-spacing:-0.01803798790765549em"> </tspan>
						<tspan x="8.34" y="0" style="letter-spacing:-0.0741879225429712em">Y</tspan>
						<tspan x="16.44" y="0">our approach: Maximize TLB coverage</tspan>
					</text><text transform="translate(43.31 182.76)"
						style="isolation:isolate;font-size:13.670151710510254px;fill:#26de81;font-family:Arial-BoldMT, Arial;font-weight:700">•
						Huge pages: Even better TLB utilization!</text><text transform="translate(400.84 82.86)"
						style="isolation:isolate;font-size:16.82480239868164px;fill:#e74c3c;font-family:Arial-BoldMT, Arial;font-weight:700">Performance
						Mathematics:</text><text transform="translate(411.35 103.89)"
						style="isolation:isolate;font-size:13.670151710510254px;fill:#fff;font-family:ArialMT, Arial">
						<tspan style="letter-spacing:-0.03711182066545358em">T</tspan>
						<tspan x="7.84" y="0">raditional: 50%</tspan>
						<tspan x="98.27" y="0" style="letter-spacing:-0.01803798790765549em"> </tspan>
						<tspan x="101.83" y="0">TLB miss rate</tspan>
					</text><text transform="translate(411.35 119.66)"
						style="isolation:isolate;font-size:13.670151710510254px;fill:#fff;font-family:ArialMT, Arial">•
						50% × 200 cycles = 100 cycles avg overhead</text><text transform="translate(411.35 135.44)"
						style="isolation:isolate;font-size:13.670151710510254px;fill:#fff;font-family:ArialMT, Arial">
						<tspan style="letter-spacing:-0.09179728499539527em">Y</tspan>
						<tspan x="7.86" y="0" style="letter-spacing:0.00003571878793595147em">our approach: 5%</tspan>
						<tspan x="115.78" y="0" style="letter-spacing:-0.018073706695591444em"> </tspan>
						<tspan x="119.33" y="0">TLB miss rate</tspan>
					</text><text transform="translate(411.35 151.21)"
						style="isolation:isolate;font-size:13.670151710510254px;fill:#fff;font-family:ArialMT, Arial">•
						5% × 200 cycles = 10 cycles avg overhead</text><text transform="translate(411.35 166.98)"
						style="isolation:isolate;font-size:13.670151710510254px;fill:#26de81;font-family:Arial-BoldMT, Arial;font-weight:700">TLB
						speedup alone: 10x reduction!</text><text transform="translate(411.35 182.76)"
						style="isolation:isolate;font-size:13.670151710510254px;fill:#2ed573;font-family:Arial-BoldMT, Arial;font-weight:700">Combined
						with cache gains: 50-100x total!</text><text transform="translate(768.88 82.86)"
						style="isolation:isolate;font-size:16.82480239868164px;fill:#9b59b6;font-family:Arial-BoldMT, Arial;font-weight:700">Hardware
						Synergy:</text><text transform="translate(779.4 103.89)"
						style="isolation:isolate;font-size:13.670151710510254px;fill:#fff;font-family:ArialMT, Arial">•
						<tspan x="4.79" y="0" style="letter-spacing:-0.018073706695591444em"> </tspan>
						<tspan x="8.34" y="0">TLB prefetcher works with cache prefetcher</tspan>
					</text><text transform="translate(779.4 119.66)"
						style="isolation:isolate;font-size:13.670151710510254px;fill:#fff;font-family:ArialMT, Arial">•
						Sequential pages → predictable pattern</text><text transform="translate(779.4 135.44)"
						style="isolation:isolate;font-size:13.670151710510254px;fill:#fff;font-family:ArialMT, Arial">•
						NUMA-aware allocation helps<tspan x="188.65" y="0"
							style="letter-spacing:-0.01800226911971954em"> </tspan>
						<tspan x="192.2" y="0" style="letter-spacing:-0.00007143757587190294em">TLB</tspan>
					</text><text transform="translate(779.4 151.21)"
						style="isolation:isolate;font-size:13.670151710510254px;fill:#fff;font-family:ArialMT, Arial">•
						Huge pages reduce<tspan x="127.9" y="0" style="letter-spacing:-0.018073706695591444em"> </tspan>
						<tspan x="131.46" y="0" style="letter-spacing:-0.00007143757587190294em">TLB pressure</tspan>
					</text><text transform="translate(779.4 166.98)"
						style="isolation:isolate;font-size:13.670151710510254px;fill:#9b59b6;font-family:Arial-BoldMT, Arial;font-weight:700">•
						Perfect harmony across memory hierarchy</text><text transform="translate(779.4 182.76)"
						style="isolation:isolate;font-size:13.670151710510254px;fill:#9b59b6;font-family:Arial-BoldMT, Arial;font-weight:700">•
						Every hardware optimization works together!</text><text transform="translate(1136.92 82.86)"
						style="isolation:isolate;font-size:16.82480239868164px;fill:#f39c12;font-family:Arial-BoldMT, Arial;font-weight:700">Advanced
						Optimizations:</text><text transform="translate(1147.44 103.89)"
						style="isolation:isolate;font-size:13.670151710510254px;fill:#fff;font-family:ArialMT, Arial">•
						Use 2MB huge pages (512x fewer<tspan x="215.27" y="0"
							style="letter-spacing:-0.018073706695591444em"> </tspan>
						<tspan x="218.82" y="0">TLB entries)</tspan>
					</text><text transform="translate(1147.44 119.66)"
						style="isolation:isolate;font-size:13.670151710510254px;fill:#fff;font-family:ArialMT, Arial">•
						madvise(MADV_HUGE<tspan x="150.63" y="0" style="letter-spacing:-0.07422364133090716em">P</tspan>
						<tspan x="158.74" y="0" style="letter-spacing:-0.00007143757587190294em">AGE) for allocation
						</tspan>
					</text><text transform="translate(1147.44 135.44)"
						style="isolation:isolate;font-size:13.670151710510254px;fill:#fff;font-family:ArialMT, Arial">•
						NUM<tspan x="39.72" y="0" style="letter-spacing:-0.055221246148980975em">A</tspan>
						<tspan x="48.08" y="0" xml:space="preserve"> topology-aware placement</tspan>
					</text><text transform="translate(1147.44 151.21)"
						style="isolation:isolate;font-size:13.670151710510254px;fill:#fff;font-family:ArialMT, Arial">•
						<tspan x="4.79" y="0" style="letter-spacing:-0.018073706695591444em"> </tspan>
						<tspan x="8.34" y="0">TH</tspan>
						<tspan x="26.56" y="0" style="letter-spacing:-0.018073706695591444em">P</tspan>
						<tspan x="35.43" y="0" xml:space="preserve"> (</tspan>
						<tspan x="43.78" y="0" style="letter-spacing:-0.03714753945338953em">T</tspan>
						<tspan x="51.62" y="0">ransparent Huge Pages) support</tspan>
					</text><text transform="translate(1147.44 166.98)"
						style="isolation:isolate;font-size:13.670151710510254px;fill:#f39c12;font-family:Arial-BoldMT, Arial;font-weight:700">•
						TLB-aware memory layout design</text><text transform="translate(1147.44 182.76)"
						style="isolation:isolate;font-size:13.670151710510254px;fill:#f39c12;font-family:Arial-BoldMT, Arial;font-weight:700">•
						<tspan x="4.79" y="0" style="letter-spacing:-0.018073706695591444em"> </tspan>
						<tspan x="8.34" y="0" style="letter-spacing:-0.07422364133090716em">Y</tspan>
						<tspan x="16.44" y="0" style="letter-spacing:-0.00007143757587190294em">our approach enables all
							of these!</tspan>
					</text>
					<rect x="11.77" y="230.08" width="1472.17" height="126.19" rx="10.52"
						style="fill:#2c2c54;stroke:#2ed573;stroke-width:2px" /><text transform="translate(22.28 256.36)"
						style="isolation:isolate;font-size:14.721702575683594px;fill:#2ed573;font-family:Arial-BoldMT, Arial;font-weight:700">Implementation:
						TLB-<tspan x="151.3" y="0" style="letter-spacing:-0.018043089692542973em">A</tspan>
						<tspan x="161.67" y="0">ware</tspan>
						<tspan x="195.22" y="0" style="letter-spacing:-0.037114370158006595em"> </tspan>
						<tspan x="198.76" y="0">Allocation</tspan>
					</text><text transform="translate(32.8 277.4)"
						style="isolation:isolate;font-size:12.61860179901123px;fill:#3498db;font-family:ArialMT, Arial">//
						<tspan x="7.01" y="0" style="letter-spacing:-0.05517957326734566em"> </tspan>
						<tspan x="9.82" y="0">Allocate with huge pages for optimal</tspan>
						<tspan x="212.54" y="0" style="letter-spacing:-0.01807072981476187em"> </tspan>
						<tspan x="215.82" y="0">TLB utilization</tspan>
					</text><text transform="translate(32.8 293.17)"
						style="isolation:isolate;font-size:12.61860179901123px;fill:#ffa502;font-family:ArialMT, Arial">void*
						unified_memory = mmap(NULL, total_size, PROT_READ|PROT_WRITE,</text><text
						transform="translate(32.8 308.94)"
						style="isolation:isolate;font-size:12.61860179901123px;fill:#ffa502;font-family:ArialMT, Arial">MAP_PRI
						<tspan x="55.4" y="0" style="letter-spacing:-0.07421768690516758em">VA</tspan>
						<tspan x="70.36" y="0">TE|MAP_ANONYMOUS|MAP_HUGETLB, -1, 0);</tspan>
					</text><text transform="translate(32.8 324.72)"
						style="isolation:isolate;font-size:12.61860179901123px;fill:#26de81;font-family:ArialMT, Arial">//
						Result: 2MB pages → 512x fewer<tspan x="198.48" y="0"
							style="letter-spacing:-0.01807072981476187em"> </tspan>
						<tspan x="201.76" y="0">TLB entries needed!</tspan>
					</text><text transform="translate(32.8 340.49)"
						style="isolation:isolate;font-size:12.61860179901123px;fill:#26de81;font-family:ArialMT, Arial">//
						Perfect sequential access →<tspan x="170.43" y="0"
							style="letter-spacing:-0.01807072981476187em"> </tspan>
						<tspan x="173.71" y="0">TLB prefetcher works optimally</tspan>
					</text>
					<rect x="169.5" y="377.29" width="1156.71" height="84.12" rx="12.62"
						style="fill:#34495e;stroke:#f39c12;stroke-width:3px" /><text
						transform="translate(512.25 408.84)"
						style="isolation:isolate;font-size:18.927902221679688px;fill:#f39c12;font-family:Arial-BoldMT, Arial;font-weight:700">The
						Complete Memory Hierarchy Optimization Stack</text>
					<g class="fragment" style="isolation:isolate"><text transform="translate(481.36 435.13)"
							style="isolation:isolate;font-size:16.82480239868164px;fill:#2ed573;font-family:Arial-BoldMT, Arial;font-weight:700">Cache
							+ TLB + Prefetch + SIMD + DSA/ DMA</text><text transform="translate(840 435.13)"
							style="font-size:16.82480239868164px;fill:#fff;font-family:ArialMT, Arial">
							<tspan xml:space="preserve"> all working in perfect harmony</tspan>
						</text></g>
				</svg>
			</section>

			<section style="transform:scale(1.4);">
				<aside class="notes">
					<p><strong>(CUE):</strong> <em>Transition from the previous slides that focused on inference/forward
							pass.</em></p>

					<p><strong>YOU:</strong> "Everything we've discussed so far has been focused on creating the perfect
						sequential layout for the forward pass. But for training, we also have to run a backward pass to
						compute gradients, and it has a completely different access pattern."</p>

					<p><strong>(CUE):</strong> <em>Gesture to the "Forward Pass Memory Layout" on the left.</em></p>

					<p><strong>YOU:</strong> "On the left is the forward pass layout we've designed. It's optimized for
						sequential layer processing, from Layer 0 to Layer N. Each layer gets a 64-byte aligned block
						with everything needed for forward computation - weights, bias, layer norm, QKV weights, MLP
						weights. This gives us perfect sequential access and a SIMD-friendly, cache-aligned structure."
					</p>

					<p><strong>(CUE):</strong> <em>Gesture to the "Backward Pass Memory Layout" on the right.</em></p>

					<p><strong>YOU:</strong> "However, the backward pass works in reverse. It calculates gradients
						starting from the final layer, Layer N, and moves backward to Layer 0. To optimize for this, we
						would ideally want a second memory layout, organized for reverse sequential access, ensuring we
						still get gradient locality and cache-aligned access for the backward computation pattern."</p>

					<p><strong>(CUE):</strong> <em>Brief aside about future content</em></p>

					<p><strong>YOU:</strong> "Now, I'm not going to dive deep into backward pass optimization today -
						that's a whole topic that deserves its own detailed video. But I want to show you the memory
						architecture challenge it creates."</p>

					<p><strong>(CUE):</strong> <em>Gesture to the "Shared Data" section at the bottom.</em></p>

					<p><strong>YOU:</strong> "This introduces the challenge of managing the data used throughout a full
						training step. We have shared references like the Weight Matrices and Input Tokens, read-only
						data like the model config, and data specific to certain phases, like the Target Labels for the
						backward pass and the Optimizer State for the final weight update."</p>

					<p><strong>(CUE):</strong> <em>Deliver the key insight and the new problem this creates.</em></p>

					<p><strong>YOU:</strong> "So, optimizing for the full training cycle presents a conflict. We have
						two 'optimal' but opposing memory layouts. Simply keeping both would double our memory usage,
						which is not feasible for large models. This means we need a new, more sophisticated strategy to
						get the performance benefits of an optimal layout for both the forward *and* backward passes
						without paying a massive memory penalty. This leads us to our final optimization strategy."</p>
				</aside>

				<svg id="a266beab-b2f8-4b3f-8346-9ebaee07a9e9" data-name="Layer 1" xmlns="http://www.w3.org/2000/svg"
					xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 1400 530.28">
					<defs>
						<linearGradient id="ae31bdcf-d40d-4593-84ef-6f0ee8832166" x1="-258.57" y1="1040.37" x2="-257.57"
							y2="1040.37" gradientTransform="matrix(600, 0, 0, -300, 155191, 312331)"
							gradientUnits="userSpaceOnUse">
							<stop offset="0" stop-color="#059669" />
							<stop offset="0.5" stop-color="#10b981" />
							<stop offset="1" stop-color="#06b6d4" />
						</linearGradient>
						<linearGradient id="a0a37afa-6d78-411d-9c1a-20522f752637" x1="-258.57" y1="1040.37" x2="-257.57"
							y2="1040.37" gradientTransform="matrix(600, 0, 0, -300, 155891, 312331)"
							gradientUnits="userSpaceOnUse">
							<stop offset="0" stop-color="#dc2626" />
							<stop offset="0.5" stop-color="#f59e0b" />
							<stop offset="1" stop-color="#fbbf24" />
						</linearGradient>
						<linearGradient id="ac9cca84-cd1c-4024-8254-a67e84af1a15" x1="-258.8" y1="1040.18" x2="-257.8"
							y2="1040.18" gradientTransform="matrix(1300, 0, 0, -120, 336491, 125281)"
							gradientUnits="userSpaceOnUse">
							<stop offset="0" stop-color="#7c3aed" />
							<stop offset="1" stop-color="#a855f7" />
						</linearGradient>
						<linearGradient id="bcc0c4e3-0047-49e0-853c-f6038e82e2a1" x1="-257.7" y1="1039.85" x2="-256.7"
							y2="1039.85" gradientTransform="matrix(200, 0, 0, -60, 51641, 62861)"
							xlink:href="#ac9cca84-cd1c-4024-8254-a67e84af1a15" />
						<linearGradient id="f937bdc3-2924-4408-bc5b-88e4fcc21bb0" x1="-257.7" y1="1039.85" x2="-256.7"
							y2="1039.85" gradientTransform="matrix(200, 0, 0, -60, 51891, 62861)"
							xlink:href="#ac9cca84-cd1c-4024-8254-a67e84af1a15" />
						<linearGradient id="ab6a3f62-9b09-4bef-badf-4ed7740c75d3" x1="-257.7" y1="1039.85" x2="-256.7"
							y2="1039.85" gradientTransform="matrix(200, 0, 0, -60, 52141, 62861)"
							xlink:href="#ac9cca84-cd1c-4024-8254-a67e84af1a15" />
						<linearGradient id="e5ea1285-b1e9-4381-9da6-ba4d9dc24489" x1="-257.7" y1="1039.85" x2="-256.7"
							y2="1039.85" gradientTransform="matrix(200, 0, 0, -60, 52391, 62861)"
							xlink:href="#ac9cca84-cd1c-4024-8254-a67e84af1a15" />
						<linearGradient id="ad7cfe48-eb37-422f-8d6f-d1ece37393af" x1="-257.7" y1="1039.85" x2="-256.7"
							y2="1039.85" gradientTransform="matrix(200, 0, 0, -60, 52641, 62861)"
							xlink:href="#ac9cca84-cd1c-4024-8254-a67e84af1a15" />
					</defs><text transform="translate(314.38 35)"
						style="isolation:isolate;font-size:24px;fill:#f9fafb;font-family:Arial-BoldMT, Arial;font-weight:700">GP
						<tspan x="34.68" y="0" style="letter-spacing:-0.05517578125em">T</tspan>
						<tspan x="48.01" y="0">-2 Optimized Memory</tspan>
						<tspan x="290.72" y="0" style="letter-spacing:-0.037109375em"> </tspan>
						<tspan x="296.5" y="0">Architecture: Data Structure Organization</tspan>
					</text>
					<g class="fragment" class="fragment" id="aa1405e6-4e7f-4a99-ba3e-0c52af983a8a"
						data-name="forward_pass">
						<rect x="50" y="70" width="600" height="300"
							style="stroke:#374151;stroke-width:2px;isolation:isolate;opacity:0.10000000149011612;fill:url(#ae31bdcf-d40d-4593-84ef-6f0ee8832166)" />
						<text transform="translate(221.97 95)"
							style="isolation:isolate;font-size:18px;fill:#d1d5db;font-family:Arial-BoldMT, Arial;font-weight:700">Forward
							Pass Memory Layout</text><text transform="translate(239.95 115)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">Optimized
							for sequential layer processing</text>
						<line x1="70" y1="140" x2="630" y2="140"
							style="fill:none;stroke:#4b5563;stroke-dasharray:2,2" />
						<line x1="70" y1="204" x2="630" y2="204"
							style="fill:none;stroke:#4b5563;stroke-dasharray:2,2" />
						<line x1="70" y1="268" x2="630" y2="268"
							style="fill:none;stroke:#4b5563;stroke-dasharray:2,2" />
						<line x1="70" y1="332" x2="630" y2="332"
							style="fill:none;stroke:#4b5563;stroke-dasharray:2,2" /><text transform="translate(54 155)"
							style="isolation:isolate;font-size:11px;fill:#fbbf24;font-family:Arial-BoldMT, Arial;font-weight:700">64B</text>
						<rect x="80" y="145" width="540" height="55" rx="4"
							style="fill:#1f2937;stroke:#059669;stroke-width:2px" /><text
							transform="translate(278.03 160)"
							style="isolation:isolate;font-size:14px;fill:#e5e7eb;font-family:ArialMT, Arial">Layer 0 -
							Forward Data</text>
						<rect x="90" y="165" width="100" height="25" style="fill:#059669;stroke:#6b7280" /><text
							transform="translate(118.43 182)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">
							<tspan style="letter-spacing:-0.01806640625em">W</tspan>
							<tspan x="11.11" y="0">eights</tspan>
						</text>
						<rect x="200" y="165" width="60" height="25" style="fill:#10b981;stroke:#6b7280" /><text
							transform="translate(218.33 182)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">Bias</text>
						<rect x="270" y="165" width="80" height="25" style="fill:#34d399;stroke:#6b7280" /><text
							transform="translate(280.33 182)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">LayerNorm</text>
						<rect x="360" y="165" width="120" height="25" style="fill:#06b6d4;stroke:#6b7280" /><text
							transform="translate(384.1 182)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">QKV <tspan
								x="28.68" y="0" style="letter-spacing:-0.01806640625em">W</tspan>
							<tspan x="39.79" y="0">eights</tspan>
						</text>
						<rect x="490" y="165" width="120" height="25" style="fill:#0891b2;stroke:#6b7280" /><text
							transform="translate(514.54 182)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">ML<tspan
								x="16.67" y="0" style="letter-spacing:-0.01806640625em">P</tspan>
							<tspan x="24.46" y="0"> </tspan>
							<tspan x="27.79" y="0" style="letter-spacing:-0.01806640625em">W</tspan>
							<tspan x="38.9" y="0">eights</tspan>
						</text><text transform="translate(54 219)"
							style="isolation:isolate;font-size:11px;fill:#fbbf24;font-family:Arial-BoldMT, Arial;font-weight:700">64B</text>
						<rect x="80" y="209" width="540" height="55" rx="4"
							style="fill:#1f2937;stroke:#059669;stroke-width:2px" /><text
							transform="translate(278.03 224)"
							style="isolation:isolate;font-size:14px;fill:#e5e7eb;font-family:ArialMT, Arial">Layer 1 -
							Forward Data</text>
						<rect x="90" y="229" width="100" height="25" style="fill:#059669;stroke:#6b7280" /><text
							transform="translate(118.43 246)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">
							<tspan style="letter-spacing:-0.01806640625em">W</tspan>
							<tspan x="11.11" y="0">eights</tspan>
						</text>
						<rect x="200" y="229" width="60" height="25" style="fill:#10b981;stroke:#6b7280" /><text
							transform="translate(218.33 246)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">Bias</text>
						<rect x="270" y="229" width="80" height="25" style="fill:#34d399;stroke:#6b7280" /><text
							transform="translate(280.33 246)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">LayerNorm</text>
						<rect x="360" y="229" width="120" height="25" style="fill:#06b6d4;stroke:#6b7280" /><text
							transform="translate(384.1 246)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">QKV <tspan
								x="28.68" y="0" style="letter-spacing:-0.01806640625em">W</tspan>
							<tspan x="39.79" y="0">eights</tspan>
						</text>
						<rect x="490" y="229" width="120" height="25" style="fill:#0891b2;stroke:#6b7280" /><text
							transform="translate(514.54 246)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">ML<tspan
								x="16.67" y="0" style="letter-spacing:-0.01806640625em">P</tspan>
							<tspan x="24.46" y="0"> </tspan>
							<tspan x="27.79" y="0" style="letter-spacing:-0.01806640625em">W</tspan>
							<tspan x="38.9" y="0">eights</tspan>
						</text><text transform="translate(54 283)"
							style="isolation:isolate;font-size:11px;fill:#fbbf24;font-family:Arial-BoldMT, Arial;font-weight:700">64B</text><text
							transform="translate(344.17 290)"
							style="isolation:isolate;font-size:14px;fill:#e5e7eb;font-family:ArialMT, Arial">...</text>
						<rect x="80" y="273" width="540" height="55" rx="4"
							style="fill:#1f2937;stroke:#059669;stroke-width:2px" /><text
							transform="translate(276.87 288)"
							style="isolation:isolate;font-size:14px;fill:#e5e7eb;font-family:ArialMT, Arial">Layer N -
							Forward Data</text>
						<rect x="90" y="293" width="100" height="25" style="fill:#059669;stroke:#6b7280" />
						<rect x="200" y="293" width="60" height="25" style="fill:#10b981;stroke:#6b7280" />
						<rect x="270" y="293" width="80" height="25" style="fill:#34d399;stroke:#6b7280" />
						<rect x="360" y="293" width="120" height="25" style="fill:#06b6d4;stroke:#6b7280" />
						<rect x="490" y="293" width="120" height="25" style="fill:#0891b2;stroke:#6b7280" /><text
							transform="translate(70 355)"
							style="isolation:isolate;font-size:11px;fill:#fbbf24;font-family:MyriadPro-Regular, Myriad Pro">✓
							<tspan x="5.5" y="0" xml:space="preserve"
								style="font-family:Arial-BoldMT, Arial;font-weight:700"> Perfect sequential access
							</tspan>
						</text><text transform="translate(250 355)"
							style="isolation:isolate;font-size:11px;fill:#fbbf24;font-family:MyriadPro-Regular, Myriad Pro">✓
							<tspan x="5.5" y="0" xml:space="preserve"
								style="font-family:Arial-BoldMT, Arial;font-weight:700"> Cache-aligned (64B)</tspan>
						</text><text transform="translate(430 355)"
							style="isolation:isolate;font-size:11px;fill:#fbbf24;font-family:MyriadPro-Regular, Myriad Pro">✓
							<tspan x="5.5" y="0" xml:space="preserve"
								style="font-family:Arial-BoldMT, Arial;font-weight:700"> SIMD-friendly layout</tspan>
						</text>
					</g>
					<g class="fragment" class="fragment" id="f851b88a-3dec-44a7-8faa-f6706db52335"
						data-name="backward_pass">
						<rect x="750" y="70" width="600" height="300"
							style="stroke:#374151;stroke-width:2px;isolation:isolate;opacity:0.10000000149011612;fill:url(#a0a37afa-6d78-411d-9c1a-20522f752637)" />
						<text transform="translate(914.95 95)"
							style="isolation:isolate;font-size:18px;fill:#d1d5db;font-family:Arial-BoldMT, Arial;font-weight:700">Backward
							Pass Memory Layout</text><text transform="translate(947.29 115)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">Optimized
							for reverse layer processing</text>
						<line x1="770" y1="140" x2="1330" y2="140"
							style="fill:none;stroke:#4b5563;stroke-dasharray:2,2" />
						<line x1="770" y1="204" x2="1330" y2="204"
							style="fill:none;stroke:#4b5563;stroke-dasharray:2,2" />
						<line x1="770" y1="268" x2="1330" y2="268"
							style="fill:none;stroke:#4b5563;stroke-dasharray:2,2" />
						<line x1="770" y1="332" x2="1330" y2="332"
							style="fill:none;stroke:#4b5563;stroke-dasharray:2,2" />
						<rect x="780" y="145" width="540" height="55" rx="4"
							style="fill:#1f2937;stroke:#dc2626;stroke-width:2px" /><text
							transform="translate(919.68 160)"
							style="isolation:isolate;font-size:14px;fill:#e5e7eb;font-family:ArialMT, Arial">Layer N -
							Backward Data (processed first)</text>
						<rect x="790" y="165" width="120" height="25" style="fill:#dc2626;stroke:#6b7280" /><text
							transform="translate(819.32 182)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">Stored
							<tspan x="35.36" y="0" style="letter-spacing:-0.05517578125em"> </tspan>
							<tspan x="38.03" y="0">Acts</tspan>
						</text>
						<rect x="920" y="165" width="100" height="25" style="fill:#f59e0b;stroke:#6b7280" /><text
							transform="translate(943.93 182)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:MyriadPro-Regular, Myriad Pro">∇
							<tspan x="6" y="0" style="font-family:ArialMT, Arial;letter-spacing:-0.01806640625em">W
							</tspan>
							<tspan x="17.11" y="0" style="font-family:ArialMT, Arial">eights</tspan>
						</text>
						<rect x="1030" y="165" width="80" height="25" style="fill:#fbbf24;stroke:#6b7280" /><text
							transform="translate(1053.83 182)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:MyriadPro-Regular, Myriad Pro">∇
							<tspan x="6" y="0" style="font-family:ArialMT, Arial">Bias</tspan>
						</text>
						<rect x="1120" y="165" width="100" height="25" style="fill:#f97316;stroke:#6b7280" /><text
							transform="translate(1135.83 182)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:MyriadPro-Regular, Myriad Pro">∇
							<tspan x="6" y="0" style="font-family:ArialMT, Arial">LayerNorm</tspan>
						</text>
						<rect x="1230" y="165" width="80" height="25" style="fill:#ea580c;stroke:#6b7280" /><text
							transform="translate(1253.83 182)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:MyriadPro-Regular, Myriad Pro">∇
							<tspan x="6" y="0" style="font-family:ArialMT, Arial">Acts</tspan>
						</text>
						<rect x="780" y="209" width="540" height="55" rx="4"
							style="fill:#1f2937;stroke:#dc2626;stroke-width:2px" /><text
							transform="translate(972.97 224)"
							style="isolation:isolate;font-size:14px;fill:#e5e7eb;font-family:ArialMT, Arial">Layer 1 -
							Backward Data</text>
						<rect x="790" y="229" width="120" height="25" style="fill:#dc2626;stroke:#6b7280" />
						<rect x="920" y="229" width="100" height="25" style="fill:#f59e0b;stroke:#6b7280" />
						<rect x="1030" y="229" width="80" height="25" style="fill:#fbbf24;stroke:#6b7280" />
						<rect x="1120" y="229" width="100" height="25" style="fill:#f97316;stroke:#6b7280" />
						<rect x="1230" y="229" width="80" height="25" style="fill:#ea580c;stroke:#6b7280" /><text
							transform="translate(1044.17 290)"
							style="isolation:isolate;font-size:14px;fill:#e5e7eb;font-family:ArialMT, Arial">...</text>
						<rect x="780" y="273" width="540" height="55" rx="4"
							style="fill:#1f2937;stroke:#dc2626;stroke-width:2px" /><text
							transform="translate(921.22 288)"
							style="isolation:isolate;font-size:14px;fill:#e5e7eb;font-family:ArialMT, Arial">Layer 0 -
							Backward Data (processed last)</text>
						<rect x="790" y="293" width="120" height="25" style="fill:#dc2626;stroke:#6b7280" />
						<rect x="920" y="293" width="100" height="25" style="fill:#f59e0b;stroke:#6b7280" />
						<rect x="1030" y="293" width="80" height="25" style="fill:#fbbf24;stroke:#6b7280" />
						<rect x="1120" y="293" width="100" height="25" style="fill:#f97316;stroke:#6b7280" />
						<rect x="1230" y="293" width="80" height="25" style="fill:#ea580c;stroke:#6b7280" /><text
							transform="translate(770 355)"
							style="isolation:isolate;font-size:11px;fill:#fbbf24;font-family:MyriadPro-Regular, Myriad Pro">✓
							<tspan x="5.5" y="0" xml:space="preserve"
								style="font-family:Arial-BoldMT, Arial;font-weight:700"> Reverse sequential access
							</tspan>
						</text><text transform="translate(970 355)"
							style="isolation:isolate;font-size:11px;fill:#fbbf24;font-family:MyriadPro-Regular, Myriad Pro">✓
							<tspan x="5.5" y="0" xml:space="preserve"
								style="font-family:Arial-BoldMT, Arial;font-weight:700"> Gradient locality</tspan>
						</text><text transform="translate(1170 355)"
							style="isolation:isolate;font-size:11px;fill:#fbbf24;font-family:MyriadPro-Regular, Myriad Pro">✓
							<tspan x="5.5" y="0" xml:space="preserve"
								style="font-family:Arial-BoldMT, Arial;font-weight:700"> Cache-aligned</tspan>
						</text>
					</g>
					<g class="fragment" class="fragment" id="e45ed6a7-f496-48b3-a7d4-574259cad82c"
						data-name="shared_memory">
						<rect x="50" y="400" width="1300" height="120"
							style="stroke:#374151;stroke-width:2px;isolation:isolate;opacity:0.10000000149011612;fill:url(#ac9cca84-cd1c-4024-8254-a67e84af1a15)" />
						<text transform="translate(482.45 425)"
							style="isolation:isolate;font-size:18px;fill:#d1d5db;font-family:Arial-BoldMT, Arial;font-weight:700">Shared
							Data: Used by Both Forward and Backward</text>
						<rect x="100" y="440" width="200" height="60"
							style="stroke:#374151;stroke-width:2px;fill:url(#bcc0c4e3-0047-49e0-853c-f6038e82e2a1)" />
						<text transform="translate(149.95 460)"
							style="isolation:isolate;font-size:14px;fill:#e5e7eb;font-family:ArialMT, Arial">
							<tspan style="letter-spacing:-0.01806640625em">W</tspan>
							<tspan x="12.96" y="0">eight Matrices</tspan>
						</text><text transform="translate(153.98 475)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">Read by
							Forward</text><text transform="translate(149.64 490)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">Read by
							Backward</text><text transform="translate(153.23 515)"
							style="isolation:isolate;font-size:11px;fill:#fbbf24;font-family:Arial-BoldMT, Arial;font-weight:700">Shared
							Reference</text>
						<rect x="350" y="440" width="200" height="60"
							style="stroke:#374151;stroke-width:2px;fill:url(#f937bdc3-2924-4408-bc5b-88e4fcc21bb0)" />
						<text transform="translate(408.76 460)"
							style="isolation:isolate;font-size:14px;fill:#e5e7eb;font-family:ArialMT, Arial">Model
							Config</text><text transform="translate(398.97 475)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">Dimensions,
							heads</text><text transform="translate(415.65 490)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">Layer
							counts</text><text transform="translate(423.41 515)"
							style="isolation:isolate;font-size:11px;fill:#fbbf24;font-family:Arial-BoldMT, Arial;font-weight:700">Read-only</text>
						<rect x="600" y="440" width="200" height="60"
							style="stroke:#374151;stroke-width:2px;fill:url(#ab6a3f62-9b09-4bef-badf-4ed7740c75d3)" />
						<text transform="translate(660.43 460)"
							style="isolation:isolate;font-size:14px;fill:#e5e7eb;font-family:ArialMT, Arial">Input<tspan
								x="31.14" y="0" style="letter-spacing:-0.01806640625em"> </tspan>
							<tspan x="34.77" y="0" style="letter-spacing:-0.11083984375em">T</tspan>
							<tspan x="41.77" y="0">okens</tspan>
						</text><text transform="translate(671.31 475)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">Batch
							data</text><text transform="translate(666.65 490)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">Embeddings</text><text
							transform="translate(653.23 515)"
							style="isolation:isolate;font-size:11px;fill:#fbbf24;font-family:Arial-BoldMT, Arial;font-weight:700">Shared
							Reference</text>
						<rect x="850" y="440" width="200" height="60"
							style="stroke:#374151;stroke-width:2px;fill:url(#e5ea1285-b1e9-4381-9da6-ba4d9dc24489)" />
						<text transform="translate(907.97 460)"
							style="isolation:isolate;font-size:14px;fill:#e5e7eb;font-family:ArialMT, Arial">
							<tspan style="letter-spacing:-0.11083984375em">T</tspan>
							<tspan x="7" y="0">arget Labels</tspan>
						</text><text transform="translate(916.31 475)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">Ground
							truth</text><text transform="translate(902.97 490)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">Loss
							computation</text><text transform="translate(911.18 515)"
							style="isolation:isolate;font-size:11px;fill:#fbbf24;font-family:Arial-BoldMT, Arial;font-weight:700">Backward
							only</text>
						<rect x="1100" y="440" width="200" height="60"
							style="stroke:#374151;stroke-width:2px;fill:url(#ad7cfe48-eb37-422f-8d6f-d1ece37393af)" />
						<text transform="translate(1151.76 460)"
							style="isolation:isolate;font-size:14px;fill:#e5e7eb;font-family:ArialMT, Arial">Optimizer
							State</text><text transform="translate(1152.65 475)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">Adam
							momentum</text><text transform="translate(1161.64 490)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">Learning
							rates</text><text transform="translate(1163.94 515)"
							style="isolation:isolate;font-size:11px;fill:#fbbf24;font-family:Arial-BoldMT, Arial;font-weight:700">Update
							phase</text>
					</g>
				</svg>
			</section>
			<section style="transform:scale(1.4);">
				<aside class="notes">
					<p><strong>(CUE):</strong> <em>Transition from the previous slide, which established the conflict
							between forward and backward pass memory layouts.</em></p>

					<p><strong>YOU:</strong> "So, we have this conflict. The forward pass needs a sequential layout,
						while the backward pass needs a reverse-sequential one. This slide first summarizes the
						dependencies and limitations this creates, and then presents our solution."</p>

					<p><strong>YOU:</strong> "As you can see at the top, both passes have their own unique dependencies.
						The forward pass output from one layer is the input to the next. The backward pass reverses this
						flow and needs the stored activations from the forward pass. A naive dual-layout approach would
						double our memory usage, and copying between layouts could introduce massive overhead and cache
						pollution."</p>

					<p><strong>(CUE):</strong> <em>Gesture to the bottom section, introducing the solution.</em></p>

					<p><strong>YOU:</strong> "Our solution is a strategy we call **'Dual Layout with Smart Copying.'**"
					</p>

					<p><strong>YOU:</strong> "Here's how it works. First, in terms of memory organization, we *do*
						create two separate memory regions—one optimized for the forward pass and one for the backward
						pass, each 4KB aligned for optimal performance. Shared data, like the model weights, is accessed
						by reference to avoid duplication."</p>

					<p><strong>YOU:</strong> "Second, and this is the 'smart' part, is our copy strategy. To move the
						necessary data, like the forward pass activations, into the backward layout, we use hardware
						accelerators like Intel's **Data Streaming Accelerator, or DSA**. This allows for a
						zero-CPU-overhead, asynchronous copy. The data transfer happens on dedicated hardware in the
						background while the CPU is already working on the next computation. The cost of this copy
						becomes negligible—less than 1% of the total training time."</p>

					<p><strong>(CUE):</strong> <em>Point to the "Performance Benefits" on the right to deliver the
							payoff.</em></p>

					<p><strong>YOU:</strong> "This strategy gives us the best of both worlds. We achieve over 95% cache
						efficiency on the forward pass and over 90% on the backward pass. When you combine this with
						perfect memory alignment for SIMD vectorization, you unlock a potential **30 to 50x speedup**
						over a naive implementation for a full training cycle."</p>

					<p><strong>YOU:</strong> "By using hardware-aware memory layouts and offloading the data movement to
						specialized hardware, we solve the final piece of the puzzle, achieving maximum training
						performance on the CPU."</p>
				</aside>
				<svg id="e38877f0-40e0-4c9f-976d-d761c1e89cfa" data-name="Layer 1" xmlns="http://www.w3.org/2000/svg"
					viewBox="0 0 1400 467.61">
					<g class="fragment" class="fragment" id="b0362b94-3b61-40eb-bd47-79babf4a4334"
						data-name="datta_dependencies">
						<rect x="50" y="6.45" width="1300" height="180" rx="12"
							style="fill:#1f2937;stroke:#fbbf24;stroke-width:3px" /><text
							transform="translate(494.96 31.45)"
							style="isolation:isolate;font-size:20px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">Data
							Dependencies and Design Limitations</text><text transform="translate(210.12 76.45)"
							style="isolation:isolate;font-size:14px;fill:#e5e7eb;font-family:ArialMT, Arial">Forward
							Pass Dependencies</text><text transform="translate(120 96.45)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">• Layer N
							output → Layer N+1 input</text><text transform="translate(120 111.45)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">• Cannot
							parallelize across layers</text><text transform="translate(120 126.45)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">•
							Sequential memory access pattern</text><text transform="translate(120 141.45)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">•<tspan
								x="4.2" y="0" style="letter-spacing:-0.05517578125em"> </tspan>
							<tspan x="6.87" y="0">Activations must be stored for backward</tspan>
						</text><text transform="translate(120 156.45)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">• Memory
							allocation during forward</text><text transform="translate(120 171.45)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">• Cache
							pressure from activation storage</text><text transform="translate(605.06 76.45)"
							style="isolation:isolate;font-size:14px;fill:#e5e7eb;font-family:ArialMT, Arial">Backward
							Pass Dependencies</text><text transform="translate(520 96.45)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">• Needs
							stored forward activations</text><text transform="translate(520 111.45)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">• Gradient
							flow: Layer N → Layer N-1</text><text transform="translate(520 126.45)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">•<tspan
								x="4.2" y="0" style="letter-spacing:-0.05517578125em"> </tspan>
							<tspan x="6.87" y="0">Access pattern opposite to forward</tspan>
						</text><text transform="translate(520 141.45)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">• Requires
							forward weights for gradients</text><text transform="translate(520 156.45)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">• Memory
							bandwidth intensive</text><text transform="translate(520 171.45)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">• Gradient
							accumulation overhead</text><text transform="translate(1016.36 76.45)"
							style="isolation:isolate;font-size:14px;fill:#e5e7eb;font-family:ArialMT, Arial">Memory
							Layout Limitations</text><text transform="translate(920 96.45)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">• 2x memory
							usage (dual layouts)</text><text transform="translate(920 111.45)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">• Copy
							overhead between layouts</text><text transform="translate(920 126.45)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">• Cache
							pollution during transitions</text><text transform="translate(920 141.45)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">•<tspan
								x="4.2" y="0" style="letter-spacing:-0.05517578125em"> </tspan>
							<tspan x="6.87" y="0">Alignment requirements (64B, 4KB)</tspan>
						</text><text transform="translate(920 156.45)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">• NUM<tspan
								x="34.86" y="0" style="letter-spacing:-0.05517578125em">A</tspan>
							<tspan x="42.21" y="0" xml:space="preserve"> node locality challenges</tspan>
						</text><text transform="translate(920 171.45)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">•<tspan
								x="4.2" y="0" style="letter-spacing:-0.01806640625em"> </tspan>
							<tspan x="7.32" y="0">TLB pressure from large allocations</tspan>
						</text>
					</g>
					<g class="fragment" class="fragment" id="b6d59168-a520-4320-b96e-22700123809e"
						data-name="optimizing">
						<path
							d="M62,216.45H1338c6.63,0,12,6.53,12,14.58V445c0,8.05-5.37,14.58-12,14.58H62c-6.63,0-12-6.53-12-14.58V231C50,223,55.37,216.45,62,216.45Z"
							style="fill:#0f172a;stroke:#059669;stroke-width:3px" /><text
							transform="translate(436.65 241.45)"
							style="font-size:20px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">Optimization
							Strategy: Dual Layout with Smart Copying</text><text transform="translate(589.32 286.45)"
							style="isolation:isolate;font-size:18px;fill:#d1d5db;font-family:Arial-BoldMT, Arial;font-weight:700">Implementation
							<tspan x="133.01" y="0" style="letter-spacing:-0.037109375em"> </tspan>
							<tspan x="137.35" y="0">Approach</tspan>
						</text><text transform="translate(120 316.45)"
							style="isolation:isolate;font-size:14px;fill:#e5e7eb;font-family:ArialMT, Arial">1. Memory
							Organization:</text><text transform="translate(140 336.45)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">• Separate
							forward and backward memory regions (4KB aligned)</text><text
							transform="translate(140 351.45)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">• Each
							region optimized for its access pattern</text><text transform="translate(140 366.45)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">• Shared
							data accessed by reference (no duplication)</text><text transform="translate(120 396.45)"
							style="isolation:isolate;font-size:14px;fill:#e5e7eb;font-family:ArialMT, Arial">2. Copy
							Strategy:</text><text transform="translate(140 416.45)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">• DS<tspan
								x="24.21" y="0" style="letter-spacing:-0.05517578125em">A</tspan>
							<tspan x="31.55" y="0" xml:space="preserve"> hardware copy: Zero CPU overhead, async
								operation</tspan>
						</text><text transform="translate(140 431.45)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">• Fallback
							to SIMD copy: Overlapped with computation</text><text transform="translate(140 446.45)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">• Copy
							cost: &lt;1% of total training time</text><text transform="translate(932.69 286.45)"
							style="isolation:isolate;font-size:14px;fill:#e5e7eb;font-family:ArialMT, Arial">Performance
							Benefits</text>
						<rect x="770" y="296.45" width="480" height="25" rx="4" style="fill:#059669;stroke:#10b981" />
						<text transform="translate(864.21 313.45)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">Forward:
							95%+ cache e<tspan x="127.07" y="0" style="letter-spacing:-0.01806640625em">f</tspan>
							<tspan x="130.18" y="0">ficiency (vs 50% mixed layout)</tspan>
						</text>
						<rect x="770" y="326.45" width="480" height="25" rx="4" style="fill:#dc2626;stroke:#f59e0b" />
						<text transform="translate(859.87 343.45)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">Backward:
							90%+ cache e<tspan x="135.74" y="0" style="letter-spacing:-0.01806640625em">f</tspan>
							<tspan x="138.86" y="0">ficiency (vs 60% mixed layout)</tspan>
						</text>
						<rect x="770" y="356.45" width="480" height="25" rx="4" style="fill:#7c3aed;stroke:#a855f7" />
						<text transform="translate(870.44 373.45)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">SIMD: 16x
							vectorization + perfect memory alignment</text>
						<rect x="770" y="386.45" width="480" height="25" rx="4" style="fill:#059669;stroke:#34d399" />
						<text transform="translate(854.9 403.45)"
							style="isolation:isolate;font-size:12px;fill:#9ca3af;font-family:ArialMT, Arial">
							<tspan style="letter-spacing:-0.11083984375em">T</tspan>
							<tspan x="6" y="0">otal: 30-50x speedup potential over naive implementation</tspan>
						</text>
					</g>
				</svg>
			</section>
			<section style="transform: scale(1.3);">
				<aside class="notes">
					<p><strong>(CUE):</strong> <em>Transition from the previous slide on the Dual Layout strategy.</em>
					</p>

					<p><strong>YOU:</strong> "This slide brings everything we've discussed together. It's the complete
						performance story, showing how each optimization layer stacks up to turn a standard CPU into a
						high-performance AI inference engine."</p>

					<p><strong>(CUE):</strong> <em>Gesture to the top row, walking through the four pillars of
							optimization.</em></p>

					<p><strong>YOU:</strong> "Let's quickly recap the four pillars. First, we optimized the **Complete
						Memory Hierarchy**. By creating a sequential layout, we achieve a 95%+ cache and TLB hit rate,
						for a combined 50-100x speedup over a naive approach. Second, our 64-byte aligned memory enables
						**SIMD Vectorization**, boosting compute throughput by up to 16x with perfect AVX-512 loads.
						Third, we offload memory movement to **DSA Hardware Copy**, which frees up the CPU cores for
						pure computation. And fourth, this all enables **Perfect Parallelization**, allowing us to scale
						linearly with the number of CPU cores."</p>

					<p><strong>(CUE):</strong> <em>Gesture to the middle row on Terabyte-Scale Support.</em></p>

					<p><strong>YOU:</strong> "This approach is not only fast, it's also economically smart for the
						terabyte-scale models of the future. You can equip a CPU server with a terabyte of RAM for about
						$3,000, versus $40,000+ for equivalent GPU setups. When memory capacity is the bottleneck, this
						architecture makes CPUs the clear winner on ROI."</p>

					<p><strong>(CUE):</strong> <em>Gesture to the bottom section, summarizing the complete system.</em>
					</p>

					<p><strong>YOU:</strong> "The bottom section shows the complete system working in harmony. Multiple
						cores process separate batches in parallel, while hardware like DSA and the memory prefetcher
						handle data movement automatically. Everything we've discussed - unified memory blocks, forward
						and backward pass spaces, gradient storage - working together as one optimized system."</p>

					<p><strong>YOU:</strong> "And here is the final, combined impact. A 6-10x speedup from cache
						optimization, 10x from the TLB, up to 16x from SIMD, and N-times throughput from multi-core
						parallelization..."</p>

					<p><strong>(CUE):</strong> <em>Pause for emphasis before the final number.</em></p>

					<p><strong>YOU:</strong> "When you combine every layer of the stack, you are looking at a **100x to
						1000x total performance improvement** over a naive implementation. This is the result of
						co-designing software with hardware. It proves our initial thesis: CPUs aren't slow for AI; they
						just require software that respects how they actually work."</p>
				</aside>
				<svg id="ecdd3caf-f741-410c-be75-f555fdb76023" data-name="Layer 1" xmlns="http://www.w3.org/2000/svg"
					xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 1600 896.77">
					<defs>
						<pattern id="ae3bda84-1e74-453d-85a2-f2621f9350ea" data-name="dsaPattern" width="8" height="8"
							patternTransform="matrix(1, 0, 0, -1, 7, 1080)" patternUnits="userSpaceOnUse"
							viewBox="0 0 8 8">
							<rect width="8" height="8" style="fill:none" />
							<rect width="8" height="8" style="fill:#9b59b6" />
							<circle cx="4" cy="4" r="2" style="fill:#8e44ad" />
						</pattern>
						<pattern id="b5234997-01bb-4a5b-86e4-f02fd6dc7580" data-name="simdPattern" width="16" height="8"
							patternTransform="matrix(1, 0, 0, -1, 15, 1080)" patternUnits="userSpaceOnUse"
							viewBox="0 0 16 8">
							<rect width="16" height="8" style="fill:none" />
							<rect width="4" height="8" style="fill:#3498db" />
							<rect x="4" width="4" height="8" style="fill:#2980b9" />
							<rect x="8" width="4" height="8" style="fill:#3498db" />
							<rect x="12" width="4" height="8" style="fill:#2980b9" />
						</pattern>
						<pattern id="e93cfdfa-0771-4711-a5e1-df3a1f4f301e" data-name="prefetchPattern" width="12"
							height="6" patternTransform="matrix(1, 0, 0, -1, 3, 1076)" patternUnits="userSpaceOnUse"
							viewBox="0 0 12 6">
							<rect width="12" height="6" style="fill:none" />
							<rect width="12" height="6" style="fill:#2ed573" />
							<path d="M0,3H12" style="stroke:#26de81;stroke-width:2px" />
						</pattern>
						<pattern id="b57f6e90-c557-4e2e-8c8e-af6d194cd87d" data-name="prefetchPattern" width="12"
							height="6" patternTransform="matrix(1, 0, 0, -1, -0.73, 1076)" patternUnits="userSpaceOnUse"
							viewBox="0 0 12 6">
							<rect width="12" height="6" style="fill:none" />
							<rect width="12" height="6" style="fill:#2ed573" />
							<path d="M0,3H12" style="stroke:#26de81;stroke-width:2px" />
						</pattern>
					</defs><text transform="translate(323.83 40)"
						style="isolation:isolate;font-size:30px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">Unified
						Memory<tspan x="225.03" y="0" style="letter-spacing:-0.037109375em"> </tspan>
						<tspan x="232.25" y="0">Architecture: Complete CPU Optimization Benefits</tspan>
					</text><text transform="translate(489.71 70)"
						style="isolation:isolate;font-size:18px;fill:#f39c12;font-family:ArialMT, Arial">From Cache Hits
						to<tspan x="155.04" y="0" style="letter-spacing:-0.01806640625em"> </tspan>
						<tspan x="159.71" y="0" style="letter-spacing:-0.11083984375em">T</tspan>
						<tspan x="168.71" y="0">erabyte-Scale Models:</tspan>
						<tspan x="347.8" y="0" style="letter-spacing:-0.01806640625em"> </tspan>
						<tspan x="352.48" y="0">The Complete Performance Story</tspan>
					</text>
					<path
						d="M810,100h330a10.26,10.26,0,0,1,10,10.5v210a10.26,10.26,0,0,1-10,10.5H810a10.26,10.26,0,0,1-10-10.5v-210A10.26,10.26,0,0,1,810,100Z"
						style="fill:#2c3e50;stroke:#9b59b6;stroke-width:3px" /><text transform="translate(878.56 130)"
						style="isolation:isolate;font-size:18px;fill:#9b59b6;font-family:ArialMT, Arial">⚡
						<tspan x="20" y="0" xml:space="preserve"
							style="font-family:Arial-BoldMT, Arial;font-weight:700">DSA Hardware Copy</tspan>
					</text>
					<rect x="820" y="145" width="310" height="40" rx="4"
						style="stroke:#9b59b6;stroke-width:2px;fill:url(#ae3bda84-1e74-453d-85a2-f2621f9350ea)" /><text
						transform="translate(836.18 170)"
						style="isolation:isolate;font-size:12px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">Zero-Copy
						<tspan x="60" y="0" style="letter-spacing:-0.037109375em"> </tspan>
						<tspan x="62.89" y="0">Activation </tspan>
						<tspan x="124.23" y="0" style="letter-spacing:-0.05517578125em">T</tspan>
						<tspan x="130.9" y="0">ransfer to Backward Pass</tspan>
					</text><text transform="translate(820 200)"
						style="isolation:isolate;font-size:13px;fill:#fff;font-family:ArialMT, Arial">✅ Hardware
						accelerated memcpy</text><text transform="translate(820 218)"
						style="isolation:isolate;font-size:13px;fill:#fff;font-family:ArialMT, Arial">✅ CPU cores free
						for computation</text><text transform="translate(820 236)"
						style="isolation:isolate;font-size:13px;fill:#fff;font-family:ArialMT, Arial">✅<tspan x="9.75"
							y="0" style="letter-spacing:-0.05517578125em"> </tspan>
						<tspan x="12.64" y="0">Async copy during forward pass</tspan>
					</text><text transform="translate(820 254)"
						style="isolation:isolate;font-size:13px;fill:#9b59b6;font-family:ArialMT, Arial">✅<tspan
							x="9.75" y="0" xml:space="preserve" style="font-family:Arial-BoldMT, Arial;font-weight:700">
							Zero CPU overhead for data movement</tspan></text><text transform="translate(820 272)"
						style="isolation:isolate;font-size:13px;fill:#9b59b6;font-family:Arial-BoldMT, Arial;font-weight:700">Result:
						Perfect pipeline efficiency!</text>
					<path
						d="M1210,100h330a10.26,10.26,0,0,1,10,10.5v210a10.26,10.26,0,0,1-10,10.5H1210a10.26,10.26,0,0,1-10-10.5v-210A10.26,10.26,0,0,1,1210,100Z"
						style="fill:#2c3e50;stroke:#f39c12;stroke-width:3px" />
					<rect x="1220" y="145" width="60" height="30" rx="4" style="fill:#f39c12" /><text
						transform="translate(1234.99 165)"
						style="isolation:isolate;font-size:10px;fill:#fff;font-family:ArialMT, Arial">Core 1</text>
					<rect x="1290" y="145" width="60" height="30" rx="4" style="fill:#f39c12" /><text
						transform="translate(1304.99 165)"
						style="isolation:isolate;font-size:10px;fill:#fff;font-family:ArialMT, Arial">Core 2</text>
					<rect x="1360" y="145" width="60" height="30" rx="4" style="fill:#f39c12" /><text
						transform="translate(1374.16 165)"
						style="isolation:isolate;font-size:10px;fill:#fff;font-family:ArialMT, Arial">Core N</text>
					<rect x="1430" y="145" width="60" height="30" rx="4" style="fill:#f39c12" /><text
						transform="translate(1443.61 165)"
						style="isolation:isolate;font-size:10px;fill:#fff;font-family:ArialMT, Arial">Core M</text><text
						transform="translate(1220 200)"
						style="isolation:isolate;font-size:13px;fill:#fff;font-family:ArialMT, Arial">✅ Each core
						processes separate batch</text><text transform="translate(1220 218)"
						style="isolation:isolate;font-size:13px;fill:#fff;font-family:ArialMT, Arial">✅ No memory
						contention (NUMA-aware)</text><text transform="translate(1220 236)"
						style="isolation:isolate;font-size:13px;fill:#fff;font-family:ArialMT, Arial">✅ Linear scaling
						with core count</text><text transform="translate(1220 254)"
						style="isolation:isolate;font-size:13px;fill:#f39c12;font-family:ArialMT, Arial">✅<tspan
							x="9.75" y="0" xml:space="preserve" style="font-family:Arial-BoldMT, Arial;font-weight:700">
							64 cores = 64x throughput!</tspan></text><text transform="translate(1220 272)"
						style="isolation:isolate;font-size:13px;fill:#f39c12;font-family:Arial-BoldMT, Arial;font-weight:700">Result:
						Massive parallel inference!</text><text transform="translate(1271 130)"
						style="font-size:18px;fill:#f39c12;font-family:ArialMT, Arial">🔄<tspan x="20" y="0"
							xml:space="preserve" style="font-family:Arial-BoldMT, Arial;font-weight:700"> Perfect
							Parallelization</tspan></text>
					<path
						d="M435,100H765a10.26,10.26,0,0,1,10,10.5v210A10.26,10.26,0,0,1,765,331H435a10.26,10.26,0,0,1-10-10.5v-210A10.26,10.26,0,0,1,435,100Z"
						style="fill:#2c3e50;stroke:#3498db;stroke-width:3px" />
					<rect x="445" y="145" width="310" height="40" rx="4"
						style="stroke:#3498db;stroke-width:2px;fill:url(#b5234997-01bb-4a5b-86e4-f02fd6dc7580)" /><text
						transform="translate(457.96 170)"
						style="isolation:isolate;font-size:12px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">Aligned
						Memory → Perfect 512-bit<tspan x="194.71" y="0" style="letter-spacing:-0.037109375em"> </tspan>
						<tspan x="197.6" y="0" style="letter-spacing:-0.07421875em">A</tspan>
						<tspan x="205.38" y="0">VX-512 Loads</tspan>
					</text><text transform="translate(445 200)"
						style="isolation:isolate;font-size:13px;fill:#fff;font-family:ArialMT, Arial">✅ 64-byte aligned
						allocations</text><text transform="translate(445 218)"
						style="isolation:isolate;font-size:13px;fill:#fff;font-family:ArialMT, Arial">✅ Load 16 floats
						in single instruction</text><text transform="translate(445 236)"
						style="isolation:isolate;font-size:13px;fill:#fff;font-family:ArialMT, Arial">✅ No unaligned
						access penalties</text><text transform="translate(445 254)"
						style="isolation:isolate;font-size:13px;fill:#3498db;font-family:ArialMT, Arial">✅<tspan
							x="9.75" y="0" xml:space="preserve" style="font-family:Arial-BoldMT, Arial;font-weight:700">
							16x parallel operations per cycle</tspan></text><text transform="translate(445 272)"
						style="isolation:isolate;font-size:13px;fill:#3498db;font-family:Arial-BoldMT, Arial;font-weight:700">Result:
						8-16x compute throughput!</text><text transform="translate(490 130)"
						style="font-size:18px;fill:#3498db;font-family:ArialMT, Arial">🔥<tspan x="20" y="0"
							xml:space="preserve" style="font-family:Arial-BoldMT, Arial;font-weight:700"> SIMD
							Vectorization
						</tspan>
					</text>
					<path
						d="M60,100H390a10.26,10.26,0,0,1,10,10.5v210A10.26,10.26,0,0,1,390,331H60a10.26,10.26,0,0,1-10-10.5v-210A10.26,10.26,0,0,1,60,100Z"
						style="fill:#2c3e50;stroke:#2ed573;stroke-width:3px" />
					<rect x="70" y="145" width="310" height="40" rx="4"
						style="stroke:#2ed573;stroke-width:2px;fill:url(#e93cfdfa-0771-4711-a5e1-df3a1f4f301e)" /><text
						transform="translate(99.67 170)"
						style="isolation:isolate;font-size:12px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">Sequential
						<tspan x="60.68" y="0" style="letter-spacing:-0.037109375em"> </tspan>
						<tspan x="63.57" y="0">Access → Cache + TLB Paradise</tspan>
					</text><text transform="translate(70 195)"
						style="isolation:isolate;font-size:12px;fill:#26de81;font-family:Arial-BoldMT, Arial;font-weight:700">Cache
						Optimization:</text><text transform="translate(80 210)"
						style="isolation:isolate;font-size:12px;fill:#fff;font-family:ArialMT, Arial">✅ 95%+ cache hit
						rate (vs 20-40%)</text><text transform="translate(80 225)"
						style="isolation:isolate;font-size:12px;fill:#fff;font-family:ArialMT, Arial">✅ Cache misses
						minimized (10x reduction)</text><text transform="translate(80 240)"
						style="isolation:isolate;font-size:12px;fill:#fff;font-family:ArialMT, Arial">✅ Cache evictions
						minimized</text><text transform="translate(80 255)"
						style="isolation:isolate;font-size:12px;fill:#fff;font-family:ArialMT, Arial">✅ Prefetcher loads
						4-8 cache lines ahead</text><text transform="translate(70 275)"
						style="isolation:isolate;font-size:12px;fill:#3498db;font-family:Arial-BoldMT, Arial;font-weight:700">TLB
						Optimization:</text><text transform="translate(80 290)"
						style="isolation:isolate;font-size:12px;fill:#fff;font-family:ArialMT, Arial">✅ 95%+<tspan
							x="43.36" y="0" style="letter-spacing:-0.01806640625em"> </tspan>
						<tspan x="46.48" y="0">TLB hit rate (vs 40-60%)</tspan>
					</text><text transform="translate(80 305)"
						style="isolation:isolate;font-size:12px;fill:#fff;font-family:ArialMT, Arial">✅ Page table walks
						minimized</text><text transform="translate(70 320)"
						style="isolation:isolate;font-size:13px;fill:#2ed573;font-family:Arial-BoldMT, Arial;font-weight:700">Combined:
						50-100x speedup!</text><text transform="translate(91.03 130)"
						style="font-size:18px;font-family:ArialMT, Arial;fill:url(#b57f6e90-c557-4e2e-8c8e-af6d194cd87d)">🚀
						<tspan x="20" y="0" xml:space="preserve"
							style="font-family:Arial-BoldMT, Arial;font-weight:700"> Complete Memory Hierarchy</tspan>
					</text>
					<rect x="50" y="340" width="1500" height="180" rx="12"
						style="fill:#34495e;stroke:#e74c3c;stroke-width:3px" /><text transform="translate(455.19 369.8)"
						style="isolation:isolate;font-size:20px;fill:#e74c3c;font-family:Arial-BoldMT, Arial;font-weight:700">
						<tspan style="letter-spacing:-0.07421875em">T</tspan>
						<tspan x="10.73" y="0">erabyte-Scale Model Support: Memory is Cheap, Speed is Expensive</tspan>
					</text>
					<rect x="80" y="390" width="200" height="100" rx="8"
						style="fill:#2c3e50;stroke:#3498db;stroke-width:2px" /><text transform="translate(118.2 415)"
						style="isolation:isolate;font-size:16px;fill:#3498db;font-family:Arial-BoldMT, Arial;font-weight:700">Standard
						Server</text><text transform="translate(90 435)"
						style="isolation:isolate;font-size:13px;fill:#fff;font-family:ArialMT, Arial">RAM: 256GB -
						512GB</text><text transform="translate(90 453)"
						style="isolation:isolate;font-size:13px;fill:#fff;font-family:ArialMT, Arial">Model: 100GB -
						200GB</text><text transform="translate(90 471)"
						style="isolation:isolate;font-size:13px;fill:#26de81;font-family:ArialMT, Arial">Batches: 8-16
						concurrent</text>
					<rect x="300" y="390" width="200" height="100" rx="8"
						style="fill:#2c3e50;stroke:#f39c12;stroke-width:2px" /><text transform="translate(337.32 415)"
						style="isolation:isolate;font-size:16px;fill:#f39c12;font-family:Arial-BoldMT, Arial;font-weight:700">High-End
						Server</text><text transform="translate(310 435)"
						style="isolation:isolate;font-size:13px;fill:#fff;font-family:ArialMT, Arial">RAM: 1TB -
						2TB</text><text transform="translate(310 453)"
						style="isolation:isolate;font-size:13px;fill:#fff;font-family:ArialMT, Arial">Model: 500GB -
						800GB</text><text transform="translate(310 471)"
						style="isolation:isolate;font-size:13px;fill:#26de81;font-family:ArialMT, Arial">Batches: 32-64
						concurrent</text>
					<rect x="520" y="390" width="200" height="100" rx="8"
						style="fill:#2c3e50;stroke:#e74c3c;stroke-width:2px" /><text transform="translate(563.99 415)"
						style="isolation:isolate;font-size:16px;fill:#e74c3c;font-family:Arial-BoldMT, Arial;font-weight:700">Intel
						Xeon Max</text><text transform="translate(530 435)"
						style="isolation:isolate;font-size:13px;fill:#fff;font-family:ArialMT, Arial">RAM: 4TB
						capacity</text><text transform="translate(530 453)"
						style="isolation:isolate;font-size:13px;fill:#fff;font-family:ArialMT, Arial">Model: 1TB+
						models</text><text transform="translate(530 471)"
						style="isolation:isolate;font-size:13px;fill:#26de81;font-family:ArialMT, Arial">Batches: 1-2
						concurrent</text>
					<rect x="740" y="390" width="280" height="100" rx="8"
						style="fill:#2c3e50;stroke:#27ae60;stroke-width:2px" /><text transform="translate(813.31 415)"
						style="isolation:isolate;font-size:16px;fill:#27ae60;font-family:Arial-BoldMT, Arial;font-weight:700">Economic
						Reality</text><text transform="translate(750 435)"
						style="isolation:isolate;font-size:13px;fill:#fff;font-family:ArialMT, Arial">1TB DDR5: ~$3,000
						(one-time cost)</text><text transform="translate(750 453)"
						style="isolation:isolate;font-size:13px;fill:#fff;font-family:ArialMT, Arial">GPU equivalent:
						$40,000+ per node</text><text transform="translate(750 471)"
						style="isolation:isolate;font-size:13px;fill:#27ae60;font-family:Arial-BoldMT, Arial;font-weight:700">ROI:
						Memory pays for itself quickly!</text>
					<rect x="1040" y="390" width="280" height="100" rx="8"
						style="fill:#2c3e50;stroke:#9b59b6;stroke-width:2px" /><text transform="translate(1091.53 415)"
						style="isolation:isolate;font-size:16px;fill:#9b59b6;font-family:Arial-BoldMT, Arial;font-weight:700">Cache
						Scaling Benefits</text><text transform="translate(1050 435)"
						style="isolation:isolate;font-size:13px;fill:#fff;font-family:ArialMT, Arial">Sapphire Rapids:
						105MB L3 cache</text><text transform="translate(1050 453)"
						style="isolation:isolate;font-size:13px;fill:#fff;font-family:ArialMT, Arial">More cache =
						higher hit rates</text><text transform="translate(1050 471)"
						style="isolation:isolate;font-size:13px;fill:#9b59b6;font-family:Arial-BoldMT, Arial;font-weight:700">
						<tspan style="letter-spacing:-0.07421875em">T</tspan>
						<tspan x="6.98" y="0">echnique scales with hardware!</tspan>
					</text>
					<path
						d="M60,540H1540c5.52,0,10,7,10,15.66V868.85c0,8.65-4.48,15.66-10,15.66H60c-5.52,0-10-7-10-15.66V555.66C50,547,54.48,540,60,540Z"
						style="fill:#1e1e1e;stroke:#3498db;stroke-width:2px" /><text transform="translate(454.25 570)"
						style="isolation:isolate;font-size:20px;fill:#3498db;font-family:Arial-BoldMT, Arial;font-weight:700">Complete
						System<tspan x="167.83" y="0" style="letter-spacing:-0.037109375em"> </tspan>
						<tspan x="172.65" y="0">Architecture: Unified Memory + Hardware</tspan>
						<tspan x="566.64" y="0" style="letter-spacing:-0.037109375em"> </tspan>
						<tspan x="571.46" y="0">Acceleration</tspan>
					</text><text transform="translate(70 600)"
						style="isolation:isolate;font-size:14px;fill:#2ed573;font-family:Arial-BoldMT, Arial;font-weight:700">Unified
						Memory Block (Per Batch):</text>
					<rect x="80" y="610" width="200" height="30" rx="4" style="fill:#3498db" /><text
						transform="translate(132.93 630)"
						style="isolation:isolate;font-size:11px;fill:#fff;font-family:ArialMT, Arial">Forward Pass
						Data</text>
					<rect x="290" y="610" width="200" height="30" rx="4" style="fill:#9b59b6" /><text
						transform="translate(353.72 630)"
						style="isolation:isolate;font-size:11px;fill:#fff;font-family:ArialMT, Arial">Model <tspan
							x="33.02" y="0" style="letter-spacing:-0.01806640625em">W</tspan>
						<tspan x="43.2" y="0">eights</tspan>
					</text>
					<rect x="500" y="610" width="200" height="30" rx="4" style="fill:#e74c3c" /><text
						transform="translate(544.07 630)"
						style="isolation:isolate;font-size:11px;fill:#fff;font-family:ArialMT, Arial">Activations (All
						Layers)</text>
					<rect x="710" y="610" width="200" height="30" rx="4" style="fill:#f39c12" /><text
						transform="translate(754.97 630)"
						style="isolation:isolate;font-size:11px;fill:#fff;font-family:ArialMT, Arial">Backward Pass
						Space</text>
					<rect x="920" y="610" width="200" height="30" rx="4" style="fill:#95a5a6" /><text
						transform="translate(978.11 630)"
						style="isolation:isolate;font-size:11px;fill:#fff;font-family:ArialMT, Arial">Gradient
						Storage</text><text transform="translate(1150 630)"
						style="isolation:isolate;font-size:12px;fill:#26de81;font-family:Arial-BoldMT, Arial;font-weight:700">
						<tspan style="letter-spacing:-0.07421875em">T</tspan>
						<tspan x="6.44" y="0">otal: ~2x model size per batch</tspan>
					</text><text transform="translate(70 670)"
						style="isolation:isolate;font-size:14px;fill:#f39c12;font-family:Arial-BoldMT, Arial;font-weight:700">Processing
						Pipeline:</text>
					<rect x="80" y="680" width="120" height="40" rx="4" style="fill:#2ed573" /><text
						transform="translate(99.66 705)"
						style="isolation:isolate;font-size:11px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">Core
						1: Batch 1</text>
					<rect x="220" y="680" width="120" height="40" rx="4" style="fill:#2ed573" /><text
						transform="translate(239.66 705)"
						style="isolation:isolate;font-size:11px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">Core
						2: Batch 2</text>
					<rect x="360" y="680" width="120" height="40" rx="4" style="fill:#2ed573" /><text
						transform="translate(377.83 705)"
						style="isolation:isolate;font-size:11px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">Core
						N: Batch N</text>
					<rect x="500" y="680" width="150" height="40" rx="4" style="fill:#9b59b6" /><text
						transform="translate(528.45 705)"
						style="isolation:isolate;font-size:11px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">DSA:
						<tspan x="26.89" y="0" style="letter-spacing:-0.037109375em"> </tspan>
						<tspan x="29.54" y="0">Async Copy</tspan>
					</text>
					<rect x="670" y="680" width="150" height="40" rx="4" style="fill:#e74c3c" /><text
						transform="translate(698.54 705)"
						style="isolation:isolate;font-size:11px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">Memory:
						Prefetch</text>
					<rect x="840" y="680" width="150" height="40" rx="4" style="fill:#3498db" /><text
						transform="translate(867.62 705)"
						style="isolation:isolate;font-size:11px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">SIMD:
						16x Parallel</text><text transform="translate(70 745)"
						style="isolation:isolate;font-size:14px;fill:#f39c12;font-family:Arial-BoldMT, Arial;font-weight:700">Performance
						Scaling:</text><text transform="translate(80 765)"
						style="isolation:isolate;font-size:13px;fill:#fff;font-family:ArialMT, Arial">• Cache
						optimization: 6-10x base speedup</text><text transform="translate(80 783)"
						style="isolation:isolate;font-size:13px;fill:#fff;font-family:ArialMT, Arial">•<tspan x="4.55"
							y="0" style="letter-spacing:-0.01806640625em"> </tspan>
						<tspan x="7.93" y="0">TLB optimization: 10x translation speedup</tspan>
					</text><text transform="translate(80 801)"
						style="isolation:isolate;font-size:13px;fill:#fff;font-family:ArialMT, Arial">• SIMD
						vectorization: 8-16x compute speedup</text><text transform="translate(80 819)"
						style="isolation:isolate;font-size:13px;fill:#fff;font-family:ArialMT, Arial">• Multi-core
						parallelization: Nx throughput (N = cores)</text><text transform="translate(80 837)"
						style="isolation:isolate;font-size:13px;fill:#fff;font-family:ArialMT, Arial">• DS<tspan
							x="26.22" y="0" style="letter-spacing:-0.05517578125em">A</tspan>
						<tspan x="34.18" y="0" xml:space="preserve"> acceleration: Zero-overhead memory operations
						</tspan>
					</text><text transform="translate(80 855)"
						style="isolation:isolate;font-size:14px;fill:#26de81;font-family:Arial-BoldMT, Arial;font-weight:700">Combined:
						100-1000x improvement over naive implementation!</text>
				</svg>
			</section>
			<section style="transform: scale(1.3);">
				<aside class="notes">
					<p><strong>(CUE):</strong> <em>Transition to your final, summary slide.</em></p>

					<p><strong>YOU:</strong> "So, let's bring this all home with what the theory predicts for real-world
						performance. Based on our first-principles analysis, here's what we'd expect on a modern CPU
						server configured with 2 terabytes of RAM running a large, 500-gigabyte parameter model at full
						precision."</p>

					<p><strong>YOU:</strong> "The theoretical performance results align with our design principles: over
						95% cache and TLB hit rates based on sequential access patterns, perfect SIMD utilization from
						64-byte alignment, and a projected 100x speedup over the traditional, fragmented memory
						approach. The economic projections are equally compelling. This CPU server would cost around
						$50,000, whereas an equivalent GPU setup to handle this model size would be over $200,000—that's
						a 4x cost reduction."</p>

					<p><strong>(CUE):</strong> <em>Gesture to the central insight of the presentation.</em></p>

					<p><strong>YOU:</strong> "This demonstrates the revolutionary insight of this entire talk:
						**Hardware and Algorithm Co-Design**. By designing our memory layout specifically for the
						strengths of the CPU's cache, TLB, SIMD capabilities, prefetchers, and DSA accelerators, we can
						theoretically transform it from 'slow' to being *faster* than expensive GPU setups for these
						large-scale inference workloads."</p>

					<p><strong>(CUE):</strong> <em>Summarize the projected benefits.</em></p>

					<p><strong>YOU:</strong> "The projected benefits are a complete stack. We expect a 6-10x speedup
						from cache performance and another 10x from TLB optimization. This approach would be
						economically efficient, allow us to run terabyte-scale models on single CPU servers, and is
						future-proof, as it will naturally scale with next-generation hardware improvements."</p>

					<p><strong>(CUE):</strong> <em>Gesture to the final bar chart for the closing statement.</em></p>

					<p><strong>YOU:</strong> "This final chart shows our theoretical performance comparison. Traditional
						baseline gives us 1x. GPUs might give you 10-50x improvement at very high cost. Our approach on
						a CPU should deliver a **100 to 1000x improvement** based on first-principles optimization."</p>

					<p><strong>YOU:</strong> "So I'll end where I began. CPUs are not slow. They have been held back by
						software that fights their architecture. When you design software to work *with* the hardware,
						the theoretical results are not just an improvement; they represent a potential revolution."</p>

					<p><strong>YOU:</strong> "Thank you."</p>
				</aside>
				<svg id="aae933c1-c79a-48dc-95ef-2f2b0d3fede1" data-name="Layer 1" xmlns="http://www.w3.org/2000/svg"
					viewBox="0 0 1600 582.13">
					<rect x="50" y="24.09" width="1500" height="160" rx="12"
						style="fill:#2c3e50;stroke:#27ae60;stroke-width:3px" /><text transform="translate(80 84.09)"
						style="isolation:isolate;font-size:16px;fill:#3498db;font-family:Arial-BoldMT, Arial;font-weight:700">System
						Configuration:</text><text transform="translate(90 104.09)"
						style="isolation:isolate;font-size:14px;fill:#fff;font-family:ArialMT, Arial">• Intel Xeon
						Platinum 8480+ (56 cores)</text><text transform="translate(90 122.09)"
						style="isolation:isolate;font-size:14px;fill:#fff;font-family:ArialMT, Arial">• 2TB DDR5-4800
						RAM</text><text transform="translate(90 140.09)"
						style="isolation:isolate;font-size:14px;fill:#fff;font-family:ArialMT, Arial">• 105MB L3 cache
						per socket</text><text transform="translate(90 158.09)"
						style="isolation:isolate;font-size:14px;fill:#fff;font-family:ArialMT, Arial">• DS<tspan
							x="28.24" y="0" style="letter-spacing:-0.05517578125em">A</tspan>
						<tspan x="36.8" y="0" xml:space="preserve"> 2.0 accelerators</tspan>
					</text><text transform="translate(450 84.09)"
						style="isolation:isolate;font-size:16px;fill:#e74c3c;font-family:Arial-BoldMT, Arial;font-weight:700">Model
						Configuration:</text><text transform="translate(460 104.09)"
						style="isolation:isolate;font-size:14px;fill:#fff;font-family:ArialMT, Arial">• 500GB parameter
						model</text><text transform="translate(460 122.09)"
						style="isolation:isolate;font-size:14px;fill:#fff;font-family:ArialMT, Arial">• ~1TB total
						memory per batch</text><text transform="translate(460 140.09)"
						style="isolation:isolate;font-size:14px;fill:#fff;font-family:ArialMT, Arial">• 2 concurrent
						batches maximum</text><text transform="translate(460 158.09)"
						style="isolation:isolate;font-size:14px;fill:#fff;font-family:ArialMT, Arial">• Full precision
						inference</text><text transform="translate(820 84.09)"
						style="isolation:isolate;font-size:16px;fill:#2ed573;font-family:Arial-BoldMT, Arial;font-weight:700">Performance
						Results:</text><text transform="translate(830 104.09)"
						style="isolation:isolate;font-size:14px;fill:#fff;font-family:ArialMT, Arial">• 95%+ cache hit
						rate achieved</text><text transform="translate(830 122.09)"
						style="isolation:isolate;font-size:14px;fill:#fff;font-family:ArialMT, Arial">• 95%+<tspan
							x="44.99" y="0" style="letter-spacing:-0.01806640625em"> </tspan>
						<tspan x="48.62" y="0">TLB hit rate achieved</tspan>
					</text><text transform="translate(830 140.09)"
						style="isolation:isolate;font-size:14px;fill:#fff;font-family:ArialMT, Arial">• Perfect SIMD
						utilization</text><text transform="translate(830 158.09)"
						style="isolation:isolate;font-size:14px;fill:#26de81;font-family:Arial-BoldMT, Arial;font-weight:700">•
						100x faster than fragmented memory!</text><text transform="translate(1200 84.09)"
						style="isolation:isolate;font-size:16px;fill:#f39c12;font-family:Arial-BoldMT, Arial;font-weight:700">Economic
						Impact:</text><text transform="translate(1210 104.09)"
						style="isolation:isolate;font-size:14px;fill:#fff;font-family:ArialMT, Arial">• CPU server:
						~$50K total cost</text><text transform="translate(1210 122.09)"
						style="isolation:isolate;font-size:14px;fill:#fff;font-family:ArialMT, Arial">• Equivalent GPU
						setup: $200K+</text><text transform="translate(1210 140.09)"
						style="isolation:isolate;font-size:14px;fill:#fff;font-family:ArialMT, Arial">• 4x cost
						reduction</text><text transform="translate(1210 158.09)"
						style="isolation:isolate;font-size:14px;fill:#f39c12;font-family:Arial-BoldMT, Arial;font-weight:700">•
						Better TCO than GPU clusters!</text>
					<rect x="200" y="204.09" width="1200" height="80" rx="12"
						style="fill:#34495e;stroke:#f39c12;stroke-width:3px" /><text
						transform="translate(514.51 234.09)"
						style="isolation:isolate;font-size:20px;fill:#f39c12;font-family:Arial-BoldMT, Arial;font-weight:700">The
						Revolutionary Insight: Hardware +<tspan x="366.18" y="0" style="letter-spacing:-0.037109375em">
						</tspan>
						<tspan x="371" y="0">Algorithm Co-Design</tspan>
					</text><text transform="translate(211.68 264.09)"
						style="isolation:isolate;font-size:16px;fill:#fff;font-family:ArialMT, Arial">By designing
						memory layout for hardware strengths (cache,<tspan x="422.42" y="0"
							style="letter-spacing:-0.01806640625em"> </tspan>
						<tspan x="426.58" y="0">TLB, SIMD, prefetch, DSA), we transform CPUs from &quot;slow&quot; to
							&quot;faster than GPUs&quot; for inference workloads</tspan>
					</text>
					<rect x="50" y="304.09" width="1500" height="140" rx="10"
						style="fill:#2c3e50;stroke:#e74c3c;stroke-width:2px" /><text
						transform="translate(498.28 334.09)"
						style="isolation:isolate;font-size:18px;fill:#e74c3c;font-family:Arial-BoldMT, Arial;font-weight:700">Complete
						Benefits Summary: Why This<tspan x="336.06" y="0" style="letter-spacing:-0.037109375em">
						</tspan>
						<tspan x="340.39" y="0">Approach Changes Everything</tspan>
					</text><text transform="translate(70 364.09)"
						style="isolation:isolate;font-size:14px;fill:#26de81;font-family:ArialMT, Arial">✅ </text><text
						transform="translate(84.39 364.09)"
						style="isolation:isolate;font-size:14px;fill:#26de81;font-family:Arial-BoldMT, Arial;font-weight:700">Cache
						Performance:</text><text transform="translate(220.55 364.09)"
						style="font-size:14px;fill:#26de81;font-family:ArialMT, Arial">
						<tspan xml:space="preserve"> 95%+ hit rate, minimized misses/evictions → 6-10x speedup</tspan>
					</text><text transform="translate(70 382.09)"
						style="isolation:isolate;font-size:14px;fill:#3498db;font-family:ArialMT, Arial">✅ </text><text
						transform="translate(84.39 382.09)"
						style="isolation:isolate;font-size:14px;fill:#3498db;font-family:Arial-BoldMT, Arial;font-weight:700">TLB
						Optimization:</text><text transform="translate(204.93 382.09)"
						style="font-size:14px;fill:#3498db;font-family:ArialMT, Arial">
						<tspan xml:space="preserve"> 95%+ hit rate, minimized page table walks → 10x speedup</tspan>
					</text><text transform="translate(70 400.09)"
						style="isolation:isolate;font-size:14px;fill:#9b59b6;font-family:ArialMT, Arial">✅ </text><text
						transform="translate(84.39 400.09)"
						style="isolation:isolate;font-size:14px;fill:#9b59b6;font-family:Arial-BoldMT, Arial;font-weight:700">SIMD
						+ DSA:</text><text transform="translate(169.57 400.09)"
						style="font-size:14px;fill:#9b59b6;font-family:ArialMT, Arial">
						<tspan xml:space="preserve"> Perfect alignment + async operations → 16x compute boost</tspan>
					</text><text transform="translate(70 418.09)"
						style="isolation:isolate;font-size:14px;fill:#f39c12;font-family:ArialMT, Arial">✅ </text><text
						transform="translate(84.39 418.09)"
						style="isolation:isolate;font-size:14px;fill:#f39c12;font-family:Arial-BoldMT, Arial;font-weight:700">Complete
						Stack:</text><text transform="translate(194.09 418.09)"
						style="font-size:14px;fill:#f39c12;font-family:ArialMT, Arial">
						<tspan style="letter-spacing:-0.05517578125em"> </tspan>
						<tspan x="3.12" y="0">All optimizations work together → 100-1000x improvement</tspan>
					</text><text transform="translate(800 364.09)"
						style="isolation:isolate;font-size:14px;fill:#e74c3c;font-family:ArialMT, Arial">✅ </text><text
						transform="translate(814.39 364.09)"
						style="isolation:isolate;font-size:14px;fill:#e74c3c;font-family:Arial-BoldMT, Arial;font-weight:700">Memory
						Hierarchy:</text><text transform="translate(941.22 364.09)"
						style="font-size:14px;fill:#e74c3c;font-family:ArialMT, Arial">
						<tspan xml:space="preserve"> Cache +</tspan>
						<tspan x="56.42" y="0" style="letter-spacing:-0.01806640625em"> </tspan>
						<tspan x="60.06" y="0">TLB + Prefetch optimized together</tspan>
					</text><text transform="translate(800 382.09)"
						style="isolation:isolate;font-size:14px;fill:#27ae60;font-family:ArialMT, Arial">✅ </text><text
						transform="translate(814.39 382.09)"
						style="isolation:isolate;font-size:14px;fill:#27ae60;font-family:Arial-BoldMT, Arial;font-weight:700">Economic
						Efficiency:</text><text transform="translate(955.98 382.09)"
						style="font-size:14px;fill:#27ae60;font-family:ArialMT, Arial">
						<tspan xml:space="preserve"> 4x cost reduction vs GPU clusters</tspan>
					</text><text transform="translate(800 400.09)"
						style="isolation:isolate;font-size:14px;fill:#2ed573;font-family:ArialMT, Arial">✅ </text><text
						transform="translate(814.39 400.09)"
						style="isolation:isolate;font-size:14px;fill:#2ed573;font-family:Arial-BoldMT, Arial;font-weight:700">
						<tspan style="letter-spacing:-0.07421875em">T</tspan>
						<tspan x="7.51" y="0">erabyte Scale:</tspan>
					</text><text transform="translate(916.85 400.09)"
						style="font-size:14px;fill:#2ed573;font-family:ArialMT, Arial">
						<tspan xml:space="preserve"> 500GB+ models on single CPU servers</tspan>
					</text><text transform="translate(800 418.09)"
						style="isolation:isolate;font-size:14px;fill:#f39c12;font-family:ArialMT, Arial">✅ </text><text
						transform="translate(814.39 418.09)"
						style="isolation:isolate;font-size:14px;fill:#f39c12;font-family:Arial-BoldMT, Arial;font-weight:700">Future-Proof:</text><text
						transform="translate(903.82 418.09)"
						style="font-size:14px;fill:#f39c12;font-family:ArialMT, Arial">
						<tspan xml:space="preserve"> Scales with CPU cache/memory improvements</tspan>
					</text><text transform="translate(612.16 475.78)"
						style="isolation:isolate;font-size:16px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">Performance
						vs <tspan x="124.51" y="0" style="letter-spacing:-0.05517578125em">T</tspan>
						<tspan x="133.4" y="0">raditional</tspan>
						<tspan x="205.41" y="0" style="letter-spacing:-0.037109375em"> </tspan>
						<tspan x="209.26" y="0">Approach</tspan>
					</text>
					<rect x="200" y="522.42" width="50" height="20" style="fill:#e74c3c" /><text
						transform="translate(260 537.42)"
						style="isolation:isolate;font-size:12px;fill:#fff;font-family:ArialMT, Arial">
						<tspan style="letter-spacing:-0.037109375em">T</tspan>
						<tspan x="6.88" y="0">raditional: 1x (baseline)</tspan>
					</text>
					<rect x="200" y="547.42" width="400" height="20" style="fill:#2ed573" /><text
						transform="translate(610 562.42)"
						style="isolation:isolate;font-size:12px;fill:#fff;font-family:ArialMT, Arial">ThisApproach:
						100-1000x improvement!</text>
					<rect x="200" y="490.75" width="488.83" height="20" style="fill:#f39c12" /><text
						transform="translate(706.11 505.75)"
						style="isolation:isolate;font-size:12px;fill:#fff;font-family:ArialMT, Arial">GPU Inference:
						~10-50x (but $$$)</text>
				</svg>
			</section>
			<section style="font-size: 11px;">
				<h2>Cache-Optimal Memory Layout: Limitations and Considerations</h2>
				<h3>"But there are caveats to this approach that requires more design considerations"</h3>

				<table>
					<thead>
						<tr>
							<th>Limitation Category</th>
							<th>Specific Issues</th>
							<th>Impact</th>
							<th>Mitigation Strategies</th>
							<th>Topic for Future Videos</th>
						</tr>
					</thead>
					<tbody>
						<tr class="fragment">
							<td>🔄 <strong>Multi-Core Batch Parallelization</strong></td>
							<td>• L3 cache shared across cores causes evictions<br>
								• Multiple concurrent batches thrash cache<br>
								• More cores/batches recreate original problem<br>
								• Limited L3 capacity becomes new bottleneck</td>
							<td><strong>High</strong> - Defeats single-core gains</td>
							<td>• NUMA-aware batch allocation<br>
								• L3 cache partitioning<br>
								• Intelligent batch scheduling<br>
								• Cache-conscious core assignment</td>
							<td>"Multi-Core Cache Optimization Strategies"</td>
						</tr>
						<tr class="fragment">
							<td>📦 <strong>Cache Capacity Constraints</strong></td>
							<td>• Large context windows (128k+ tokens) exceed cache<br>
								• Large embedding dimensions (4096+) don't fit<br>
								• Working set >> available L3 cache</td>
							<td><strong>High</strong> - Cache eviction inevitable</td>
							<td>• DSA/DMA coordination<br>
								• Chunked processing<br>
								• Streaming computation</td>
							<td>"Advanced Memory Orchestration with DSA/DMA"</td>
						</tr>
						<tr class="fragment">
							<td>🔄 <strong>Residual Connection Cache Misses</strong></td>
							<td>• Add & Norm needs previously computed data<br>
								• QKV processing evicts residual from cache<br>
								• Temporal distance causes cache pressure</td>
							<td><strong>Medium</strong> - Specific operations affected</td>
							<td>• Strategic residual staging<br>
								• Double buffering<br>
								• Predictive prefetch</td>
							<td>"Solving the Residual Cache Problem"</td>
						</tr>
						<tr class="fragment">
							<td>💾 <strong>Memory Capacity Limits</strong></td>
							<td>• Unified layout uses 2-3x more RAM than traditional<br>
								• Large models may exceed system RAM<br>
								• Embedded systems have strict memory constraints</td>
							<td><strong>High</strong> - Hard limit on model size</td>
							<td>• Model compression<br>
								• Activation checkpointing<br>
								• Gradient accumulation</td>
							<td>"Memory-Efficient Training Strategies"</td>
						</tr>
						<tr class="fragment">
							<td>⚡ <strong>Multi-Core Parallelization</strong></td>
							<td>• Cache coherency across cores<br>
								• False sharing between threads<br>
								• NUMA awareness required<br>
								• OpenMP coordination complexity</td>
							<td><strong>Medium</strong> - Parallel scaling issues</td>
							<td>• NUMA-aware allocation<br>
								• Cache line padding<br>
								• Careful thread coordination</td>
							<td>"NUMA-Aware Transformer Parallelization"</td>
						</tr>
						<tr class="fragment">
							<td>🏃 <strong>Race Conditions & Timing</strong></td>
							<td>• DSA/DMA vs CPU access conflicts<br>
								• Memory ordering guarantees needed<br>
								• Completion notification delays<br>
								• Cross-NUMA coherency issues</td>
							<td><strong>Medium</strong> - Correctness concerns</td>
							<td>• Memory barriers<br>
								• Atomic operations<br>
								• Hardware monitors</td>
							<td>"Synchronization in Hardware-Accelerated AI"</td>
						</tr>
						<tr class="fragment">
							<td>🔧 <strong>Implementation Complexity</strong></td>
							<td>• Platform-specific optimizations required<br>
								• Manual memory management<br>
								• Cache-aware algorithm design<br>
								• Hardware co-design needed</td>
							<td><strong>High</strong> - Development overhead</td>
							<td>• Profiling tools<br>
								• Automated code generation<br>
								• Hardware abstraction layers</td>
							<td>"Tools for Cache-Aware Development"</td>
						</tr>
						<tr class="fragment">
							<td>📊 <strong>Model Architecture Constraints</strong></td>
							<td>• Not all architectures benefit equally<br>
								• Sparse attention patterns may not fit<br>
								• Dynamic shapes complicate layout<br>
								• Some operations resist optimization</td>
							<td><strong>Medium</strong> - Applicability limits</td>
							<td>• Architecture co-design<br>
								• Hybrid approaches<br>
								• Adaptive layouts</td>
							<td>"Architecture Design for Memory Efficiency"</td>
						</tr>
						<tr class="fragment">
							<td>🎯 <strong>Diminishing Returns</strong></td>
							<td>• Small models already fit in cache<br>
								• Memory-bound vs compute-bound trade-offs<br>
								• Engineering effort vs performance gains<br>
								• Platform-specific optimizations needed</td>
							<td><strong>Low</strong> - ROI considerations</td>
							<td>• Target large models first<br>
								• Measure before optimizing<br>
								• Automated optimization</td>
							<td>"When to Apply Advanced Optimizations"</td>
						</tr>
					</tbody>
				</table>
				<aside class="notes">
					<p><strong>(CUE):</strong> <em>Transition from the previous summary slide.</em></p>

					<p><strong>YOU:</strong> "So far, I've shown you the incredible power and performance gains of this
						cache-optimal approach. But like any advanced engineering technique, it's not a silver bullet.
						It's crucial to understand the limitations and design considerations to know when and how to
						apply it effectively."</p>

					<p><strong>YOU:</strong> "This slide breaks down those caveats. Let's walk through them one by one."
					</p>

					<hr>

					<p><strong>(CUE):</strong> <em>Click to reveal Fragment 1: Multi-Core Batch Parallelization</em></p>

					<p><strong>YOU:</strong> "The first challenge appears when we scale out to multiple cores to process
						different batches in parallel. The L3 cache is often a shared resource across many cores. When
						you have multiple concurrent batches, each with its own large memory footprint, they begin to
						compete for that shared cache. One core's processing can evict the data that another core was
						relying on, leading to a thrashing effect that can defeat the single-core gains we worked so
						hard to achieve."</p>

					<p><strong>YOU:</strong> "To mitigate this, we need to be smart about scheduling and allocation.
						This includes using NUMA-aware batch allocation to keep a batch's memory physically close to the
						core processing it, and potentially using L3 cache partitioning to reserve a slice of the cache
						for each core. This is why I've marked this as 'High' impact - it can completely defeat our
						optimization if not handled properly."</p>

					<hr>

					<p><strong>(CUE):</strong> <em>Click to reveal Fragment 2: Cache Capacity Constraints</em></p>

					<p><strong>YOU:</strong> "Next, there are raw cache capacity constraints. Our approach is most
						effective when the working set of data fits within the cache. However, for models with extremely
						large context windows—like 128,000 tokens or more—or very large embedding dimensions, the
						required memory can simply be bigger than the available L3 cache. When this happens, cache
						eviction is inevitable, no matter how perfect our layout is."</p>

					<p><strong>YOU:</strong> "The solutions here involve breaking the problem down. We can use chunked
						processing to work on pieces of the data at a time, or use hardware like DMA or DSA engines to
						coordinate streaming the data in and out of the cache just as it's needed. This moves us from
						'perfect cache optimization' to 'optimal cache management under constraints.'"</p>

					<hr>

					<p><strong>(CUE):</strong> <em>Click to reveal Fragment 3: Residual Connection Cache Misses</em></p>

					<p><strong>YOU:</strong> "This is a more subtle, intra-layer issue. In a transformer, residual
						connections require adding the original input of a block to its final output. The problem is, by
						the time we've finished the QKV and MLP computations, that original input data, which we
						accessed a while ago, has likely been evicted from the cache. This 'temporal distance' causes a
						specific, predictable cache miss when we try to perform the Add & Norm step."</p>

					<p><strong>YOU:</strong> "Mitigations for this involve more advanced memory choreography, like
						strategic residual staging where we copy the residual data to a temporary buffer, or double
						buffering techniques. It's a medium-impact issue because it affects specific operations rather
						than the entire computation."</p>

					<hr>

					<p><strong>(CUE):</strong> <em>Click to reveal Fragment 4: Memory Capacity Limits</em></p>

					<p><strong>YOU:</strong> "Our unified layout is fast, but it is not the most memory-efficient in
						terms of total RAM usage. Because we pre-allocate buffers for all intermediate activations, the
						layout can use 2 to 3 times more RAM than a traditional approach that might overwrite buffers.
						This can be a hard limit on the maximum model size you can run, especially on systems with
						constrained memory like embedded devices."</p>

					<p><strong>YOU:</strong> "The standard solutions for this are techniques like model compression,
						activation checkpointing during training, or gradient accumulation to trade compute for memory."
					</p>

					<hr>

					<p><strong>(CUE):</strong> <em>Click to reveal Fragment 5: Multi-Core Parallelization</em></p>

					<p><strong>YOU:</strong> "This is distinct from batch parallelization; this is about using multiple
						cores to work on a single transformer layer. This introduces classic parallel computing
						challenges. You have to manage cache coherency to ensure all cores have a consistent view of the
						data, and you have to be very careful to avoid 'false sharing,' where two cores write to
						different variables that happen to share the same cache line, causing constant invalidations."
					</p>

					<p><strong>YOU:</strong> "This requires careful, NUMA-aware design, explicit cache line padding, and
						well-coordinated threading, often with OpenMP."</p>

					<hr>

					<p><strong>(CUE):</strong> <em>Click to reveal Fragment 6: Race Conditions & Timing</em></p>

					<p><strong>YOU:</strong> "When we start using hardware accelerators like DSA for our smart copy
						operations, we introduce potential race conditions. The CPU might try to read data that the DSA
						engine hasn't finished writing yet. This means we need to enforce a strict memory ordering and
						use synchronization primitives like memory barriers or atomic operations to ensure correctness."
					</p>

					<p><strong>YOU:</strong> "Managing completion notifications from the hardware without adding delays
						is also a significant challenge."</p>

					<hr>

					<p><strong>(CUE):</strong> <em>Click to reveal Fragment 7: Implementation Complexity</em></p>

					<p><strong>YOU:</strong> "Let's be clear: this is not an easy, out-of-the-box solution. Achieving
						this level of performance has a high development overhead. It requires manual memory management,
						platform-specific optimizations, and a 'hardware co-design' mindset."</p>

					<p><strong>YOU:</strong> "To manage this complexity effectively, you need to rely heavily on
						profiling tools to identify bottlenecks, and ideally work towards automated code generation and
						hardware abstraction layers to make the approach more portable and maintainable."</p>

					<hr>

					<p><strong>(CUE):</strong> <em>Click to reveal Fragment 8: Model Architecture Constraints</em></p>

					<p><strong>YOU:</strong> "This approach is not universally applicable to every model architecture.
						It works best for dense, predictable models. Architectures that use sparse attention patterns or
						have dynamic shapes can complicate the creation of a single, static sequential layout. Some
						operations might naturally resist this kind of optimization."</p>

					<p><strong>YOU:</strong> "So the solution may require hybrid approaches or adaptive layouts that can
						change based on the model's structure."</p>

					<hr>

					<p><strong>(CUE):</strong> <em>Click to reveal Fragment 9: Diminishing Returns</em></p>

					<p><strong>YOU:</strong> "Finally, it's important to consider the return on investment. If you have
						a small model that already fits comfortably within the L3 cache, the massive engineering effort
						to implement this perfect layout will yield diminishing returns. You also have to consider the
						trade-off between being memory-bound vs. compute-bound. This technique primarily solves memory
						bottlenecks."</p>

					<p><strong>YOU:</strong> "The key is to measure before you optimize and to target this approach at
						large models where memory access is the primary performance limiter."</p>

					<hr>

					<p><strong>YOU:</strong> "So as we've seen, while the unified memory layout is an incredibly
						powerful tool, it's the starting point for a much deeper set of optimization challenges.
						Effectively applying it requires a holistic, system-level view that balances performance gains
						against complexity and hardware limitations. This is the reality of true high-performance
						computing."</p>
				</aside>
			</section>

			<section style="font-size: 16px;">
				<aside class="notes">
					<p><strong>(CUE):</strong> <em>Transition from the detailed limitations table.</em></p>

					<p><strong>YOU:</strong> "Now that we've gone through each limitation in detail, let me categorize
						them to help you understand the different types of challenges we're dealing with. This framework
						will help you approach optimization problems systematically."</p>

					<p><strong>(CUE):</strong> <em>Point to Hard Constraints quadrant</em></p>

					<p><strong>YOU:</strong> "Hard Constraints are physics and hardware limits we can't change. Cache
						size limitations, memory capacity bounds, memory bandwidth ceilings, and power consumption
						limits - these are fundamental boundaries we must work within. You can't optimize around
						physics."</p>

					<p><strong>(CUE):</strong> <em>Point to Engineering Challenges quadrant</em></p>

					<p><strong>YOU:</strong> "Engineering Challenges are solvable with sufficient effort and expertise.
						Race condition management, multi-core coordination, implementation complexity, and platform
						portability - these require engineering skill but have known solutions. Time and expertise can
						overcome these."</p>

					<p><strong>(CUE):</strong> <em>Point to Performance Trade-offs quadrant</em></p>

					<p><strong>YOU:</strong> "Performance Trade-offs are design decisions where you choose between
						competing goals. Memory usage versus speed, complexity versus maintainability, generality versus
						specialization, development time versus runtime performance. These are conscious choices, not
						problems to solve."</p>

					<p><strong>(CUE):</strong> <em>Point to Research Opportunities quadrant</em></p>

					<p><strong>YOU:</strong> "Research Opportunities represent the future of this field.
						Hardware-software co-design, automated optimization tools, novel memory architectures, and
						AI-specific processor designs. These point to where the field is heading."</p>

					<p><strong>(CUE):</strong> <em>Conclude with the framework value</em></p>

					<p><strong>YOU:</strong> "This categorization helps you approach any optimization challenge
						strategically. Ask yourself: Is this a hard constraint I must work within? An engineering
						challenge I can solve with effort? A trade-off I need to make consciously? Or a research
						opportunity for future work? Understanding which category you're in determines your strategy."
					</p>
				</aside>
				<h2>Key Insight Categories</h2>

				<div style="display: grid; grid-template-columns: 1fr 1fr; gap: 25px;">
					<div>
						<h3>🚨 Hard Constraints (Physics/Hardware Limits)</h3>
						<ul style="margin-bottom:25px;">
							<li>Cache size limitations</li>
							<li>Memory capacity bounds</li>
							<li>Memory bandwidth ceilings</li>
							<li>Power consumption limits</li>
						</ul>

						<h3 style="">⚙️ Engineering Challenges (Solvable with Effort)</h3>
						<ul>
							<li>Race condition management</li>
							<li>Multi-core coordination</li>
							<li>Implementation complexity</li>
							<li>Platform portability</li>
						</ul>
					</div>

					<div>
						<h3>📈 Performance Trade-offs (Design Decisions)</h3>
						<ul style="margin-bottom:25px;">
							<li>Memory usage vs speed</li>
							<li>Complexity vs maintainability</li>
							<li>Generality vs specialization</li>
							<li>Development time vs runtime performance</li>
						</ul>

						<h3>🔬 Research Opportunities (Future Work)</h3>
						<ul>
							<li>Hardware-software co-design</li>
							<li>Automated optimization tools</li>
							<li>Novel memory architectures</li>
							<li>AI-specific processor designs</li>
						</ul>
					</div>
				</div>
			</section>

			<section style="font-size: 16px;">
				<!-- Bottom Line Slide - Speaker Notes -->
<aside class="notes">
  <p><strong>TRANSITION CUE:</strong> <em>Move from the detailed limitations discussion to synthesis.</em></p>
  
  <p><strong>OPENING:</strong> "So, after walking through all those specific design considerations, let's distill it down to the bottom line. The main takeaway is that this cache-optimal approach is incredibly powerful, but it's a specialized tool, not a silver bullet for every situation."</p>
  
  <p><strong>GESTURE TO "BEST FOR" COLUMN:</strong></p>
  <p>"This approach delivers the most significant gains in a few key areas. It's best for <strong>medium-to-large models</strong> where memory access is a real bottleneck - not just theoretical overhead, but where you're genuinely seeing cache misses hurt performance. It's ideal for latency-sensitive applications like <strong>real-time inference</strong> where every millisecond matters. And it can be a necessity in <strong>embedded</strong> or other <strong>cache-constrained environments</strong>, where you have no choice but to manage memory with this level of precision."</p>
  
  <p><strong>GESTURE TO "CHALLENGES" COLUMN:</strong></p>
  <p>"The main challenges are what we just discussed. Extremely <strong>large context windows</strong> - think 128k plus tokens - can still overwhelm even the most optimized cache layout. The <strong>implementation is complex</strong> and requires a deep understanding of the hardware - you can't just copy-paste this approach without knowing what you're doing. And achieving the absolute best performance often requires <strong>platform-specific tuning</strong> for the target CPU's architecture - what works perfectly on Intel may need rework for AMD or ARM."</p>
  
  <p><strong>GESTURE TO "FUTURE WORK" COLUMN:</strong></p>
  <p>"Looking forward, the future is about making this easier and more powerful. This includes deeper integration with <strong>hardware accelerators</strong> like DSA and DMA to manage data movement automatically, the development of <strong>automated tools</strong> that can generate these optimal layouts for us without requiring deep hardware expertise, and eventually, <strong>novel hardware architectures</strong> designed from the ground up for this kind of sequential data flow pattern."</p>
  
  <p><strong>FINAL SUMMARY - GESTURE TO BOTTOM TEXT:</strong></p>
  <p>"Ultimately, it all comes down to this: <strong>The key is understanding when and how to apply these optimizations effectively.</strong> This isn't about always using the most complex approach - it's about making informed engineering trade-offs. When memory access is your bottleneck and you have the expertise to implement it properly, this approach can give you 10 to 100x performance improvements. But if you're working on small models that already fit in cache, or if your team doesn't have hardware optimization experience, the engineering cost may not be worth it."</p>
  
  <p><strong>CLOSING EMPHASIS:</strong></p>
  <p>"It's about getting every last drop of performance out of the hardware <em>when that performance actually matters for your specific use case.</em>"</p>
  
  <p><strong>PAUSE FOR QUESTIONS</strong></p>
</aside>
				<h2>Bottom Line</h2>
				<h3>"This approach is powerful but not a silver bullet"</h3>

				<div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 20px; text-align: left;">
					<div>
						<h4>✅ <strong>Best for</strong>:</h4>
						<ul>
							<li>Medium-to-large models</li>
							<li>Real-time inference</li>
							<li>Embedded systems</li>
							<li>Cache-constrained environments</li>
						</ul>
					</div>

					<div>
						<h4>⚠️ <strong>Challenges</strong>:</h4>
						<ul>
							<li>Large context windows</li>
							<li>Complex implementation</li>
							<li>Platform-specific tuning</li>
						</ul>
					</div>

					<div>
						<h4>🔬 <strong>Future work</strong>:</h4>
						<ul>
							<li>Hardware accelerators (DSA/DMA)</li>
							<li>Automated tools</li>
							<li>Novel architectures</li>
						</ul>
					</div>
				</div>

				<p><strong>The key is understanding when and how to apply these optimizations effectively.</strong></p>
			</section>

			<section style="font-size: 16px;">
				<!-- Universal Computing Constraints: CPU vs GPU Reality - Speaker Notes -->
<aside class="notes">
  <p><strong>TRANSITION CUE:</strong> <em>Move from bottom line to broader context.</em></p>
  
  <p><strong>OPENING:</strong> "Now let's step back and look at the bigger picture. One of the most important insights from this work is that these optimization principles aren't just CPU-specific - they're actually universal computing constraints that affect both CPUs and GPUs. The difference is in how each platform handles these constraints."</p>
  
  <p><strong>GESTURE TO CACHE CAPACITY ROW:</strong></p>
  <p>"Let's start with cache capacity. CPUs have massive L3 caches - up to 504MB on modern Xeon processors. GPUs have much smaller shared memory per streaming multiprocessor - typically 48 to 164KB, with only 6 to 50MB total L2 cache across the entire GPU. But here's the key insight: <strong>sequential memory layout optimizes both</strong>. Whether you have 500MB or 50KB of cache, organizing your data sequentially maximizes your hit rate."</p>
  
  <p><strong>GESTURE TO RESIDUAL CACHE MISSES ROW:</strong></p>
  <p>"Both platforms struggle with residual connections and layer norm operations that need previously computed data. CPUs face Add & Norm cache eviction when intermediate results get pushed out of cache. GPUs face shared memory pressure during attention computation when multiple warps compete for limited shared memory. Our solution - <strong>strategic staging of intermediate results</strong> - works on both platforms by carefully managing what data stays resident."</p>
  
  <p><strong>GESTURE TO MEMORY CAPACITY ROW:</strong></p>
  <p>"This is where we see a fundamental difference. CPUs can scale to 2TB plus of DDR5 memory that's expandable. GPUs are limited to whatever HBM they shipped with - typically 80GB and that's it. Our unified layout approach actually <strong>scales to whatever memory is available</strong>, which means CPUs can handle much larger models than GPUs, but GPUs get better memory bandwidth utilization from their fixed allocation."</p>
  
  <p><strong>GESTURE TO PARALLELIZATION ROW:</strong></p>
  <p>"For parallelization, CPUs deal with NUMA topology and cache coherency across cores - you need to be aware of which memory is local to which CPU socket. GPUs handle thread divergence and occupancy - making sure all threads in a warp are doing useful work. The beautiful thing is that <strong>cache-aware design benefits both</strong> - NUMA-aware allocation improves CPU cache locality, and coalesced memory access patterns improve GPU occupancy."</p>
  
  <p><strong>GESTURE TO RACE CONDITIONS ROW:</strong></p>
  <p>"Finally, race conditions look different but require similar solutions. CPUs need coordination between DSA engines and CPU cores accessing the same memory regions. GPUs need kernel synchronization when multiple kernels access shared global memory. In both cases, the solution is applying universal <strong>hardware coordination principles</strong> - whether that's memory barriers, atomic operations, or hardware monitors."</p>
  
  <p><strong>KEY INSIGHT - PAUSE FOR EMPHASIS:</strong></p>
  <p>"The profound insight here is that <strong>these aren't CPU tricks or GPU tricks - these are fundamental computer architecture principles</strong>. Sequential access beats random access on any hardware. Locality beats distant memory on any platform. The specific numbers change, but the optimization strategies remain universal."</p>
  
  <p><strong>PRACTICAL IMPLICATION:</strong></p>
  <p>"What this means practically is that if you design your memory layout and algorithms correctly, you can write code that performs well on both CPUs and GPUs without platform-specific hacks. The same sequential layout that gives you 95% cache hit rates on CPU also gives you optimal memory coalescing on GPU."</p>
  
  <p><strong>CLOSING:</strong></p>
  <p>"This universality is why these optimization techniques are so powerful - you're not just optimizing for one piece of hardware, you're optimizing for the fundamental realities of how modern computing systems move data around."</p>
</aside>
				<h2>Universal Computing Constraints: CPU vs GPU Reality</h2>
				<table>
					<thead>
						<tr>
							<th>Constraint Category</th>
							<th>CPU Reality</th>
							<th>GPU Reality</th>
							<th>This Approach Benefits Both</th>
						</tr>
					</thead>
					<tbody>
						<tr>
							<td>📦 <strong>Cache Capacity</strong></td>
							<td>L3: 504MB</td>
							<td>Shared Memory: 48-164KB per SM<br>L2: 6-50MB total</td>
							<td>Sequential memory layout optimizes both</td>
						</tr>
						<tr>
							<td>🔄 <strong>Residual Cache Misses</strong></td>
							<td>Add & Norm cache eviction</td>
							<td>Shared memory pressure during attention</td>
							<td>Strategic staging works on both platforms</td>
						</tr>
						<tr>
							<td>💾 <strong>Memory Capacity</strong></td>
							<td>DDR5: 2TB+ expandable</td>
							<td>HBM: 80GB fixed limit</td>
							<td>This unified layout scales to available memory</td>
						</tr>
						<tr>
							<td>⚡ <strong>Parallelization</strong></td>
							<td>NUMA + cache coherency</td>
							<td>Thread divergence + occupancy</td>
							<td>Cache-aware design benefits both</td>
						</tr>
						<tr>
							<td>🏃 <strong>Race Conditions</strong></td>
							<td>DSA/DMA coordination needed</td>
							<td>Kernel synchronization required</td>
							<td>Hardware coordination principles universal</td>
						</tr>
					</tbody>
				</table>
			</section>


		</div>
	</div>

	<script src="../reveal.js/dist/reveal.js"></script>
	<script src="../reveal.js/plugin/zoom/zoom.js"></script>
	<script src="../reveal.js/plugin/notes/notes.js"></script>
	<script src="../reveal.js/plugin/search/search.js"></script>
	<script src="../reveal.js/plugin/markdown/markdown.js"></script>
	<script src="../reveal.js/plugin/highlight/highlight.js"></script>
	<script src="https://cdn.jsdelivr.net/npm/gsap@3.13.0/dist/gsap.min.js"></script>

	<script src="https://cdn.jsdelivr.net/npm/gsap@3.13.0/dist/DrawSVGPlugin.min.js"></script>
	<script src="https://cdn.jsdelivr.net/npm/gsap@3.13.0/dist/MotionPathHelper.min.js"></script>
	<script src="https://cdn.jsdelivr.net/npm/gsap@3.13.0/dist/MotionPathPlugin.min.js"></script>
	<script src="https://cdn.jsdelivr.net/npm/gsap@3.13.0/dist/MorphSVGPlugin.min.js"></script>
	<script src="https://cdn.jsdelivr.net/npm/gsap@3.13.0/dist/Physics2DPlugin.min.js"></script>
	<script src="https://cdn.jsdelivr.net/npm/gsap@3.13.0/dist/SplitText.min.js"></script>
	<script src="https://cdn.jsdelivr.net/npm/gsap@3.13.0/dist/TextPlugin.min.js"></script>

	<!-- RoughEase, ExpoScaleEase and SlowMo are all included in the EasePack file -->
	<script src="https://cdn.jsdelivr.net/npm/gsap@3.13.0/dist/EasePack.min.js"></script>
	<script src="https://cdn.jsdelivr.net/npm/gsap@3.13.0/dist/CustomEase.min.js"></script>
	<script>

		// Also available as an ES module, see:
		// https://revealjs.com/initialization/
		Reveal.initialize({
			controls: true,
			progress: true,
			center: true,
			hash: true,

			// Learn about plugins: https://revealjs.com/plugins/
			plugins: [RevealZoom, RevealNotes, RevealSearch, RevealMarkdown, RevealHighlight]
		});

	</script>
	<script>
		// use a script tag or an external JS file
		document.addEventListener("DOMContentLoaded", (event) => {
			gsap.registerPlugin(DrawSVGPlugin, MotionPathHelper, MotionPathPlugin, MorphSVGPlugin, Physics2DPlugin, SplitText, TextPlugin, SlowMo, CustomEase)
			// gsap code here!
		});

	</script>
	<script>
		gsap.to("#demo-rect", {
			motionPath: "#demo_path",
			duration: 10,
			ease: "none",
			repeat: -1
		});
	</script>

</body>

</html>