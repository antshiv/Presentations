<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title>Deep LLM Layer Architecture</title>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.3.1/reset.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.3.1/reveal.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.3.1/theme/black.min.css" id="theme">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/monokai.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/3.9.1/gsap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.3.1/reveal.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.3.1/plugin/highlight/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.3.1/plugin/markdown/markdown.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.3.1/plugin/notes/notes.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.3.1/plugin/math/math.min.js"></script>

    <style>
        /* General Reveal.js styling */
        .reveal {
            font-family: 'Inter', sans-serif;
            font-size: 30px;
        }
        .reveal h1, .reveal h2, .reveal h3, .reveal h4, .reveal h5, .reveal h6 {
            text-transform: none;
            font-weight: 700;
        }
        .reveal section img {
            margin: 15px 0px;
            background: rgba(255, 255, 255, 0.12);
            border: 4px solid #eee;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.15);
        }
        .reveal .slides section .fragment {
            opacity: 0;
            visibility: hidden;
            transition: opacity 0.8s ease-in-out;
        }
        .reveal .slides section .fragment.visible {
            opacity: 1;
            visibility: visible;
        }
        .reveal p, .reveal ul, .reveal ol {
            font-size: 0.8em;
            line-height: 1.4;
        }

        /* SVG Specific Styles */
        .llm-layer-svg {
            width: 95%; /* Make it take up most of the width */
            height: 95%; /* Make it take up most of the height */
            margin: auto;
            display: block;
        }

        /* Common Box Styles */
        .llm-box {
            fill: #333;
            stroke: #666;
            stroke-width: 1.5;
            rx: 8;
            ry: 8;
        }
        .llm-text {
            fill: #FFF;
            font-size: 11px;
            text-anchor: middle;
        }
        .llm-dim-text {
            fill: #AAA;
            font-size: 9px;
            text-anchor: middle;
        }
        .llm-label {
            fill: #42affa;
            font-size: 14px;
            font-weight: bold;
            text-anchor: middle;
        }

        /* Specific Component Colors */
        .input-output-box { fill: #00BCD4; }
        .layernorm-box { fill: #FFC107; }
        .linear-box { fill: #E91E63; }
        .attention-qk-box { fill: #FFEB3B; }
        .attention-v-box { fill: #8BC34A; }
        .softmax-box { fill: #9C27B0; }
        .add-norm-box { fill: #4CAF50; }
        .mlp-fc1-box { fill: #2196F3; }
        .mlp-gelu-box { fill: #FF5722; }
        .mlp-fc2-box { fill: #673AB7; }

        /* Arrows */
        .arrow-line {
            stroke: #FFF;
            stroke-width: 1.5;
            fill: none;
            marker-end: url(#arrowhead);
        }
        .residual-line {
            stroke: #FFD700; /* Gold color for residual connections */
            stroke-width: 1.5;
            fill: none;
            marker-end: url(#arrowhead);
        }
        .attention-line {
            stroke: #FFC107;
            stroke-width: 1;
            fill: none;
            marker-end: url(#arrowhead-small);
        }

        /* Definitions for arrow markers */
        defs marker {
            overflow: visible;
        }
        #arrowhead {
            fill: #FFF;
            stroke: none;
        }
        #arrowhead-small {
            fill: #FFC107;
            stroke: none;
        }

        /* Memory Layout SVG specific styles */
        .memory-arena {
            fill: #1a1a2e;
            stroke: #4ecdc4;
            stroke-width: 2;
            rx: 10;
            ry: 10;
        }
        .memory-block {
            fill: #333;
            stroke: #666;
            stroke-width: 1;
            rx: 5;
            ry: 5;
        }
        .memory-label {
            fill: #FFF;
            font-size: 10px;
            text-anchor: start;
        }
        .memory-offset {
            fill: #AAA;
            font-size: 8px;
            text-anchor: end;
        }
        .memory-flow-arrow {
            stroke: #FFC107;
            stroke-width: 1.5;
            fill: none;
            marker-end: url(#arrowhead);
        }
        .memory-flow-label {
            fill: #FFC107;
            font-size: 9px;
        }

        /* New styles for Memory Hierarchy and Data Flow */
        .memory-level {
            fill: rgba(255, 255, 255, 0.05);
            stroke: #666;
            stroke-width: 1;
            rx: 8;
            ry: 8;
        }
        .ddr-level { fill: rgba(0, 188, 212, 0.2); } /* Cyan */
        .l3-level { fill: rgba(255, 193, 7, 0.2); } /* Amber */
        .l2-level { fill: rgba(76, 175, 80, 0.2); } /* Green */
        .l1-level { fill: rgba(156, 39, 176, 0.2); } /* Purple */

        .memory-level-label {
            fill: #FFF;
            font-size: 14px;
            font-weight: bold;
            text-anchor: middle;
        }
        .memory-level-info {
            fill: #AAA;
            font-size: 10px;
            text-anchor: middle;
        }

        .cpu-core-box {
            fill: #E91E63; /* Pink */
            stroke: #FFF;
            stroke-width: 1.5;
            rx: 5;
            ry: 5;
        }
        .cpu-core-label {
            fill: #FFF;
            font-size: 12px;
            font-weight: bold;
            text-anchor: middle;
        }
        .cpu-core-simd {
            fill: #EEE;
            font-size: 9px;
            text-anchor: middle;
        }

        .token-slice-data {
            stroke: #FFF;
            stroke-width: 1;
            rx: 3; ry: 3;
        }
        .token-slice-label {
            fill: #000;
            font-size: 9px;
            text-anchor: middle;
        }

        .flow-arrow-mem-hierarchy {
            stroke: #00BFFF;
            stroke-width: 2;
            fill: none;
            marker-end: url(#arrowhead);
        }
        .flow-label-mem-hierarchy {
            fill: #00BFFF;
            font-size: 9px;
        }
        .simd-compute-circle {
            stroke: #FFF;
            stroke-width: 1.5;
        }
        .simd-compute-text {
            fill: #000;
            font-size: 8px;
            font-weight: bold;
        }
    </style>
</head>
<body>
    <div class="reveal">
        <div class="slides">

            <!-- Title Slide -->
            <section>
                <h1>Anatomy of an LLM Layer</h1>
                <h2>Visualizing the Transformer Block</h2>
                <p><small>Understanding the flow of data and computations</small></p>
                <aside class="notes">
                    <h3>Welcome and Introduction</h3>
                    <ul>
                        <li>Welcome everyone to this deep dive into the architecture and optimization of a single layer within a Large Language Model.</li>
                        <li>Today, we'll peel back the layers of abstraction to understand how these powerful models actually process information at a fundamental level.</li>
                        <li>Our focus will be on the core computational blocks and how data flows through them, highlighting the importance of low-level optimizations.</li>
                        <li>This presentation is part of a larger project on building a CPU-optimized LLM runtime from scratch.</li>
                    </ul>
                </aside>
            </section>

            <!-- LLM Layer Diagram Slide -->
            <section>
                <h2>Transformer Layer: From Input to Output</h2>
                <svg class="llm-layer-svg" viewBox="0 0 900 600">
                    <defs>
                        <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="0" refY="3.5" orient="auto">
                            <polygon points="0 0, 10 3.5, 0 7" />
                        </marker>
                        <marker id="arrowhead-small" markerWidth="6" markerHeight="4" refX="0" refY="2" orient="auto">
                            <polygon points="0 0, 6 2, 0 4" />
                        </marker>
                    </defs>

                    <!-- Dimensions -->
                    <text x="750" y="30" class="llm-dim-text">D_embed = 8192</text>
                    <text x="750" y="45" class="llm-dim-text">N_ctx = 4096</text>
                    <text x="750" y="60" class="llm-dim-text">Num_Heads = 128</text>
                    <text x="750" y="75" class="llm-dim-text">Head_Dim = 64</text>

                    <!-- Input -->
                    <rect x="50" y="100" width="80" height="40" class="llm-box input-output-box" id="input-box"/>
                    <text x="90" y="125" class="llm-text">Input</text>
                    <text x="90" y="140" class="llm-dim-text">($N_{ctx} \times D_{embed}$)</text>
                    <path d="M130 120 H180" class="arrow-line"/>

                    <!-- LayerNorm 1 -->
                    <rect x="180" y="100" width="80" height="40" class="llm-box layernorm-box" id="ln1-box"/>
                    <text x="220" y="125" class="llm-text">LayerNorm</text>
                    <text x="220" y="140" class="llm-dim-text">($N_{ctx} \times D_{embed}$)</text>
                    <path d="M260 120 H310" class="arrow-line"/>

                    <!-- Multi-Head Attention Block -->
                    <rect x="310" y="90" width="380" height="200" class="llm-box" fill="rgba(0,0,0,0.2)" stroke="#FFD700" stroke-dasharray="5 5"/>
                    <text x="500" y="80" class="llm-label">Multi-Head Attention</text>

                        <!-- Linear Q, K, V Projections -->
                        <rect x="320" y="110" width="60" height="30" class="llm-box linear-box"/>
                        <text x="350" y="128" class="llm-text">Linear Q</text>
                        <text x="350" y="140" class="llm-dim-text">($D_{embed} \times D_{embed}$)</text>

                        <rect x="320" y="150" width="60" height="30" class="llm-box linear-box"/>
                        <text x="350" y="168" class="llm-text">Linear K</text>
                        <text x="350" y="180" class="llm-dim-text">($D_{embed} \times D_{embed}$)</text>

                        <rect x="320" y="190" width="60" height="30" class="llm-box linear-box"/>
                        <text x="350" y="208" class="llm-text">Linear V</text>
                        <text x="350" y="220" class="llm-dim-text">($D_{embed} \times D_{embed}$)</text>

                        <!-- Input to Q,K,V projections -->
                        <path d="M310 120 L315 120 V125 H320" class="attention-line"/>
                        <path d="M310 120 L315 120 V165 H320" class="attention-line"/>
                        <path d="M310 120 L315 120 V205 H320" class="attention-line"/>

                        <!-- Q, K, V Matrices -->
                        <rect x="390" y="110" width="50" height="30" class="llm-box attention-qk-box"/>
                        <text x="415" y="128" class="llm-text">Q</text>
                        <text x="415" y="140" class="llm-dim-text">($N_{ctx} \times D_{embed}$)</text>

                        <rect x="390" y="150" width="50" height="30" class="llm-box attention-qk-box"/>
                        <text x="415" y="168" class="llm-text">K</text>
                        <text x="415" y="180" class="llm-dim-text">($N_{ctx} \times D_{embed}$)</text>

                        <rect x="390" y="190" width="50" height="30" class="llm-box attention-v-box"/>
                        <text x="415" y="208" class="llm-text">V</text>
                        <text x="415" y="220" class="llm-dim-text">($N_{ctx} \times D_{embed}$)</text>

                        <!-- Q x K.T -->
                        <path d="M440 125 H470" class="attention-line"/>
                        <path d="M440 165 H470" class="attention-line"/>
                        <text x="455" y="145" class="llm-text" font-size="16">×</text>
                        <text x="455" y="175" class="llm-text" font-size="8">K.T</text>

                        <rect x="480" y="130" width="60" height="30" class="llm-box attention-qk-box"/>
                        <text x="510" y="148" class="llm-text">Scores</text>
                        <text x="510" y="160" class="llm-dim-text">($N_{ctx} \times N_{ctx}$)</text>
                        <path d="M540 145 H570" class="attention-line"/>

                        <!-- Softmax -->
                        <rect x="570" y="130" width="60" height="30" class="llm-box softmax-box"/>
                        <text x="600" y="148" class="llm-text">Softmax</text>
                        <text x="600" y="160" class="llm-dim-text">($N_{ctx} \times N_{ctx}$)</text>
                        <path d="M630 145 H660" class="attention-line"/>

                        <!-- Softmax Output x V -->
                        <text x="675" y="160" class="llm-text" font-size="16">×</text>
                        <path d="M440 205 L660 205 V170" class="attention-line"/> <!-- V to multiplication -->

                        <rect x="680" y="130" width="60" height="30" class="llm-box linear-box"/>
                        <text x="710" y="148" class="llm-text">Proj</text>
                        <text x="710" y="160" class="llm-dim-text">($D_{embed} \times D_{embed}$)</text>
                        <path d="M740 145 H770" class="arrow-line"/>

                        <!-- Concatenation & Final Linear Projection (Implicit in Proj) -->
                        <text x="600" y="270" class="llm-dim-text">Multi-Head Output: ($N_{ctx} \times D_{embed}$)</text>
                    
                    <!-- Add & Norm 1 -->
                    <rect x="770" y="130" width="80" height="40" class="llm-box add-norm-box" id="addnorm1-box"/>
                    <text x="810" y="155" class="llm-text">Add & Norm</text>
                    <text x="810" y="170" class="llm-dim-text">($N_{ctx} \times D_{embed}$)</text>
                    <path d="M850 150 H880" class="arrow-line"/>

                    <!-- Residual connection 1 -->
                    <path d="M90 120 H100 V200 H780 V150" class="residual-line"/>

                    <!-- MLP Block -->
                    <rect x="180" y="320" width="500" height="100" class="llm-box" fill="rgba(0,0,0,0.2)" stroke="#FFD700" stroke-dasharray="5 5"/>
                    <text x="430" y="310" class="llm-label">Feed-Forward Network (MLP)</text>

                        <!-- Input to MLP -->
                        <path d="M880 150 L880 350 H180" class="arrow-line"/>

                        <!-- FC1 -->
                        <rect x="200" y="350" width="80" height="40" class="llm-box mlp-fc1-box"/>
                        <text x="240" y="375" class="llm-text">FC1</text>
                        <text x="240" y="390" class="llm-dim-text">($D_{embed} \times 4D_{embed}$)</text>
                        <path d="M280 370 H310" class="arrow-line"/>

                        <!-- GELU -->
                        <rect x="310" y="350" width="80" height="40" class="llm-box mlp-gelu-box"/>
                        <text x="350" y="375" class="llm-text">GELU</text>
                        <text x="350" y="390" class="llm-dim-text">($N_{ctx} \times 4D_{embed}$)</text>
                        <path d="M390 370 H420" class="arrow-line"/>

                        <!-- FC2 -->
                        <rect x="420" y="350" width="80" height="40" class="llm-box mlp-fc2-box"/>
                        <text x="460" y="375" class="llm-text">FC2</text>
                        <text x="460" y="390" class="llm-dim-text">($4D_{embed} \times D_{embed}$)</text>
                        <path d="M500 370 H530" class="arrow-line"/>

                    <!-- Add & Norm 2 -->
                    <rect x="530" y="350" width="80" height="40" class="llm-box add-norm-box" id="addnorm2-box"/>
                    <text x="570" y="375" class="llm-text">Add & Norm</text>
                    <text x="570" y="390" class="llm-dim-text">($N_{ctx} \times D_{embed}$)</text>
                    <path d="M610 370 H640" class="arrow-line"/>

                    <!-- Residual connection 2 -->
                    <path d="M880 150 L880 350 L620 350 V370" class="residual-line"/>

                    <!-- Output -->
                    <rect x="640" y="350" width="80" height="40" class="llm-box input-output-box" id="output-box"/>
                    <text x="680" y="375" class="llm-text">Output</text>
                    <text x="680" y="390" class="llm-dim-text">($N_{ctx} \times D_{embed}$)</text>
                </svg>
                <aside class="notes">
                    <h3>Transformer Layer Overview</h3>
                    <ul>
                        <li>This slide provides a high-level overview of a single Transformer layer, which is the fundamental building block of modern LLMs.</li>
                        <li>We start with the input, typically a sequence of tokens represented as embeddings.</li>
                        <li>The data then flows through two main sub-layers: the Multi-Head Attention mechanism and the Feed-Forward Network (or MLP).</li>
                        <li>Crucially, each sub-layer is followed by a "Residual Connection" (Add & Norm), which helps with training stability and allows the network to be very deep.</li>
                        <li>We've also indicated the typical dimensions for each major block, using D_embed for embedding dimension and N_ctx for context window size.</li>
                        <li>In the following slides, we'll dive into each of these components in more detail.</li>
                    </ul>
                </aside>
            </section>

            <!-- Memory Layout of a Transformer Layer Slide -->
            <section>
                <h2>Memory Layout of a Transformer Layer</h2>
                <h3>Contiguous Allocation for Optimal Performance</h3>
                <svg class="llm-layer-svg" viewBox="0 0 900 600">
                    <defs>
                        <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="0" refY="3.5" orient="auto">
                            <polygon points="0 0, 10 3.5, 0 7" />
                        </marker>
                        <marker id="mem-arrowhead" markerWidth="10" markerHeight="7" refX="0" refY="3.5" orient="auto">
                            <polygon points="0 0, 10 3.5, 0 7" fill="#FFC107" />
                        </marker>
                    </defs>

                    <!-- Memory Arena Outline -->
                    <rect x="50" y="50" width="200" height="500" class="memory-arena"/>
                    <text x="150" y="35" class="llm-label">Memory Arena (Single Block)</text>
                    <text x="150" y="560" class="llm-dim-text">Total Floats: ~500 GiB</text>

                    <!-- Memory Blocks (Offsets) -->
                    <g class="fragment fade-in">
                        <rect x="60" y="60" width="180" height="40" class="memory-block" fill="#00BCD4"/>
                        <text x="65" y="75" class="memory-label">Input (Activations)</text>
                        <text x="230" y="85" class="memory-offset">Offset 0</text>
                        <text x="150" y="95" class="llm-dim-text">($N_{ctx} \times D_{embed}$)</text>
                    </g>

                    <g class="fragment fade-in">
                        <rect x="60" y="110" width="180" height="20" class="memory-block" fill="#FFC107"/>
                        <text x="65" y="125" class="memory-label">LN1 Weights</text>
                        <text x="230" y="125" class="memory-offset">Offset 1</text>
                        <text x="150" y="135" class="llm-dim-text">($D_{embed}$)</text>
                    </g>
                    <g class="fragment fade-in">
                        <rect x="60" y="135" width="180" height="20" class="memory-block" fill="#FFC107"/>
                        <text x="65" y="150" class="memory-label">LN1 Bias</text>
                        <text x="230" y="150" class="memory-offset">Offset 2</text>
                        <text x="150" y="160" class="llm-dim-text">($D_{embed}$)</text>
                    </g>

                    <g class="fragment fade-in">
                        <rect x="60" y="170" width="180" height="60" class="memory-block" fill="#E91E63"/>
                        <text x="65" y="185" class="memory-label">QKV Weights</text>
                        <text x="230" y="200" class="memory-offset">Offset 3</text>
                        <text x="150" y="220" class="llm-dim-text">($3D_{embed} \times D_{embed}$)</text>
                    </g>
                    <g class="fragment fade-in">
                        <rect x="60" y="235" width="180" height="30" class="memory-block" fill="#E91E63"/>
                        <text x="65" y="250" class="memory-label">QKV Bias</text>
                        <text x="230" y="250" class="memory-offset">Offset 4</text>
                        <text x="150" y="260" class="llm-dim-text">($3D_{embed}$)</text>
                    </g>

                    <g class="fragment fade-in">
                        <rect x="60" y="275" width="180" height="40" class="memory-block" fill="#FFEB3B"/>
                        <text x="65" y="290" class="memory-label">Attention Output</text>
                        <text x="230" y="290" class="memory-offset">Offset 5</text>
                        <text x="150" y="305" class="llm-dim-text">($N_{ctx} \times D_{embed}$)</text>
                    </g>
                    
                    <g class="fragment fade-in">
                        <rect x="60" y="325" width="180" height="20" class="memory-block" fill="#4CAF50"/>
                        <text x="65" y="340" class="memory-label">LN2 Weights</text>
                        <text x="230" y="340" class="memory-offset">Offset 6</text>
                        <text x="150" y="350" class="llm-dim-text">($D_{embed}$)</text>
                    </g>
                    <g class="fragment fade-in">
                        <rect x="60" y="350" width="180" height="20" class="memory-block" fill="#4CAF50"/>
                        <text x="65" y="365" class="memory-label">LN2 Bias</text>
                        <text x="230" y="365" class="memory-offset">Offset 7</text>
                        <text x="150" y="375" class="llm-dim-text">($D_{embed}$)</text>
                    </g>

                    <g class="fragment fade-in">
                        <rect x="60" y="385" width="180" height="60" class="memory-block" fill="#2196F3"/>
                        <text x="65" y="400" class="memory-label">FC1 Weights</text>
                        <text x="230" y="415" class="memory-offset">Offset 8</text>
                        <text x="150" y="430" class="llm-dim-text">($4D_{embed} \times D_{embed}$)</text>
                    </g>
                    <g class="fragment fade-in">
                        <rect x="60" y="450" width="180" height="30" class="memory-block" fill="#2196F3"/>
                        <text x="65" y="465" class="memory-label">FC1 Bias</text>
                        <text x="230" y="465" class="memory-offset">Offset 9</text>
                        <text x="150" y="475" class="llm-dim-text">($4D_{embed}$)</text>
                    </g>

                    <g class="fragment fade-in">
                        <rect x="60" y="490" width="180" height="40" class="memory-block" fill="#673AB7"/>
                        <text x="65" y="505" class="memory-label">MLP Output</text>
                        <text x="230" y="505" class="memory-offset">Offset 10</text>
                        <text x="150" y="520" class="llm-dim-text">($N_{ctx} \times D_{embed}$)</text>
                    </g>


                    <!-- Data Flow Arrows (simplified for this diagram) -->
                    <g class="fragment fade-in">
                        <path d="M250 80 H350 L350 120 L310 120" class="memory-flow-arrow"/>
                        <text x="300" y="70" class="memory-flow-label">Input to LN1</text>
                    </g>

                    <g class="fragment fade-in">
                        <path d="M250 120 H310" class="memory-flow-arrow"/>
                        <text x="280" y="110" class="memory-flow-label">LN1 Output</text>
                    </g>

                    <g class="fragment fade-in">
                        <path d="M250 200 H310" class="memory-flow-arrow"/>
                        <text x="280" y="190" class="memory-flow-label">QKV Output</text>
                    </g>
                    
                    <g class="fragment fade-in">
                        <path d="M250 290 H350 L350 370 L530 370" class="memory-flow-arrow"/>
                        <text x="300" y="280" class="memory-flow-label">Attention Output to MLP</text>
                    </g>

                    <g class="fragment fade-in">
                        <path d="M250 490 H350 L350 450 L640 450" class="memory-flow-arrow"/>
                        <text x="300" y="480" class="memory-flow-label">MLP Output to Final</text>
                    </g>

                    <g class="fragment fade-in">
                        <text x="450" y="560" class="llm-dim-text">Key Features: Bump Allocation, Zero Fragmentation, Cache Alignment</text>
                    </g>

                </svg>
                <aside class="notes">
                    <h3>Memory Layout Details</h3>
                    <ul>
                        <li>This diagram illustrates our approach to memory management within the LLM runtime.</li>
                        <li>We allocate a single, large, contiguous block of memory (the "Memory Arena") for an entire Transformer layer.</li>
                        <li>All weights, biases, and intermediate activations for a layer are laid out sequentially within this single block.</li>
                        <li>This "bump allocation" strategy minimizes memory fragmentation and improves cache locality significantly.</li>
                        <li>We ensure 64-byte alignment for all tensors, which is critical for optimal performance with modern CPU vector instructions like AVX-512.</li>
                        <li>The offsets shown represent the starting points of different data structures within this contiguous memory block. This design avoids costly dynamic memory allocations during runtime and allows for predictable memory access patterns.</li>
                    </ul>
                </aside>
            </section>

            <!-- Layer Normalization & Add & Norm Slide -->
            <section>
                <h2>Layer Normalization & Residual Connections</h2>
                <h3>Stabilizing Activations and Preserving Information</h3>
                <svg class="llm-layer-svg" viewBox="0 0 900 600">
                    <defs>
                        <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="0" refY="3.5" orient="auto">
                            <polygon points="0 0, 10 3.5, 0 7" />
                        </marker>
                    </defs>

                    <!-- Dimensions -->
                    <text x="750" y="30" class="llm-dim-text">D_embed = 8192</text>
                    <text x="750" y="45" class="llm-dim-text">N_ctx = 4096</text>

                    <!-- LayerNorm Section -->
                    <text x="250" y="80" class="llm-label">Layer Normalization</text>
                    <rect x="100" y="100" width="100" height="50" class="llm-box input-output-box"/>
                    <text x="150" y="125" class="llm-text">Input X</text>
                    <text x="150" y="140" class="llm-dim-text">($N_{ctx} \times D_{embed}$)</text>
                    <path d="M200 125 H250" class="arrow-line"/>

                    <rect x="250" y="100" width="100" height="50" class="llm-box layernorm-box"/>
                    <text x="300" y="125" class="llm-text">LayerNorm</text>
                    <text x="300" y="140" class="llm-dim-text">($\mu, \sigma$)</text>
                    <path d="M350 125 H400" class="arrow-line"/>

                    <rect x="400" y="100" width="100" height="50" class="llm-box input-output-box"/>
                    <text x="450" y="125" class="llm-text">Output X'</text>
                    <text x="450" y="140" class="llm-dim-text">($N_{ctx} \times D_{embed}$)</text>

                    <text x="300" y="170" class="llm-dim-text">Normalizes activations across the feature dimension for each sample.</text>

                    <!-- Add & Norm Section -->
                    <text x="450" y="250" class="llm-label">Add & Norm (Residual Connection)</text>

                    <!-- Input to Add & Norm -->
                    <rect x="100" y="300" width="100" height="50" class="llm-box input-output-box"/>
                    <text x="150" y="325" class="llm-text">Input X</text>
                    <text x="150" y="340" class="llm-dim-text">($N_{ctx} \times D_{embed}$)</text>
                    
                    <!-- Sub-layer Output -->
                    <rect x="300" y="300" width="100" height="50" class="llm-box linear-box"/>
                    <text x="350" y="325" class="llm-text">Sub-layer</text>
                    <text x="350" y="340" class="llm-dim-text">Output F(X)</text>
                    <path d="M200 325 H250" class="arrow-line"/>
                    <path d="M400 325 H450" class="arrow-line"/>

                    <!-- Add Operation -->
                    <circle cx="275" cy="325" r="15" fill="#FF5722"/>
                    <text x="275" y="330" class="llm-text" font-size="18">+</text>
                    <path d="M150 350 V370 H275" class="residual-line"/> <!-- Residual from Input X -->

                    <!-- LayerNorm after Add -->
                    <rect x="450" y="300" width="100" height="50" class="llm-box add-norm-box"/>
                    <text x="500" y="325" class="llm-text">Add & Norm</text>
                    <text x="500" y="340" class="llm-dim-text">($N_{ctx} \times D_{embed}$)</text>
                    <path d="M550 325 H600" class="arrow-line"/>

                    <!-- Output of Add & Norm -->
                    <rect x="600" y="300" width="100" height="50" class="llm-box input-output-box"/>
                    <text x="650" y="325" class="llm-text">Output</text>
                    <text x="650" y="340" class="llm-dim-text">($N_{ctx} \times D_{embed}$)</text>

                    <text x="450" y="370" class="llm-dim-text">Adds input to sub-layer output, then normalizes. Key for deep networks.</text>

                </svg>
                <aside class="notes">
                    <h3>Layer Normalization</h3>
                    <ul>
                        <li>Layer Normalization is a crucial component for stabilizing the training of deep neural networks like Transformers.</li>
                        <li>It normalizes the activations across the feature dimension for each individual sample (token). This means it calculates the mean and variance of all elements within a single token's embedding vector.</li>
                        <li>This helps prevent exploding or vanishing gradients and allows for higher learning rates.</li>
                        <li>The dimensions remain the same: ($N_{ctx} \times D_{embed}$).</li>
                    </ul>
                    <h3>Add & Norm (Residual Connections)</h3>
                    <ul>
                        <li>The "Add & Norm" block combines two important concepts: residual connections and layer normalization.</li>
                        <li>A residual connection (or skip connection) takes the input to a sub-layer and adds it directly to the output of that sub-layer. This helps gradients flow more easily through the network, mitigating the vanishing gradient problem in very deep models.</li>
                        <li>After the addition, another Layer Normalization is applied.</li>
                        <li>This pattern is used twice within a Transformer layer: once after Multi-Head Attention and once after the Feed-Forward Network.</li>
                        <li>It ensures that the network can learn residual functions, making it easier to optimize identity mappings.</li>
                    </ul>
                </aside>
            </section>

            <!-- Multi-Head Attention Deep Dive Slide -->
            <section>
                <h2>Multi-Head Attention: QKV, Scores & Projection</h2>
                <h3>The Core of Transformer's Power</h3>
                <svg class="llm-layer-svg" viewBox="0 0 900 600">
                    <defs>
                        <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="0" refY="3.5" orient="auto">
                            <polygon points="0 0, 10 3.5, 0 7" fill="#FFF" />
                        </marker>
                        <marker id="attention-arrowhead" markerWidth="10" markerHeight="7" refX="0" refY="3.5" orient="auto">
                            <polygon points="0 0, 10 3.5, 0 7" fill="#FFC107" />
                        </marker>
                    </defs>

                    <!-- Input X -->
                    <rect x="50" y="100" width="80" height="50" class="llm-box input-output-box"/>
                    <text x="90" y="125" class="llm-text">Input X</text>
                    <text x="90" y="140" class="llm-dim-text">($N_{ctx} \times D_{embed}$)</text>
                    <path d="M130 125 H180" class="arrow-line"/>

                    <!-- Linear Projections for Q, K, V -->
                    <rect x="180" y="70" width="80" height="40" class="llm-box linear-box"/>
                    <text x="220" y="95" class="llm-text">Linear Q</text>
                    <text x="220" y="110" class="llm-dim-text">($D_{embed} \times D_{head}$)</text>
                    <path d="M180 125 V90 H180" class="attention-line"/>

                    <rect x="180" y="130" width="80" height="40" class="llm-box linear-box"/>
                    <text x="220" y="155" class="llm-text">Linear K</text>
                    <text x="220" y="170" class="llm-dim-text">($D_{embed} \times D_{head}$)</text>
                    <path d="M180 125 V150 H180" class="attention-line"/>

                    <rect x="180" y="190" width="80" height="40" class="llm-box linear-box"/>
                    <text x="220" y="215" class="llm-text">Linear V</text>
                    <text x="220" y="230" class="llm-dim-text">($D_{embed} \times D_{head}$)</text>
                    <path d="M180 125 V210 H180" class="attention-line"/>

                    <!-- Q, K, V Matrices (Output of Linear Projections) -->
                    <rect x="270" y="80" width="60" height="30" class="llm-box attention-qk-box"/>
                    <text x="300" y="98" class="llm-text">Q</text>
                    <text x="300" y="110" class="llm-dim-text">($N_{ctx} \times D_{head}$)</text>
                    <path d="M260 90 H270" class="attention-line"/>

                    <rect x="270" y="140" width="60" height="30" class="llm-box attention-qk-box"/>
                    <text x="300" y="158" class="llm-text">K</text>
                    <text x="300" y="170" class="llm-dim-text">($N_{ctx} \times D_{head}$)</text>
                    <path d="M260 150 H270" class="attention-line"/>

                    <rect x="270" y="200" width="60" height="30" class="llm-box attention-v-box"/>
                    <text x="300" y="218" class="llm-text">V</text>
                    <text x="300" y="230" class="llm-dim-text">($N_{ctx} \times D_{head}$)</text>
                    <path d="M260 210 H270" class="attention-line"/>

                    <!-- Q x K.T (Scores) -->
                    <path d="M330 95 H360" class="attention-line"/>
                    <path d="M330 155 H360" class="attention-line"/>
                    <text x="345" y="125" class="llm-text" font-size="16">×</text>
                    <text x="345" y="140" class="llm-text" font-size="8">K.T</text>

                    <rect x="370" y="110" width="80" height="40" class="llm-box attention-qk-box"/>
                    <text x="410" y="135" class="llm-text">Scores</text>
                    <text x="410" y="150" class="llm-dim-text">($N_{ctx} \times N_{ctx}$)</text>
                    <path d="M450 130 H480" class="attention-line"/>

                    <!-- Softmax -->
                    <rect x="480" y="110" width="80" height="40" class="llm-box softmax-box"/>
                    <text x="520" y="135" class="llm-text">Softmax</text>
                    <text x="520" y="150" class="llm-dim-text">($N_{ctx} \times N_{ctx}$)</text>
                    <path d="M560 130 H590" class="attention-line"/>

                    <!-- Softmax Output x V -->
                    <text x="605" y="145" class="llm-text" font-size="16">×</text>
                    <path d="M330 215 L590 215 V150" class="attention-line"/> <!-- V to multiplication -->

                    <rect x="620" y="110" width="80" height="40" class="llm-box linear-box"/>
                    <text x="660" y="135" class="llm-text">Output</text>
                    <text x="660" y="150" class="llm-dim-text">($N_{ctx} \times D_{head}$)</text>
                    <path d="M700 130 H730" class="arrow-line"/>

                    <!-- Multi-Head Concatenation (Conceptual) -->
                    <rect x="730" y="110" width="80" height="40" class="llm-box input-output-box"/>
                    <text x="770" y="135" class="llm-text">Concat Heads</text>
                    <text x="770" y="150" class="llm-dim-text">($N_{ctx} \times D_{embed}$)</text>
                    <path d="M810 130 H840" class="arrow-line"/>

                    <!-- Final Linear Projection (Output of Multi-Head Attention) -->
                    <rect x="840" y="110" width="80" height="40" class="llm-box linear-box"/>
                    <text x="880" y="135" class="llm-text">Linear Proj</text>
                    <text x="880" y="150" class="llm-dim-text">($D_{embed} \times D_{embed}$)</text>
                    <path d="M920 130 H950" class="arrow-line"/>

                    <text x="450" y="270" class="llm-dim-text">Each head processes independently, then outputs are concatenated and linearly transformed.</text>
                </svg>
                <aside class="notes">
                    <h3>Multi-Head Attention Deep Dive</h3>
                    <ul>
                        <li>Multi-Head Attention is the heart of the Transformer. It allows the model to weigh the importance of different parts of the input sequence when processing each token.</li>
                        <li>**Linear Projections (Q, K, V):** The input `X` is first linearly transformed into three different matrices: Query (Q), Key (K), and Value (V). These are typically dense matrix multiplications. The dimensions here are `D_embed` to `D_head` (embedding dimension to head dimension).</li>
                        <li>**Scores (Q * K.T):** The Query matrix is multiplied by the transpose of the Key matrix. This dot product calculates the "attention scores" between each token in the sequence and every other token. The output dimension is `N_ctx x N_ctx`.</li>
                        <li>**Softmax:** A softmax function is applied to the scores. This normalizes the scores into probability distributions, indicating how much attention each token should pay to other tokens.</li>
                        <li>**Weighted Sum (Softmax Output * V):** The softmax output (attention weights) is then multiplied by the Value matrix. This is where the actual "information" from the relevant parts of the sequence is aggregated based on the calculated attention weights.</li>
                        <li>**Concatenation & Linear Projection:** Since we have multiple "heads" (e.g., 128 heads), each producing its own output, these outputs are concatenated back together along the embedding dimension. A final linear projection then transforms this concatenated output back to the original `D_embed` dimension.</li>
                        <li>This parallel processing across multiple heads allows the model to learn different types of relationships and dependencies within the sequence simultaneously.</li>
                    </ul>
                </aside>
            </section>

            <!-- Feed-Forward Network (MLP) Deep Dive Slide -->
            <section>
                <h2>Feed-Forward Network (MLP)</h2>
                <h3>Non-linear Transformation for Feature Learning</h3>
                <svg class="llm-layer-svg" viewBox="0 0 900 600">
                    <defs>
                        <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="0" refY="3.5" orient="auto">
                            <polygon points="0 0, 10 3.5, 0 7" />
                        </marker>
                    </defs>

                    <!-- Dimensions -->
                    <text x="750" y="30" class="llm-dim-text">D_embed = 8192</text>
                    <text x="750" y="45" class="llm-dim-text">N_ctx = 4096</text>

                    <!-- Input to MLP -->
                    <rect x="50" y="150" width="100" height="50" class="llm-box input-output-box"/>
                    <text x="100" y="175" class="llm-text">Input X</text>
                    <text x="100" y="190" class="llm-dim-text">($N_{ctx} \times D_{embed}$)</text>
                    <path d="M150 175 H200" class="arrow-line"/>

                    <!-- FC1 (Linear Layer 1) -->
                    <rect x="200" y="150" width="100" height="50" class="llm-box mlp-fc1-box"/>
                    <text x="250" y="175" class="llm-text">FC1</text>
                    <text x="250" y="190" class="llm-dim-text">($D_{embed} \times 4D_{embed}$)</text>
                    <path d="M300 175 H350" class="arrow-line"/>

                    <!-- GELU (Activation Function) -->
                    <rect x="350" y="150" width="100" height="50" class="llm-box mlp-gelu-box"/>
                    <text x="400" y="175" class="llm-text">GELU</text>
                    <text x="400" y="190" class="llm-dim-text">($N_{ctx} \times 4D_{embed}$)</text>
                    <path d="M450 175 H500" class="arrow-line"/>

                    <!-- FC2 (Linear Layer 2) -->
                    <rect x="500" y="150" width="100" height="50" class="llm-box mlp-fc2-box"/>
                    <text x="550" y="175" class="llm-text">FC2</text>
                    <text x="550" y="190" class="llm-dim-text">($4D_{embed} \times D_{embed}$)</text>
                    <path d="M600 175 H650" class="arrow-line"/>

                    <!-- Output of MLP -->
                    <rect x="650" y="150" width="100" height="50" class="llm-box input-output-box"/>
                    <text x="700" y="175" class="llm-text">Output</text>
                    <text x="700" y="190" class="llm-dim-text">($N_{ctx} \times D_{embed}$)</text>

                    <text x="450" y="250" class="llm-dim-text">The MLP expands the dimensionality, applies non-linearity, then projects back.</text>
                    <text x="450" y="270" class="llm-dim-text">This block allows the model to learn complex patterns.</text>
                </svg>
                <aside class="notes">
                    <h3>Feed-Forward Network (MLP)</h3>
                    <ul>
                        <li>The Feed-Forward Network, or MLP, is the second major sub-layer in a Transformer block.</li>
                        <li>It consists of two linear transformations with a non-linear activation function (typically GELU or ReLU) in between.</li>
                        <li>**FC1 (First Fully Connected Layer):** This layer expands the dimensionality of the input. For example, it might project the `D_embed` dimension to `4 * D_embed`. This allows the model to process information in a higher-dimensional space.</li>
                        <li>**GELU (Gaussian Error Linear Unit):** This is a non-linear activation function applied element-wise. Non-linearities are crucial for neural networks to learn complex, non-linear relationships in data. Without them, the network would only be able to learn linear transformations.</li>
                        <li>**FC2 (Second Fully Connected Layer):** This layer projects the expanded dimensionality back down to the original `D_embed` dimension.</li>
                        <li>The MLP's role is to independently process each position (token) in the sequence, allowing the model to learn complex patterns and features that are not necessarily dependent on other tokens in the sequence (unlike attention).</li>
                    </ul>
                </aside>
            </section>

            <!-- Token Streaming & SIMD Compute Slide -->
            <section>
                <h2>Token Streaming & SIMD Compute: The Data Journey</h2>
                <h3>From DDR to CPU: How a single layer processes tokens</h3>
                <svg class="llm-layer-svg" viewBox="0 0 850 550"> <!-- Adjusted viewBox -->
                    <defs>
                        <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="0" refY="3.5" orient="auto">
                            <polygon points="0 0, 10 3.5, 0 7" />
                        </marker>
                    </defs>

                    <!-- Memory Hierarchy Layers -->
                    <g class="fragment fade-in">
                        <rect class="memory-level ddr-level" x="50" y="450" width="750" height="80" />
                        <text class="memory-level-label" x="425" y="475">DDR (Main Memory)</text>
                        <text class="memory-level-info" x="425" y="495">Layer Input, Weights, Activations</text>
                    </g>

                    <g class="fragment fade-in">
                        <rect class="memory-level l3-level" x="100" y="350" width="650" height="80" />
                        <text class="memory-level-label" x="425" y="375">L3 Cache (Shared)</text>
                        <text class="memory-level-info" x="425" y="395">Large, shared data blocks</text>
                    </g>

                    <!-- L2 Caches (per core/socket, simplified to per core for visual) -->
                    <g class="l2-caches fragment fade-in">
                        <rect class="memory-level l2-level" x="150" y="250" width="150" height="80" />
                        <text class="memory-level-label" x="225" y="275">L2 Cache (Core 0)</text>
                        <text class="memory-level-info" x="225" y="295">Local to Core Group</text>

                        <rect class="memory-level l2-level" x="350" y="250" width="150" height="80" />
                        <text class="memory-level-label" x="425" y="275">L2 Cache (Core 1)</text>
                        <text class="memory-level-info" x="425" y="295">Local to Core Group</text>

                        <text class="memory-level-label" x="575" y="285">...</text>

                        <rect class="memory-level l2-level" x="650" y="250" width="150" height="80" />
                        <text class="memory-level-label" x="725" y="275">L2 Cache (Core N)</text>
                        <text class="memory-level-info" x="725" y="295">Local to Core Group</text>
                    </g>

                    <!-- L1 Caches (per core) -->
                    <g class="l1-caches fragment fade-in">
                        <rect class="memory-level l1-level" x="180" y="150" width="90" height="60" />
                        <text class="memory-level-label" x="225" y="175">L1 Cache (Core 0)</text>
                        <text class="memory-level-info" x="225" y="190">Fastest, per core</text>

                        <rect class="memory-level l1-level" x="380" y="150" width="90" height="60" />
                        <text class="memory-level-label" x="425" y="175">L1 Cache (Core 1)</text>
                        <text class="memory-level-info" x="425" y="190">Fastest, per core</text>

                        <text class="memory-level-label" x="575" y="180">...</text>

                        <rect class="memory-level l1-level" x="680" y="150" width="90" height="60" />
                        <text class="memory-level-label" x="725" y="175">L1 Cache (Core N)</text>
                        <text class="memory-level-info" x="725" y="190">Fastest, per core</text>
                    </g>

                    <!-- CPU Cores / SIMD Units -->
                    <g class="cpu-cores fragment fade-in">
                        <rect class="cpu-core-box" x="195" y="50" width="60" height="60" />
                        <text class="cpu-core-label" x="225" y="75">Core 0</text>
                        <text class="cpu-core-simd" x="225" y="95">SIMD</text>

                        <rect class="cpu-core-box" x="395" y="50" width="60" height="60" />
                        <text class="cpu-core-label" x="425" y="75">Core 1</text>
                        <text class="cpu-core-simd" x="425" y="95">SIMD</text>

                        <text class="cpu-core-label" x="575" y="80">...</text>

                        <rect class="cpu-core-box" x="695" y="50" width="60" height="60" />
                        <text class="cpu-core-label" x="725" y="75">Core N</text>
                        <text class="cpu-core-simd" x="725" y="95">SIMD</text>
                    </g>

                    <!-- Data Flow for a single token slice (Core 0) -->
                    <g class="data-flow-core0 fragment fade-in">
                        <!-- Token Slice (start at DDR) -->
                        <rect class="token-slice-data" x="400" y="450" width="50" height="20" fill="#00BCD4" />
                        <text class="token-slice-label" x="425" y="465">Token Slice</text>

                        <!-- DDR to L3 -->
                        <path class="flow-arrow-mem-hierarchy" d="M425 445 V430" />
                        <text class="flow-label-mem-hierarchy" x="430" y="435">Data Fetch</text>

                        <!-- L3 to L2 (Core 0) -->
                        <path class="flow-arrow-mem-hierarchy" d="M425 350 V330 H225 V310" />
                        <text class="flow-label-mem-hierarchy" x="350" y="340">L3 to L2</text>

                        <!-- L2 to L1 (Core 0) -->
                        <path class="flow-arrow-mem-hierarchy" d="M225 250 V210" />
                        <text class="flow-label-mem-hierarchy" x="230" y="220">L2 to L1</text>

                        <!-- L1 to Core/SIMD (Core 0) -->
                        <path class="flow-arrow-mem-hierarchy" d="M225 150 V110" />
                        <text class="flow-label-mem-hierarchy" x="230" y="120">L1 to SIMD</text>

                        <!-- SIMD Compute -->
                        <circle class="simd-compute-circle" cx="225" cy="40" r="15" fill="#FFC107" opacity="0.8" />
                        <text class="simd-compute-text" x="225" y="45" text-anchor="middle">Compute</text>

                        <!-- Result back to L1 (simplified) -->
                        <path class="flow-arrow-mem-hierarchy" d="M225 30 V50" />
                        <text class="flow-label-mem-hierarchy" x="230" y="40">Result</text>
                    </g>

                    <!-- Prefetcher -->
                    <g class="prefetcher fragment fade-in">
                        <rect x="750" y="50" width="80" height="40" fill="#4CAF50" rx="5" ry="5" />
                        <text x="790" y="75" class="llm-text">Prefetcher</text>
                        <path class="flow-arrow-mem-hierarchy" d="M790 90 V430 H425 V450" />
                        <text class="flow-label-mem-hierarchy" x="700" y="350">Anticipate Next Data</text>
                    </g>

                    <text x="450" y="530" class="llm-dim-text">Optimized for sequential access &amp; cache locality</text>
                </svg>
                <aside class="notes">
                    <h3>Token Streaming and SIMD Compute</h3>
                    <ul>
                        <li>This slide illustrates the journey of data, specifically a "token slice," through the CPU's memory hierarchy during computation.</li>
                        <li>**DDR (Main Memory):** This is where the bulk of the model's weights and the full input sequence reside. It's slow but large.</li>
                        <li>**L3 Cache (Shared):** Data is fetched from DDR into the L3 cache, which is shared among all CPU cores. This acts as a large buffer for frequently accessed data.</li>
                        <li>**L2 Cache (Per Core Group):** From L3, data moves to the L2 cache, which is typically local to a group of cores or a single core. It's faster than L3 but smaller.</li>
                        <li>**L1 Cache (Per Core):** The fastest and smallest cache, directly accessible by the CPU core. This is where the active data for immediate computation resides.</li>
                        <li>**CPU Core / SIMD Unit:** Finally, the data reaches the SIMD (Single Instruction, Multiple Data) units within the CPU core, where the actual vectorized computations (like AVX-512 GEMMs) occur.</li>
                        <li>**Token Streaming:** In our "Token-Parallel Orchestration," each core is assigned a specific "token slice." This means data for that slice is streamed up the memory hierarchy to that core's local caches (L2, L1).</li>
                        <li>**Prefetcher:** A critical component is the hardware prefetcher. It intelligently anticipates which data the CPU will need next based on access patterns and proactively fetches it into higher-level caches. For our sequential memory layouts and token streaming, the prefetcher is highly effective, minimizing memory stalls.</li>
                        <li>The goal is to keep the SIMD units busy by ensuring data is always available in the fastest cache levels, minimizing trips to slower memory.</li>
                    </ul>
                </aside>
            </section>

            <!-- Conclusion Slide -->
            <section>
                <h2>Key Takeaways: LLM Layer Structure</h2>
                <ul>
                    <li>Each Transformer layer processes input through **Self-Attention** and a **Feed-Forward Network**.</li>
                    <li>**Layer Normalization** and **Residual Connections (Add & Norm)** are crucial for stable training and information flow.</li>
                    <li>The entire layer is built upon fundamental **Matrix Multiplications (GEMM)**, highlighting why their optimization is critical.</li>
                    <li>This modular design allows for stacking many layers to form deep, powerful LLMs.</li>
                </ul>
                <aside class="notes">
                    <h3>Key Takeaways and Summary</h3>
                    <ul>
                        <li>To summarize, we've explored the intricate architecture of a single LLM Transformer layer.</li>
                        <li>We saw how data flows through Layer Normalization, Multi-Head Attention, and the Feed-Forward Network.</li>
                        <li>The importance of residual connections and normalization for training deep models was highlighted.</li>
                        <li>Crucially, all these complex operations ultimately boil down to highly optimized matrix multiplications (GEMMs).</li>
                        <li>Understanding the memory hierarchy and how data streams to the CPU cores is essential for maximizing performance.</li>
                        <li>Our CPU-optimized runtime focuses on these low-level details to achieve significant speedups.</li>
                        <li>This modularity allows for scaling to very large models by simply stacking more layers.</li>
                    </ul>
                </aside>
            </section>

        </div>
    </div>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            Reveal.initialize({
                controls: true,
                progress: true,
                center: true,
                hash: true,
                transition: 'slide', // none/fade/slide/convex/concave/zoom
                plugins: [ RevealHighlight, RevealMarkdown, RevealNotes, RevealMath.KaTeX ]
            });
        });
    </script>
</body>
</html>
