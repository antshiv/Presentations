<svg viewBox="0 0 1400 1000" xmlns="http://www.w3.org/2000/svg">
  <!-- Styles with stages- prefix to avoid conflicts -->
  <defs>
    <style>
      .stages-background { fill: #1a1a2e; }
      .stages-title { fill: #ffffff; font-size: 28px; font-weight: bold; text-anchor: middle; }
      .stages-subtitle { fill: #aaaaaa; font-size: 16px; text-anchor: middle; }
      .stages-section-bg { fill: #2d2d44; stroke: #4a4a6a; stroke-width: 2; }
      .stages-stage-title { fill: #ffffff; font-size: 18px; font-weight: bold; text-anchor: middle; }
      .stages-header-bg { fill: #4a4a6a; }
      .stages-pretraining-bg { fill: #FF6B6B; }
      .stages-sft-bg { fill: #4ECDC4; }
      .stages-rl-bg { fill: #45B7D1; }
      .stages-text { fill: #ffffff; font-size: 12px; }
      .stages-subtext { fill: #cccccc; font-size: 11px; }
      .stages-code-bg { fill: #2a2a3e; stroke: #4a4a6a; stroke-width: 1; }
      .stages-code-text { fill: #4CAF50; font-size: 10px; font-family: monospace; }
      .stages-example-bg { fill: #3d3d5c; stroke: #6a6a8a; stroke-width: 1; }
      .stages-input-text { fill: #ffeb3b; font-size: 11px; font-style: italic; }
      .stages-output-text { fill: #4CAF50; font-size: 11px; }
      .stages-problem-text { fill: #ff6b6b; font-size: 11px; }
      .stages-weight-text { fill: #ff9800; font-size: 11px; font-weight: bold; }
      .stages-process-text { fill: #45B7D1; font-size: 11px; font-weight: bold; }
      .stages-arrow { stroke: #ffeb3b; stroke-width: 2; fill: none; }
      .stages-arrow-head { fill: #ffeb3b; }
    </style>
    <marker id="stagesArrow" markerWidth="8" markerHeight="8" refX="7" refY="3" orient="auto" markerUnits="strokeWidth">
      <path d="M0,0 L0,6 L7,3 z" class="stages-arrow-head"/>
    </marker>
  </defs>
  
  <!-- Background -->
  <rect width="1400" height="1000" class="stages-background"/>
  
  <!-- Main Title -->
  <text x="700" y="40" class="stages-title">LLM Training Stages: What Actually Happens</text>
  <text x="700" y="65" class="stages-subtitle">Detailed breakdown of Pre-training, SFT, and Reinforcement Learning</text>
  
  <!-- Stage 1: Pre-training -->
  <g transform="translate(50, 100)">
    <rect x="0" y="0" width="400" height="250" class="stages-section-bg" rx="10"/>
    <rect x="10" y="10" width="380" height="35" class="stages-pretraining-bg" rx="5"/>
    <text x="200" y="33" class="stages-stage-title">PRE-TRAINING</text>
    
    <!-- What it does -->
    <g transform="translate(20, 65)">
      <text x="0" y="15" class="stages-text">What it does:</text>
      <text x="0" y="30" class="stages-subtext">• Learns to predict next token from massive text</text>
      <text x="0" y="45" class="stages-subtext">• Develops language understanding and knowledge</text>
      <text x="0" y="60" class="stages-subtext">• Creates internal representations of concepts</text>
    </g>
    
    <!-- Training process -->
    <g transform="translate(20, 130)">
      <text x="0" y="15" class="stages-process-text">Training Process:</text>
      <text x="0" y="30" class="stages-subtext">1. Tokenize internet text into sequences</text>
      <text x="0" y="45" class="stages-subtext">2. Mask next token, predict it</text>
      <text x="0" y="60" class="stages-subtext">3. Calculate loss, update weights</text>
      <text x="0" y="75" class="stages-subtext">4. Repeat for trillions of tokens</text>
    </g>
    
    <!-- What happens to weights -->
    <g transform="translate(20, 210)">
      <text x="0" y="15" class="stages-weight-text">Weight Changes:</text>
      <text x="0" y="30" class="stages-subtext">Random → Meaningful representations</text>
    </g>
  </g>
  
  <!-- Pre-training Example -->
  <g transform="translate(470, 120)">
    <rect x="0" y="0" width="350" height="130" class="stages-example-bg" rx="5"/>
    <text x="175" y="20" class="stages-stage-title">Pre-training Example</text>
    
    <text x="10" y="40" class="stages-input-text">Input: "The capital of France is"</text>
    <text x="10" y="55" class="stages-output-text">Model learns: "Paris" (most likely next token)</text>
    
    <text x="10" y="75" class="stages-input-text">Input: "To solve this math problem"</text>
    <text x="10" y="90" class="stages-output-text">Model learns: reasoning patterns</text>
    
    <text x="10" y="110" class="stages-text">Result: Knows language but doesn't follow instructions</text>
  </g>
  
  <!-- Arrow to SFT -->
  <path d="M 450 225 L 500 225" class="stages-arrow" marker-end="url(#stagesArrow)"/>
  
  <!-- Stage 2: Supervised Fine-Tuning -->
  <g transform="translate(850, 100)">
    <rect x="0" y="0" width="400" height="250" class="stages-section-bg" rx="10"/>
    <rect x="10" y="10" width="380" height="35" class="stages-sft-bg" rx="5"/>
    <text x="200" y="33" class="stages-stage-title">SUPERVISED FINE-TUNING</text>
    
    <!-- What it does -->
    <g transform="translate(20, 65)">
      <text x="0" y="15" class="stages-text">What it does:</text>
      <text x="0" y="30" class="stages-subtext">• Teaches model to follow instructions</text>
      <text x="0" y="45" class="stages-subtext">• Learns question-answer patterns</text>
      <text x="0" y="60" class="stages-subtext">• Adapts to conversational format</text>
    </g>
    
    <!-- Training process -->
    <g transform="translate(20, 130)">
      <text x="0" y="15" class="stages-process-text">Training Process:</text>
      <text x="0" y="30" class="stages-subtext">1. Curate instruction-response pairs</text>
      <text x="0" y="45" class="stages-subtext">2. Supervised learning on conversations</text>
      <text x="0" y="60" class="stages-subtext">3. Fine-tune existing weights</text>
      <text x="0" y="75" class="stages-subtext">4. Validate on held-out examples</text>
    </g>
    
    <!-- What happens to weights -->
    <g transform="translate(20, 210)">
      <text x="0" y="15" class="stages-weight-text">Weight Changes:</text>
      <text x="0" y="30" class="stages-subtext">Task-specific adaptations on top of base knowledge</text>
    </g>
  </g>
  
  <!-- SFT Example -->
  <g transform="translate(520, 280)">
    <rect x="0" y="0" width="350" height="130" class="stages-example-bg" rx="5"/>
    <text x="175" y="20" class="stages-stage-title">SFT Example</text>
    
    <text x="10" y="40" class="stages-input-text">Input: "Explain photosynthesis"</text>
    <text x="10" y="55" class="stages-output-text">Output: Structured explanation (not just completion)</text>
    
    <text x="10" y="75" class="stages-input-text">Input: "Write a Python function"</text>
    <text x="10" y="90" class="stages-output-text">Output: Complete code with explanation</text>
    
    <text x="10" y="110" class="stages-problem-text">Problem: May not align with human preferences</text>
  </g>
  
  <!-- Arrow to RL -->
  <path d="M 700 400 L 750 400" class="stages-arrow" marker-end="url(#stagesArrow)"/>
  
  <!-- Stage 3: Reinforcement Learning -->
  <g transform="translate(50, 450)">
    <rect x="0" y="0" width="400" height="280" class="stages-section-bg" rx="10"/>
    <rect x="10" y="10" width="380" height="35" class="stages-rl-bg" rx="5"/>
    <text x="200" y="33" class="stages-stage-title">REINFORCEMENT LEARNING</text>
    
    <!-- What it does -->
    <g transform="translate(20, 65)">
      <text x="0" y="15" class="stages-text">What it does:</text>
      <text x="0" y="30" class="stages-subtext">• Aligns model with human preferences</text>
      <text x="0" y="45" class="stages-subtext">• Teaches helpful, harmless, honest behavior</text>
      <text x="0" y="60" class="stages-subtext">• Optimizes for human-preferred outputs</text>
    </g>
    
    <!-- Training process -->
    <g transform="translate(20, 130)">
      <text x="0" y="15" class="stages-process-text">Training Process (RLHF):</text>
      <text x="0" y="30" class="stages-subtext">1. Collect human preference data (A vs B)</text>
      <text x="0" y="45" class="stages-subtext">2. Train reward model to predict preferences</text>
      <text x="0" y="60" class="stages-subtext">3. Use PPO to optimize against reward model</text>
      <text x="0" y="75" class="stages-subtext">4. Iteratively improve alignment</text>
    </g>
    
    <!-- What happens to weights -->
    <g transform="translate(20, 210)">
      <text x="0" y="15" class="stages-weight-text">Weight Changes:</text>
      <text x="0" y="30" class="stages-subtext">Fine-grained preference optimization</text>
      <text x="0" y="45" class="stages-subtext">Strengthens helpful behaviors</text>
      <text x="0" y="60" class="stages-subtext">Suppresses harmful outputs</text>
    </g>
  </g>
  
  <!-- RL Example -->
  <g transform="translate(470, 470)">
    <rect x="0" y="0" width="350" height="160" class="stages-example-bg" rx="5"/>
    <text x="175" y="20" class="stages-stage-title">RL Example</text>
    
    <text x="10" y="40" class="stages-input-text">Input: "How to make explosives?"</text>
    <text x="10" y="55" class="stages-output-text">Before RL: Provides dangerous instructions</text>
    <text x="10" y="70" class="stages-output-text">After RL: "I can't help with that, but..."</text>
    
    <text x="10" y="90" class="stages-input-text">Input: "Explain quantum physics"</text>
    <text x="10" y="105" class="stages-output-text">Before RL: Technically correct but confusing</text>
    <text x="10" y="120" class="stages-output-text">After RL: Clear, helpful explanation</text>
    
    <text x="10" y="145" class="stages-text">Result: ChatGPT-like helpful assistant</text>
  </g>
  
  <!-- Technical Details Section -->
  <g transform="translate(850, 450)">
    <rect x="0" y="0" width="500" height="280" class="stages-section-bg" rx="10"/>
    <text x="250" y="25" class="stages-stage-title">Technical Details</text>
    
    <!-- Data scales -->
    <g transform="translate(20, 50)">
      <text x="0" y="15" class="stages-text">Data Scale Comparison:</text>
      <text x="0" y="35" class="stages-subtext">Pre-training: ~10TB of text (trillions of tokens)</text>
      <text x="0" y="50" class="stages-subtext">SFT: ~10MB of examples (thousands of conversations)</text>
      <text x="0" y="65" class="stages-subtext">RL: ~1MB of preferences (human rankings)</text>
    </g>
    
    <!-- Compute requirements -->
    <g transform="translate(20, 105)">
      <text x="0" y="15" class="stages-text">Compute Requirements:</text>
      <text x="0" y="35" class="stages-subtext">Pre-training: 1000s of GPUs for months</text>
      <text x="0" y="50" class="stages-subtext">SFT: 10s of GPUs for days</text>
      <text x="0" y="65" class="stages-subtext">RL: 10s of GPUs for hours</text>
    </g>
    
    <!-- Loss functions -->
    <g transform="translate(20, 160)">
      <text x="0" y="15" class="stages-text">Loss Functions:</text>
      <text x="0" y="35" class="stages-subtext">Pre-training: Cross-entropy (next token prediction)</text>
      <text x="0" y="50" class="stages-subtext">SFT: Cross-entropy (sequence-to-sequence)</text>
      <text x="0" y="65" class="stages-subtext">RL: Policy gradient (reward maximization)</text>
    </g>
    
    <!-- Key insight -->
    <g transform="translate(20, 215)">
      <text x="0" y="15" class="stages-process-text">Key Insight:</text>
      <text x="0" y="35" class="stages-subtext">Each stage builds on the previous one.</text>
      <text x="0" y="50" class="stages-subtext">Can't skip pre-training, but can skip RL</text>
      <text x="0" y="65" class="stages-subtext">for some applications.</text>
    </g>
  </g>
  
  <!-- Bottom Summary -->
  <g transform="translate(50, 750)">
    <rect x="0" y="0" width="1300" height="80" class="stages-example-bg" rx="10"/>
    <text x="650" y="25" class="stages-stage-title">Training Pipeline Summary</text>
    <text x="650" y="45" class="stages-subtitle">Pre-training creates the foundation, SFT teaches instruction following, RL aligns with human values</text>
    <text x="650" y="65" class="stages-subtext">Each stage requires different data, techniques, and serves different purposes in creating useful AI assistants</text>
  </g>
</svg>