<svg id="b7fd8055-ebed-4f87-bcf0-cfc5ba82671f" data-name="Layer 1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1400 670.74"><text transform="translate(439.69 40)" style="isolation:isolate;font-size:28px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">Parameter-Efficient Fine-<tspan x="331.42" y="0" style="letter-spacing:-0.07421875em">T</tspan><tspan x="346.45" y="0">uning (PEFT)</tspan></text><text transform="translate(430.53 65)" style="isolation:isolate;font-size:16px;fill:#aaa;font-family:ArialMT, Arial">How to practically fine-tune models without breaking the bank (or the model)</text><g id="e5ea8491-c783-4395-be1b-0529c3944ea6" data-name="lora"><path d="M60,100H420c5.52,0,10,5.17,10,11.55v230.9c0,6.38-4.48,11.55-10,11.55H60c-5.52,0-10-5.17-10-11.55V111.55C50,105.17,54.48,100,60,100Z" style="fill:#2d2d44;stroke:#4a4a6a;stroke-width:2px"/><rect x="60" y="110" width="360" height="30" rx="5" style="fill:#4ecdc4"/><text transform="translate(129.5 130)" style="isolation:isolate;font-size:16px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">LoR<tspan x="31.1" y="0" style="letter-spacing:-0.037109375em">A</tspan><tspan x="42.06" y="0" xml:space="preserve"> (Low-Rank</tspan><tspan x="128.28" y="0" style="letter-spacing:-0.037109375em"> </tspan><tspan x="132.13" y="0">Adaptation)</tspan></text><text transform="translate(70 170)" style="isolation:isolate;font-size:12px;fill:#fff;font-family:ArialMT, Arial">How it works:</text><text transform="translate(70 185)" style="isolation:isolate;font-size:11px;fill:#45b7d1;font-family:Arial-BoldMT, Arial;font-weight:700">• Original weights FROZEN</text><text transform="translate(70 200)" style="isolation:isolate;font-size:11px;fill:#ff9800;font-family:Arial-BoldMT, Arial;font-weight:700">• Small adapter matrices added</text><text transform="translate(70 215)" style="isolation:isolate;font-size:11px;fill:#ccc;font-family:ArialMT, Arial">• Low-rank decomposition: B ×<tspan x="149.06" y="0" xml:space="preserve" style="letter-spacing:-0.05517578125em"> A</tspan></text><text transform="translate(70 230)" style="isolation:isolate;font-size:11px;fill:#ccc;font-family:ArialMT, Arial">•<tspan x="3.85" y="0" style="letter-spacing:-0.01806640625em"> </tspan><tspan x="6.71" y="0" style="letter-spacing:-0.05517578125em">T</tspan><tspan x="12.82" y="0">ypical rank: 4-64</tspan></text><text transform="translate(70 255)" style="isolation:isolate;font-size:12px;fill:#fff;font-family:ArialMT, Arial">Benefits:</text><text transform="translate(70 270)" style="isolation:isolate;font-size:11px;fill:#4caf50;font-family:ArialMT, Arial">• 0.1-1% of original parameters</text><text transform="translate(70 285)" style="isolation:isolate;font-size:11px;fill:#4caf50;font-family:ArialMT, Arial">• Base model preserved</text><text transform="translate(70 300)" style="isolation:isolate;font-size:11px;fill:#4caf50;font-family:ArialMT, Arial">• Multiple adapters possible</text><text transform="translate(70 315)" style="isolation:isolate;font-size:11px;fill:#4caf50;font-family:ArialMT, Arial">• Fast training and switching</text><text transform="translate(70 340)" style="isolation:isolate;font-size:12px;fill:#ffeb3b;font-family:Arial-BoldMT, Arial;font-weight:700"><tspan style="letter-spacing:-0.01806640625em">W</tspan><tspan x="11.11" y="0">orks with: Both SFT and R</tspan><tspan x="161.1" y="0" style="letter-spacing:-0.01806640625em">L</tspan></text></g><g id="acb898b5-b777-453a-838e-b26e9fba3b2f" data-name="qlora"><path d="M519.86,100h360c5.53,0,10,5.17,10,11.55v230.9c0,6.38-4.47,11.55-10,11.55h-360c-5.52,0-10-5.17-10-11.55V111.55C509.86,105.17,514.34,100,519.86,100Z" style="fill:#2d2d44;stroke:#4a4a6a;stroke-width:2px"/><rect x="519.86" y="110" width="360" height="30" rx="5" style="fill:#45b7d1"/><text transform="translate(602.84 130)" style="isolation:isolate;font-size:16px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">QLoR<tspan x="43.55" y="0" style="letter-spacing:-0.037109375em">A</tspan><tspan x="54.51" y="0" xml:space="preserve"> (Quantized LoRA)</tspan></text><text transform="translate(529.86 170)" style="isolation:isolate;font-size:12px;fill:#fff;font-family:ArialMT, Arial">Enhanced LoR<tspan x="79.39" y="0" style="letter-spacing:-0.05517578125em">A</tspan><tspan x="86.74" y="0" xml:space="preserve"> approach:</tspan></text><text transform="translate(529.86 185)" style="isolation:isolate;font-size:11px;fill:#45b7d1;font-family:Arial-BoldMT, Arial;font-weight:700">• 4-bit quantized base model</text><text transform="translate(529.86 200)" style="isolation:isolate;font-size:11px;fill:#ff9800;font-family:Arial-BoldMT, Arial;font-weight:700">• 16-bit LoR<tspan x="60.68" y="0" style="letter-spacing:-0.037109375em">A</tspan><tspan x="68.22" y="0" xml:space="preserve"> adapters</tspan></text><text transform="translate(529.86 215)" style="isolation:isolate;font-size:11px;fill:#ccc;font-family:ArialMT, Arial">• Even more memory e<tspan x="112.05" y="0" style="letter-spacing:-0.01806640625em">f</tspan><tspan x="114.91" y="0">ficient</tspan></text><text transform="translate(529.86 240)" style="isolation:isolate;font-size:12px;fill:#fff;font-family:ArialMT, Arial">Memory savings:</text><text transform="translate(529.86 255)" style="isolation:isolate;font-size:11px;fill:#4caf50;font-family:ArialMT, Arial">• 7B model: 28GB → 7GB</text><text transform="translate(529.86 270)" style="isolation:isolate;font-size:11px;fill:#4caf50;font-family:ArialMT, Arial">• Fits on consumer GPUs</text><text transform="translate(529.86 285)" style="isolation:isolate;font-size:11px;fill:#4caf50;font-family:ArialMT, Arial">• No performance loss</text><text transform="translate(529.86 310)" style="isolation:isolate;font-size:12px;fill:#fff;font-family:ArialMT, Arial">Use cases:</text><text transform="translate(529.86 325)" style="isolation:isolate;font-size:11px;fill:#ccc;font-family:ArialMT, Arial">• Limited GPU memory</text><text transform="translate(529.86 340)" style="isolation:isolate;font-size:11px;fill:#ccc;font-family:ArialMT, Arial">• Cost-e<tspan x="39.31" y="0" style="letter-spacing:-0.01806640625em">f</tspan><tspan x="42.16" y="0">fective fine-tuning</tspan></text></g><g id="a5c31982-288d-49fb-bf38-a101bd46c23e" data-name="full_fine_tuning"><path d="M979.73,100h360c5.52,0,10,5.17,10,11.55v230.9c0,6.38-4.48,11.55-10,11.55h-360c-5.52,0-10-5.17-10-11.55V111.55C969.73,105.17,974.21,100,979.73,100Z" style="fill:#2d2d44;stroke:#4a4a6a;stroke-width:2px"/><rect x="979.73" y="110" width="360" height="30" rx="5" style="fill:#ff6b6b"/><text transform="translate(1049.97 130)" style="isolation:isolate;font-size:16px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">Full Fine-<tspan x="71.1" y="0" style="letter-spacing:-0.07421875em">T</tspan><tspan x="79.69" y="0">uning (</tspan><tspan x="133" y="0" style="letter-spacing:-0.037109375em">A</tspan><tspan x="143.96" y="0">void This)</tspan></text><text transform="translate(989.73 170)" style="isolation:isolate;font-size:12px;fill:#fff;font-family:ArialMT, Arial">What happens:</text><text transform="translate(989.73 185)" style="isolation:isolate;font-size:11px;fill:#ff6b6b;font-family:ArialMT, Arial">•<tspan x="3.85" y="0" style="letter-spacing:-0.05517578125em"> </tspan><tspan x="6.3" y="0">AL</tspan><tspan x="19.75" y="0" style="letter-spacing:-0.037109375em">L</tspan><tspan x="25.46" y="0" xml:space="preserve"> weights trainable</tspan></text><text transform="translate(989.73 200)" style="isolation:isolate;font-size:11px;fill:#ff6b6b;font-family:ArialMT, Arial">• Original model modified</text><text transform="translate(989.73 215)" style="isolation:isolate;font-size:11px;fill:#ff6b6b;font-family:ArialMT, Arial">• Risk of catastrophic forgetting</text><text transform="translate(989.73 240)" style="isolation:isolate;font-size:12px;fill:#fff;font-family:ArialMT, Arial">Problems:</text><text transform="translate(989.73 255)" style="isolation:isolate;font-size:11px;fill:#ff6b6b;font-family:ArialMT, Arial">• Expensive (billions of params)</text><text transform="translate(989.73 270)" style="isolation:isolate;font-size:11px;fill:#ff6b6b;font-family:ArialMT, Arial">• Memory intensive</text><text transform="translate(989.73 285)" style="isolation:isolate;font-size:11px;fill:#ff6b6b;font-family:ArialMT, Arial">• Can &quot;spoil&quot; base model</text><text transform="translate(989.73 300)" style="isolation:isolate;font-size:11px;fill:#ff6b6b;font-family:ArialMT, Arial">• Loses general knowledge</text><text transform="translate(989.73 325)" style="isolation:isolate;font-size:12px;fill:#fff;font-family:ArialMT, Arial">When needed:</text><text transform="translate(989.73 340)" style="isolation:isolate;font-size:11px;fill:#ccc;font-family:ArialMT, Arial">• Major domain shifts only</text></g><g id="bbee4213-1319-4c5a-a030-ac6bf83ac2fd" data-name="practical_lora"><path d="M60,411.27H640c5.52,0,10,5.3,10,11.84v213c0,6.54-4.48,11.84-10,11.84H60c-5.52,0-10-5.3-10-11.84v-213C50,416.57,54.48,411.27,60,411.27Z" style="fill:#3d3d5c;stroke:#6a6a8a"/><text transform="translate(258.26 436.27)" style="isolation:isolate;font-size:16px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700">Practical LoR<tspan x="102.26" y="0" style="letter-spacing:-0.037109375em">A</tspan><tspan x="113.22" y="0" xml:space="preserve"> Example</tspan></text><text transform="translate(70 471.27)" style="isolation:isolate;font-size:10px;fill:#4caf50;font-family:CourierNewPSMT, Courier New"># Load base model (frozen)</text><text transform="translate(70 486.27)" style="isolation:isolate;font-size:10px;fill:#4caf50;font-family:CourierNewPSMT, Courier New">model = AutoModelForCausalLM.from_pretrained(&quot;mistral-7b-instruct&quot;)</text><text transform="translate(70 506.27)" style="isolation:isolate;font-size:10px;fill:#4caf50;font-family:CourierNewPSMT, Courier New"># Add LoRA adapters</text><text transform="translate(70 521.27)" style="isolation:isolate;font-size:10px;fill:#4caf50;font-family:CourierNewPSMT, Courier New">lora_config = LoraConfig(r=16, target_modules=[&quot;q_proj&quot;, &quot;v_proj&quot;])</text><text transform="translate(70 536.27)" style="isolation:isolate;font-size:10px;fill:#4caf50;font-family:CourierNewPSMT, Courier New">peft_model = get_peft_model(model, lora_config)</text><text transform="translate(70 556.27)" style="isolation:isolate;font-size:10px;fill:#4caf50;font-family:CourierNewPSMT, Courier New"># Result: 7B params frozen + 14M params trainable</text><text transform="translate(70 571.27)" style="isolation:isolate;font-size:10px;fill:#4caf50;font-family:CourierNewPSMT, Courier New"># Training cost: $10-100 instead of $1000+</text><text transform="translate(70 596.27)" style="isolation:isolate;font-size:12px;fill:#ffeb3b;font-family:Arial-BoldMT, Arial;font-weight:700">Multiple<tspan x="45.33" y="0" style="letter-spacing:-0.037109375em"> </tspan><tspan x="48.22" y="0">Adapters Strategy:</tspan></text><text transform="translate(70 611.27)" style="isolation:isolate;font-size:12px;fill:#fff;font-family:ArialMT, Arial">SQL_adapte<tspan x="67.39" y="0" style="letter-spacing:-0.05517578125em">r</tspan><tspan x="70.72" y="0">.pth, Medical_adapte</tspan><tspan x="182.13" y="0" style="letter-spacing:-0.05517578125em">r</tspan><tspan x="185.47" y="0">.pth, Code_adapte</tspan><tspan x="284.21" y="0" style="letter-spacing:-0.05517578125em">r</tspan><tspan x="287.55" y="0">.pth</tspan></text><text transform="translate(70 626.27)" style="isolation:isolate;font-size:12px;fill:#fff;font-family:ArialMT, Arial">Switch between specialists without retraining base model</text></g><g id="b035d6d1-ad9d-40a2-b3a6-f4df0865effd" data-name="training_method"><path d="M680,411.27h660c5.52,0,10,5.3,10,11.84v213c0,6.54-4.48,11.84-10,11.84H680c-5.52,0-10-5.3-10-11.84v-213C670,416.57,674.48,411.27,680,411.27Z" style="fill:#2d2d44;stroke:#4a4a6a;stroke-width:2px"/><text transform="translate(899.33 436.27)" style="isolation:isolate;font-size:16px;fill:#fff;font-family:Arial-BoldMT, Arial;font-weight:700"><tspan style="letter-spacing:-0.05517578125em">T</tspan><tspan x="8.89" y="0">raining Method Comparison</tspan></text><text transform="translate(700 476.27)" style="isolation:isolate;font-size:12px;fill:#fff;font-family:ArialMT, Arial">Method</text><text transform="translate(850 476.27)" style="isolation:isolate;font-size:12px;fill:#fff;font-family:ArialMT, Arial">Parameters</text><text transform="translate(1000 476.27)" style="isolation:isolate;font-size:12px;fill:#fff;font-family:ArialMT, Arial">Memory</text><text transform="translate(1150 476.27)" style="isolation:isolate;font-size:12px;fill:#fff;font-family:ArialMT, Arial">Cost</text><text transform="translate(1250 476.27)" style="isolation:isolate;font-size:12px;fill:#fff;font-family:ArialMT, Arial">Risk</text><text transform="translate(700 496.27)" style="isolation:isolate;font-size:11px;fill:#ff6b6b;font-family:ArialMT, Arial">Full Fine-tuning</text><text transform="translate(850 496.27)" style="isolation:isolate;font-size:11px;fill:#ff6b6b;font-family:ArialMT, Arial">7B trainable</text><text transform="translate(1000 496.27)" style="isolation:isolate;font-size:11px;fill:#ff6b6b;font-family:ArialMT, Arial">28GB+</text><text transform="translate(1150 496.27)" style="isolation:isolate;font-size:11px;fill:#ff6b6b;font-family:ArialMT, Arial">$1000+</text><text transform="translate(1250 496.27)" style="isolation:isolate;font-size:11px;fill:#ff6b6b;font-family:ArialMT, Arial">High</text><text transform="translate(700 516.27)" style="isolation:isolate;font-size:11px;fill:#4caf50;font-family:ArialMT, Arial">LoR<tspan x="20.18" y="0" style="letter-spacing:-0.05517578125em">A</tspan></text><text transform="translate(850 516.27)" style="isolation:isolate;font-size:11px;fill:#4caf50;font-family:ArialMT, Arial">14M trainable</text><text transform="translate(1000 516.27)" style="isolation:isolate;font-size:11px;fill:#4caf50;font-family:ArialMT, Arial">16GB</text><text transform="translate(1150 516.27)" style="isolation:isolate;font-size:11px;fill:#4caf50;font-family:ArialMT, Arial">$10-100</text><text transform="translate(1250 516.27)" style="isolation:isolate;font-size:11px;fill:#4caf50;font-family:ArialMT, Arial">Low</text><text transform="translate(700 536.27)" style="isolation:isolate;font-size:11px;fill:#4caf50;font-family:ArialMT, Arial">QLoR<tspan x="28.74" y="0" style="letter-spacing:-0.05517578125em">A</tspan></text><text transform="translate(850 536.27)" style="isolation:isolate;font-size:11px;fill:#4caf50;font-family:ArialMT, Arial">14M trainable</text><text transform="translate(1000 536.27)" style="isolation:isolate;font-size:11px;fill:#4caf50;font-family:ArialMT, Arial">7GB</text><text transform="translate(1150 536.27)" style="isolation:isolate;font-size:11px;fill:#4caf50;font-family:ArialMT, Arial">$5-50</text><text transform="translate(1250 536.27)" style="isolation:isolate;font-size:11px;fill:#4caf50;font-family:ArialMT, Arial">Low</text><text transform="translate(700 566.27)" style="isolation:isolate;font-size:12px;fill:#ffeb3b;font-family:Arial-BoldMT, Arial;font-weight:700">Key Insight: LoR<tspan x="95.33" y="0" style="letter-spacing:-0.037109375em">A</tspan><tspan x="103.55" y="0" xml:space="preserve"> preserves base model while adding new capabilities</tspan></text><text transform="translate(700 586.27)" style="isolation:isolate;font-size:12px;fill:#fff;font-family:ArialMT, Arial">99.9% of practical fine-tuning uses LoR<tspan x="208.79" y="0" style="letter-spacing:-0.05517578125em">A</tspan><tspan x="216.13" y="0" xml:space="preserve"> or QLoR</tspan><tspan x="264.81" y="0" style="letter-spacing:-0.05517578125em">A</tspan></text></g></svg>