<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title>GPT-2 Backpropagation - From Theory to Feature-Parallel AVX-512</title>

    <link rel="stylesheet" href="../reveal.js/dist/reset.css">
    <link rel="stylesheet" href="../reveal.js/dist/reveal.css">
    <link rel="stylesheet" href="../reveal.js/dist/theme/black.css" id="theme">

    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="../reveal.js/plugin/highlight/monokai.css">
    <link rel="stylesheet" href="../css/styles.css">
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/3.9.1/gsap.min.js"></script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({
            startOnLoad: true,
            theme: 'dark',
            themeVariables: {
                primaryColor: '#60a5fa',
                primaryTextColor: '#fff',
                primaryBorderColor: '#3b82f6',
                lineColor: '#888',
                secondaryColor: '#8b5cf6',
                tertiaryColor: '#34d399',
                background: '#1a1a1a',
                mainBkg: '#2d333b',
                secondBkg: '#1e1e1e',
                textColor: '#f0f0f0',
                fontSize: '14px'
            }
        });
    </script>

    <style>
        :root { --r-main-font-size: 28px; }
        .reveal .slides section { font-size: 1em; text-align: center;}
        .reveal h1, .reveal h2, .reveal h3, .reveal h4 { text-transform: none; text-align: center;}
        .reveal p { text-align: center; }
        .reveal ul { text-align: left; }
        .reveal pre { margin: 20px auto; }
        .reveal code { font-size: 0.75em; line-height: 1.4; }

        /* Scrollable slides for large SVG content */
        .reveal .scrollable-slide {
            height: 100vh;
            overflow-y: auto !important;
            overflow-x: hidden;
            padding-bottom: 100px;
        }
        .reveal .scrollable-slide::-webkit-scrollbar {
            width: 8px;
        }
        .reveal .scrollable-slide::-webkit-scrollbar-track {
            background: #2a2a2a;
        }
        .reveal .scrollable-slide::-webkit-scrollbar-thumb {
            background: #4ade80;
            border-radius: 4px;
        }
        .reveal .scrollable-slide::-webkit-scrollbar-thumb:hover {
            background: #22c55e;
        }

        /* SVG container with zoom capability */
        .svg-container {
            position: relative;
            width: 100%;
            max-height: 85vh;
            overflow: auto;
            background: #1a1a1a;
            border-radius: 10px;
            margin-top: 20px;
            margin-bottom: 80px;
            padding-bottom: 100px;
        }
        .svg-container::-webkit-scrollbar {
            width: 8px;
            height: 8px;
        }
        .svg-container::-webkit-scrollbar-track {
            background: #2a2a2a;
        }
        .svg-container::-webkit-scrollbar-thumb {
            background: #4ade80;
            border-radius: 4px;
        }
        .svg-container img {
            width: 100%;
            height: auto;
            cursor: default;
            transform-origin: center center;
            will-change: transform;
        }

        /* Zoom controls */
        .zoom-controls {
            position: absolute;
            top: 10px;
            right: 10px;
            display: flex;
            gap: 5px;
            z-index: 100;
        }
        .zoom-btn {
            background: rgba(74, 222, 128, 0.8);
            border: none;
            color: white;
            width: 30px;
            height: 30px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 18px;
            font-weight: bold;
            transition: all 0.2s;
            position: relative;
        }
        .zoom-btn:hover {
            background: rgba(74, 222, 128, 1);
            transform: scale(1.1);
        }
        .zoom-btn:active {
            transform: scale(0.95);
        }

        /* Scroll indicator */
        .scroll-indicator {
            position: fixed;
            bottom: 40px;
            left: 50%;
            transform: translateX(-50%);
            color: #4ade80;
            font-size: 0.8em;
            animation: bounce 2s infinite;
            z-index: 200;
            background: rgba(0, 0, 0, 0.8);
            padding: 8px 16px;
            border-radius: 20px;
            transition: opacity 0.3s;
        }
        @keyframes bounce {
            0%, 20%, 50%, 80%, 100% { transform: translateX(-50%) translateY(0); }
            40% { transform: translateX(-50%) translateY(-10px); }
            60% { transform: translateX(-50%) translateY(-5px); }
        }

        .formula-box { background-color: rgba(45, 51, 59, 0.8); border-radius: 15px; padding: 15px; margin-top: 15px; border: 1px solid #484f58; }
        .dim-table { margin: 15px auto; font-size: 0.75em; border-collapse: collapse; }
        .dim-table th, .dim-table td { border: 1px solid #484f58; padding: 8px 15px; }
        .transformer-block { display: flex; flex-direction: column; align-items: center; gap: 10px; margin-top: 20px;}
        .block-component { border: 2px solid #484f58; border-radius: 10px; padding: 10px 20px; width: 400px; text-align: center; background-color: #2d333b; }
        .block-component.highlight { border-color: #ff6f00; background-color: #4d3c20; box-shadow: 0 0 15px #ff6f00; }
        .arrow-down { width: 0; height: 0; border-left: 15px solid transparent; border-right: 15px solid transparent; border-top: 20px solid #484f58; margin: 0 auto; }

        .code-block-small { font-size: 0.75em !important; line-height: 1.5 !important; }
        .code-block-medium { font-size: 0.85em !important; line-height: 1.5 !important; }
        .highlight-box { border: 3px solid #ff6f00 !important; box-shadow: 0 0 10px #ff6f00; }

        .split-container { display: flex; justify-content: space-between; gap: 20px; margin-top: 20px; }
        .split-left, .split-right { flex: 1; }

        .gradient-flow { display: flex; flex-direction: column; gap: 15px; align-items: center; margin-top: 20px; width: 100%; }
        .grad-step { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 5px; border-radius: 10px; width: 70%; text-align: center; font-size: 1.1em; }
        .grad-step.atomic { background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%); }
        .grad-step.optimized { background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%); }

        .perf-comparison { display: grid; grid-template-columns: 1fr 1fr; gap: 30px; margin: 30px auto; max-width: 900px; }
        .perf-card { background-color: #2d333b; border: 2px solid #484f58; border-radius: 10px; padding: 30px; text-align: center; color: #ccc; }
        .perf-card h3 { color: #f0f0f0; margin-bottom: 10px; }
        .perf-card p { color: #ccc; }
        .perf-card.fast { border-color: #4ade80; }
        .perf-card.slow { border-color: #f87171; }
        .perf-number { font-size: 3.5em; font-weight: bold; margin: 15px 0; }
        .perf-number.fast { color: #4ade80; }
        .perf-number.slow { color: #f87171; }

        /* Animation classes */
        .fade-in { opacity: 0; }
        .slide-up { transform: translateY(50px); opacity: 0; }
        .scale-in { transform: scale(0.8); opacity: 0; }
    </style>
</head>
<body>
    <div class="reveal">
        <div class="slides">
            <!-- Title Slide -->
            <section>
                <h1 class="gradient">GPT-2 Backpropagation</h1>
                <h3>From Theory to Feature-Parallel AVX-512</h3>
                <p>Training Transformers on CPU with SIMD and Zero-Atomic Gradient Accumulation</p>
                <p><small>ANTSHIV ROBOTICS</small></p>

                <aside class="notes">
                    This is about training GPT-2 entirely on CPU. No GPUs. We're going to show you how to make backprop fast using AVX-512 and smart parallelization. The key trick? Eliminating atomic operations in the hot loops.
                </aside>
            </section>

            <!-- Agenda -->
            <section>
                <h2 class="gradient" style="margin-bottom: 60px;">What We'll Cover</h2>
                <ul style="display: inline-block; text-align: left; font-size: 1em; line-height: 1.8;">
                    <li class="fragment fade-in">üß† Understanding Backprop: What & Why</li>
                    <li class="fragment fade-in">üîó The Chain Rule: How Gradients Flow</li>
                    <li class="fragment fade-in">‚úÖ Training Loop & Data Preloading (Zero-Copy)</li>
                    <li class="fragment fade-in">‚úÖ Backward Pass Architecture (Loss ‚Üí Embeddings)</li>
                    <li class="fragment fade-in">‚úÖ Feature-Parallel Gradient Accumulation (No Atomics!)</li>
                    <li class="fragment fade-in">‚úÖ AVX-512 Vectorization (16-Wide FMA)</li>
                    <li class="fragment fade-in">‚úÖ Performance Results (3-4√ó Speedup)</li>
                    <li class="fragment fade-in">‚úÖ Numerical Validation (Perplexity 2.34)</li>
                </ul>
            </section>

            <!-- Part 1: Understanding Backprop -->
            <section>
                <h2 class="gradient">Part 1: What Is Backpropagation?</h2>
                <h4 style="text-align: center; margin-top: 40px;">Let's Start From First Principles</h4>
            </section>

            <!-- Forward Pass: The Problem -->
            <section>
                <h2 class="gradient">The Forward Pass: We Have a Problem</h2>
                <div style="margin-top: 40px;">
                    <pre class="fragment fade-in"><code class="text code-block-medium" data-trim>
Input Data (x)
    ‚Üì
    √ó W‚ÇÅ  ‚Üê Weights (initialized randomly)
    ‚Üì
  Hidden
    ‚Üì
    √ó W‚ÇÇ  ‚Üê Weights (initialized randomly)
    ‚Üì
  Output (≈∑)

Expected Output (y) = "The cat sat on the mat"
Actual Output (≈∑)   = "asdf qwer zxcv lkjh poiu"
                    </code></pre>
                    <p class="fragment fade-in" style="margin-top: 30px; font-size: 1em; text-align: center; color: #f87171;">
                        ‚ùå Random weights give random output<br>
                        <strong>The Goal:</strong> Find weight values that minimize error
                    </p>
                </div>

                <aside class="notes">
                    When you initialize a network, weights are random. So the output is garbage. The question everyone asks is: where do the right weight values come from? They don't appear by magic. We learn them through training.
                </aside>
            </section>

            <!-- Where Do Values Come From? -->
            <section>
                <h2 class="gradient">Where Do Weight Values Come From?</h2>
                <h4 style="text-align: center; margin-top: 40px;">Training With Data</h4>
                <div style="margin-top: 40px; font-size: 0.9em;">
                    <pre class="fragment fade-in"><code class="text code-block-medium" data-trim>
1. Feed input data ‚Üí Get output
2. Compare output to expected ‚Üí Calculate error
3. Accumulate error across all samples ‚Üí Loss function
4. Propagate backwards ‚Üí Adjust weights to reduce loss
5. Repeat thousands of times ‚Üí Weights converge to good values
                    </code></pre>
                    <p class="fragment fade-in" style="margin-top: 30px; text-align: center; color: #4ade80; font-size: 1em;">
                        <strong>Key Insight:</strong> Weights aren't "figured out" by magic.<br>
                        They're learned through repeated adjustment based on error.
                    </p>
                </div>
            </section>

            <!-- The Training Flow -->
            <section>
                <h2 class="gradient">The Training Flow</h2>
                <div class="gradient-flow">
                    <div class="grad-step fragment fade-in">
                        <strong>1. Forward Pass:</strong> Input √ó Weights ‚Üí Output
                    </div>
                    <div class="arrow-down fragment fade-in"></div>
                    <div class="grad-step atomic fragment fade-in">
                        <strong>2. Compute Error:</strong> Expected vs Actual
                    </div>
                    <div class="arrow-down fragment fade-in"></div>
                    <div class="grad-step atomic fragment fade-in">
                        <strong>3. Accumulate Loss:</strong> Aggregate error across samples
                    </div>
                    <div class="arrow-down fragment fade-in"></div>
                    <div class="grad-step optimized fragment fade-in">
                        <strong>4. Backprop:</strong> Calculate weight adjustments
                    </div>
                    <div class="arrow-down fragment fade-in"></div>
                    <div class="grad-step optimized fragment fade-in">
                        <strong>5. Update Weights:</strong> W = W - learning_rate √ó gradient
                    </div>
                </div>
                <p class="fragment fade-in" style="margin-top: 40px; font-size: 0.9em; text-align: center;">
                    This cycle repeats thousands of times until loss converges
                </p>
            </section>

            <!-- Primary Goal of Backprop -->
            <section>
                <h2 class="gradient">The Primary Goal of Backpropagation</h2>
                <h3 class="fragment fade-in" style="text-align: center; margin-top: 40px; color: #4ade80;">
                    Give me values that tell me how to adjust my weights
                </h3>
                <div style="margin-top: 40px; font-size: 0.95em;">
                    <p class="fragment fade-in" style="text-align: center;"><strong>Not:</strong> "What should the weights be?"</p>
                    <p class="fragment fade-in" style="text-align: center; color: #4ade80;"><strong>But:</strong> "How much should I change each weight?"</p>
                    <div class="fragment fade-in" style="margin-top: 40px; background: #2d333b; padding: 25px; border-radius: 10px;">
                        <pre><code class="text code-block-medium" data-trim>
Backprop gives us: ‚àÇL/‚àÇW  (gradient of loss w.r.t. weight)

This tells us:
  ‚Ä¢ Direction: Should we increase or decrease this weight?
  ‚Ä¢ Magnitude: How much impact does this weight have on the error?

Then we update:
  W_new = W_old - learning_rate √ó (‚àÇL/‚àÇW)
                        </code></pre>
                    </div>
                </div>

                <aside class="notes">
                    This is the most important thing to understand. Backprop doesn't tell you what the weights should be. It tells you how to nudge them. Direction and magnitude. That's it. You repeat this thousands of times and the weights converge to good values.
                </aside>
            </section>

            <!-- What Are We Updating? -->
            <section>
                <h2 class="gradient">What Are We Actually Updating?</h2>
                <h4 style="text-align: center; margin-top: 20px;">The Gradients (Not the Weights Directly)</h4>
                <div style="margin-top: 40px; font-size: 0.8em;">
                    <div style="background: #2d333b; padding: 20px; border-radius: 10px; margin-bottom: 20px;">
                        <p style="color: #4ade80;"><strong>Step 1: Compute gradients for each training sample</strong></p>
                        <pre><code class="text" data-trim>
Sample 1: "The cat" ‚Üí ‚àÇL‚ÇÅ/‚àÇW = [0.3, -0.1, 0.5, ...]
Sample 2: "sat on"  ‚Üí ‚àÇL‚ÇÇ/‚àÇW = [0.2,  0.4, -0.2, ...]
Sample 3: "the mat" ‚Üí ‚àÇL‚ÇÉ/‚àÇW = [-0.1, 0.2, 0.3, ...]
                        </code></pre>
                    </div>
                    <div style="background: #2d333b; padding: 20px; border-radius: 10px; margin-bottom: 20px;">
                        <p style="color: #4ade80;"><strong>Step 2: Accumulate to get aggregate gradient</strong></p>
                        <pre><code class="text" data-trim>
‚àÇL/‚àÇW = (‚àÇL‚ÇÅ/‚àÇW + ‚àÇL‚ÇÇ/‚àÇW + ‚àÇL‚ÇÉ/‚àÇW) / 3
      = [0.13, 0.17, 0.2, ...]  ‚Üê This is what we use
                        </code></pre>
                    </div>
                    <div style="background: #2d333b; padding: 20px; border-radius: 10px;">
                        <p style="color: #4ade80;"><strong>Step 3: Update weights using aggregate</strong></p>
                        <pre><code class="text" data-trim>
W_new = W_old - learning_rate √ó ‚àÇL/‚àÇW
                        </code></pre>
                    </div>
                </div>
            </section>

            <!-- How Do We Know What Values? -->
            <section>
                <h2 class="gradient">How Do We Know What Gradient Values to Use?</h2>
                <h3 style="text-align: center; margin-top: 40px; color: #4ade80;">This is where the Chain Rule comes in</h3>
                <div style="margin-top: 40px; font-size: 0.85em;">
                    <p style="text-align: center;">We need to compute <strong>‚àÇL/‚àÇW</strong> for every weight in the network</p>
                    <div style="margin-top: 30px; background: #2d333b; padding: 20px; border-radius: 10px;">
                        <pre><code class="text" data-trim style="font-size: 0.9em;">
Forward:  Input ‚Üí Layer1 ‚Üí Layer2 ‚Üí ... ‚Üí Output ‚Üí Loss

Backward: How does Loss change if we change W‚ÇÅ?
          We need to trace backwards through all layers!

Chain Rule tells us:
  ‚àÇL/‚àÇW‚ÇÅ = ‚àÇL/‚àÇOutput √ó ‚àÇOutput/‚àÇLayer2 √ó ‚àÇLayer2/‚àÇLayer1 √ó ‚àÇLayer1/‚àÇW‚ÇÅ
           ‚Üë              ‚Üë                  ‚Üë                  ‚Üë
        final          parent             parent           local
        gradient       gradient           gradient         gradient
                        </code></pre>
                    </div>
                    <p style="margin-top: 30px; text-align: center; color: #4ade80;">
                        <strong>Every gradient = Parent Gradient √ó Local Gradient</strong>
                    </p>
                </div>
            </section>

            <!-- Two-Step Understanding -->
            <section>
                <h2 class="gradient">Understanding Backprop: Two Steps</h2>
                <div class="split-container" style="margin-top: 40px;">
                    <div class="split-left" style="background: #2d333b; padding: 20px; border-radius: 10px;">
                        <h3 style="text-align: center; color: #4ade80;">Step 1: WHAT are we doing?</h3>
                        <ul style="font-size: 0.8em; margin-top: 20px;">
                            <li>Finding weight adjustments</li>
                            <li>Computing gradients (‚àÇL/‚àÇW)</li>
                            <li>Accumulating across samples</li>
                            <li>Updating weights</li>
                        </ul>
                        <p style="margin-top: 20px; font-size: 0.75em; color: #888;">
                            <strong>Goal:</strong> Minimize loss by adjusting weights in the direction that reduces error
                        </p>
                    </div>
                    <div class="split-right" style="background: #2d333b; padding: 20px; border-radius: 10px;">
                        <h3 style="text-align: center; color: #4ade80;">Step 2: HOW are we doing it?</h3>
                        <ul style="font-size: 0.8em; margin-top: 20px;">
                            <li>Chain rule for derivatives</li>
                            <li>Parent gradient √ó Local gradient</li>
                            <li>Propagate from loss to input</li>
                            <li>Layer-by-layer computation</li>
                        </ul>
                        <p style="margin-top: 20px; font-size: 0.75em; color: #888;">
                            <strong>Method:</strong> Calculate how each weight affects the final loss through the chain of operations
                        </p>
                    </div>
                </div>
            </section>

            <!-- Chain Rule Formula -->
            <section>
                <h2 class="gradient">The Chain Rule: Every Layer Follows the Same Pattern</h2>
                <div style="margin-top: 40px; background: #2d333b; padding: 30px; border-radius: 10px;">
                    <h3 style="text-align: center; color: #4ade80; margin-bottom: 30px;">
                        Gradient at Layer N = Parent Gradient √ó Local Gradient
                    </h3>
                    <pre><code class="text" data-trim style="font-size: 0.75em;">
Loss
  ‚Üì
  ‚àÇL/‚àÇOutput_N          ‚Üê Start here (from loss function)
  ‚Üì
  ‚àÇL/‚àÇW_N = ‚àÇL/‚àÇOutput_N √ó ‚àÇOutput_N/‚àÇW_N
            ‚Üëparent          ‚Üëlocal

  ‚àÇL/‚àÇInput_N = ‚àÇL/‚àÇOutput_N √ó ‚àÇOutput_N/‚àÇInput_N
                ‚Üëparent          ‚Üëlocal
  ‚Üì
  (‚àÇL/‚àÇInput_N becomes parent for layer N-1)
  ‚Üì
  ‚àÇL/‚àÇW_N-1 = ‚àÇL/‚àÇOutput_N-1 √ó ‚àÇOutput_N-1/‚àÇW_N-1
              ‚Üëparent (came from above)  ‚Üëlocal

... repeat for all layers back to input
                    </code></pre>
                    <p style="margin-top: 30px; text-align: center; font-size: 0.85em;">
                        Each layer receives a <strong>parent gradient</strong> from the layer above,<br>
                        multiplies by its <strong>local gradient</strong>, and passes result to layer below
                    </p>
                </div>

                <aside class="notes">
                    The chain rule is super simple. At each layer, you get a gradient from the layer above - that's your parent gradient. You multiply it by your local gradient - how this layer changes with respect to its inputs. That's it. Then you pass the result down. Every single operation in backprop follows this same pattern.
                </aside>
            </section>

            <!-- Complete Gradient Flow Diagram -->
            <section>
                <h2 class="gradient">Complete Gradient Flow: Chain Rule in Action</h2>
                <h4 style="text-align: center; margin-bottom: 20px; color: #60a5fa;">Parent Gradient √ó Local Gradient at Every Step</h4>

                <div class="mermaid" style="transform: scale(0.75); transform-origin: top center;">
graph TB
    %% Forward Pass
    Loss["üéØ Loss<br/>L = CrossEntropy(logits, targets)<br/><br/>‚àÇL/‚àÇL = 1.0 (start here)"]

    Softmax["Softmax Backward<br/>‚àÇL/‚àÇlogits = softmax(logits) - y_true<br/><br/>Parent: ‚àÇL/‚àÇL<br/>Local: ‚àÇL/‚àÇlogits"]

    LMHead["LM Head Backward<br/>‚àÇL/‚àÇfinal_ln = d_logits @ W_embed<br/>‚àÇL/‚àÇW_embed += d_logits^T @ final_ln<br/><br/>Parent: ‚àÇL/‚àÇlogits<br/>Local: ‚àÇlogits/‚àÇfinal_ln, ‚àÇlogits/‚àÇW"]

    FinalLN["Final LayerNorm Backward<br/>‚àÇL/‚àÇlayer6_out = layernorm_backward(...)<br/><br/>Parent: ‚àÇL/‚àÇfinal_ln<br/>Local: ‚àÇfinal_ln/‚àÇlayer6_out"]

    Layer6["Layer 6 Backward<br/>Residual2: d_ln2 + d_mlp ‚Üí d_residual2<br/>MLP: FC2 ‚Üí GELU ‚Üí FC1<br/>Residual1: d_ln1 + d_attn ‚Üí d_residual1<br/>Attention: QKV projections<br/><br/>Parent: ‚àÇL/‚àÇlayer6_out<br/>Local: ‚àÇlayer6/‚àÇlayer5"]

    Layers["Layers 5 ‚Üí 4 ‚Üí 3 ‚Üí 2 ‚Üí 1<br/>Same pattern repeated<br/>Each layer:<br/>‚Ä¢ Split gradients at residuals<br/>‚Ä¢ Backward through MLP<br/>‚Ä¢ Backward through Attention<br/><br/>Parent: ‚àÇL/‚àÇlayer_n<br/>Local: ‚àÇlayer_n/‚àÇlayer_n-1"]

    PosEmbed["Position Embedding Backward<br/>‚àÇL/‚àÇtoken_embed = d_layer1_in<br/><br/>Parent: ‚àÇL/‚àÇlayer1_in<br/>Local: ‚àÇlayer1/‚àÇembeds"]

    TokenEmbed["Token Embedding Backward<br/>for each token t:<br/>  d_W_embed[tokens[t]] += d_embed[t]<br/><br/>Parent: ‚àÇL/‚àÇtoken_embed<br/>Local: ‚àÇembed/‚àÇW_embed"]

    %% Arrows showing gradient flow
    Loss -->|‚àÇL/‚àÇL = 1.0| Softmax
    Softmax -->|‚àÇL/‚àÇlogits| LMHead
    LMHead -->|‚àÇL/‚àÇfinal_ln| FinalLN
    FinalLN -->|‚àÇL/‚àÇlayer6_out| Layer6
    Layer6 -->|‚àÇL/‚àÇlayer5_out| Layers
    Layers -->|‚àÇL/‚àÇlayer1_in| PosEmbed
    PosEmbed -->|‚àÇL/‚àÇtoken_embed| TokenEmbed

    %% Styling
    classDef loss fill:#ef4444,stroke:#dc2626,stroke-width:3px,color:#fff
    classDef backward fill:#60a5fa,stroke:#3b82f6,stroke-width:2px,color:#fff
    classDef layers fill:#8b5cf6,stroke:#7c3aed,stroke-width:2px,color:#fff
    classDef embed fill:#34d399,stroke:#10b981,stroke-width:2px,color:#fff

    class Loss loss
    class Softmax,LMHead,FinalLN backward
    class Layer6,Layers layers
    class PosEmbed,TokenEmbed embed
                </div>

                <div style="margin-top: 20px; background: rgba(96, 165, 250, 0.2); border: 2px solid #60a5fa; border-radius: 10px; padding: 20px;">
                    <h3 style="color: #60a5fa; text-align: center; margin-bottom: 15px;">Key Insight: Chain Rule Cascade</h3>
                    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; color: #ccc; font-size: 0.9em;">
                        <div>
                            <p style="color: #4ade80; font-weight: bold; margin-bottom: 10px;">‚úÖ Parent Gradient</p>
                            <p>Gradient flowing <strong>from above</strong><br/>Tells us: "How does loss change with respect to this layer's output?"</p>
                        </div>
                        <div>
                            <p style="color: #ff6f00; font-weight: bold; margin-bottom: 10px;">‚úÖ Local Gradient</p>
                            <p>Gradient of <strong>this layer</strong><br/>Tells us: "How does this layer's output change with respect to its inputs/weights?"</p>
                        </div>
                    </div>
                    <p style="text-align: center; margin-top: 15px; font-size: 1.1em; color: #60a5fa; font-weight: bold;">
                        Multiply them together ‚Üí Gradient flows backward!
                    </p>
                </div>
            </section>

            <!-- Now Let's See It In Code -->
            <section>
                <h2 class="gradient">Now Let's See How We Implement This</h2>
                <h4 style="text-align: center; margin-top: 40px;">GPT-2 Backpropagation in Pure C</h4>
                <ul style="margin-top: 40px; font-size: 0.9em;">
                    <li>‚úÖ We know <strong>WHAT:</strong> Computing gradients to adjust weights</li>
                    <li>‚úÖ We know <strong>HOW:</strong> Chain rule (parent √ó local)</li>
                    <li>üöÄ Now let's see the <strong>IMPLEMENTATION:</strong></li>
                    <ul style="margin-top: 20px; font-size: 0.85em; margin-left: 40px;">
                        <li>Zero-copy data preloading</li>
                        <li>Feature-parallel gradient accumulation</li>
                        <li>AVX-512 vectorization</li>
                        <li>Atomic-free hot loops</li>
                    </ul>
                </ul>
            </section>

            <!-- Full Forward Pass Visualization -->
            <section data-auto-animate class="scrollable-slide">
                <h2 class="gradient" style="margin-bottom: 10px;">GPT-2 Forward Pass: Complete Architecture</h2>
                <p style="font-size: 0.7em; color: #888; margin-bottom: 5px;">üí° Use +/‚àí buttons or Ctrl+Scroll to zoom ‚Ä¢ Alt+Click for reveal.js zoom</p>
                <div class="svg-container" data-svg-zoom>
                    <div class="zoom-controls">
                        <button class="zoom-btn zoom-in">+</button>
                        <button class="zoom-btn zoom-out">‚àí</button>
                        <button class="zoom-btn zoom-reset">‚ü≤</button>
                    </div>
                    <img src="forward_pass_full.svg" alt="GPT-2 Forward Pass"/>
                </div>
                <div class="scroll-indicator">‚Üì Scroll for more ‚Üì</div>
            </section>

            <!-- Full Backward Pass Visualization -->
            <section data-auto-animate class="scrollable-slide">
                <h2 class="gradient" style="margin-bottom: 10px;">GPT-2 Backward Pass: Gradient Flow</h2>
                <p style="font-size: 0.7em; color: #888; margin-bottom: 5px;">üí° Use +/‚àí buttons or Ctrl+Scroll to zoom ‚Ä¢ Alt+Click for reveal.js zoom</p>
                <div class="svg-container" data-svg-zoom>
                    <div class="zoom-controls">
                        <button class="zoom-btn zoom-in">+</button>
                        <button class="zoom-btn zoom-out">‚àí</button>
                        <button class="zoom-btn zoom-reset">‚ü≤</button>
                    </div>
                    <img src="backward_pass_full.svg" alt="GPT-2 Backward Pass"/>
                </div>
                <div class="scroll-indicator">‚Üì Scroll for more ‚Üì</div>
            </section>

            <!-- Residual Connection Gradient Flow - Part 1: Concept -->
            <section data-auto-animate>
                <h2 class="gradient" style="margin-bottom: 5px;">Tricky Area #1: Residual Connections</h2>
                <h4 style="text-align: center; color: #f87171; margin-bottom: 10px; font-size: 0.9em;">Gradients Split Then Accumulate</h4>

                <div style="display: flex; justify-content: space-around; gap: 15px; margin-top: 10px; transform: scale(0.85); transform-origin: top;">
                    <!-- Forward Pass -->
                    <div style="flex: 1; border: 2px solid #4ade80; border-radius: 10px; padding: 15px; background: #1a2a1a;">
                        <h3 style="color: #4ade80; text-align: center; margin-bottom: 12px; font-size: 1.1em;">Forward Pass</h3>
                        <div style="display: flex; flex-direction: column; align-items: center; gap: 10px;">
                            <div style="background: #4ade80; padding: 10px 20px; border-radius: 8px; color: #1a1a1a; font-weight: bold; font-size: 0.9em;">Input x</div>
                            <div style="font-size: 1.5em; color: #4ade80;">‚Üì</div>
                            <div style="background: #fbbf24; padding: 10px 20px; border-radius: 8px; color: #1a1a1a; font-size: 0.9em;">F(x)<br/><span style="font-size: 0.7em;">(Attention/MLP)</span></div>
                            <div style="font-size: 1.5em; color: #4ade80;">‚Üì</div>
                            <div style="display: flex; align-items: center; gap: 10px;">
                                <div style="font-size: 1.5em; color: #4ade80; background: #2a4a2a; padding: 8px; border-radius: 50%;">+</div>
                                <div style="font-size: 1.2em; color: #4ade80;">‚Üê</div>
                                <div style="font-size: 0.8em; color: #4ade80; font-style: italic;">Skip</div>
                            </div>
                            <div style="font-size: 1.5em; color: #4ade80;">‚Üì</div>
                            <div style="background: #4ade80; padding: 10px 20px; border-radius: 8px; color: #1a1a1a; font-weight: bold; font-size: 0.9em;">Output<br/><span style="font-size: 0.75em;">x + F(x)</span></div>
                        </div>
                    </div>

                    <!-- Backward Pass -->
                    <div style="flex: 1; border: 2px solid #60a5fa; border-radius: 10px; padding: 15px; background: #1a2a3a;">
                        <h3 style="color: #60a5fa; text-align: center; margin-bottom: 12px; font-size: 1.1em;">Backward Pass</h3>
                        <div style="display: flex; flex-direction: column; align-items: center; gap: 10px;">
                            <div style="background: #60a5fa; padding: 10px 20px; border-radius: 8px; color: #1a1a1a; font-weight: bold; font-size: 0.9em;">‚àÇL/‚àÇoutput<br/><span style="font-size: 0.7em;">(from above)</span></div>
                            <div style="font-size: 1.5em; color: #60a5fa;">‚Üì</div>
                            <div style="font-size: 1.2em; color: #f87171; font-weight: bold;">SPLIT POINT</div>
                            <div style="display: flex; gap: 25px; margin-top: 8px;">
                                <div style="display: flex; flex-direction: column; align-items: center;">
                                    <div style="font-size: 0.8em; color: #60a5fa; margin-bottom: 8px;">Copy 1</div>
                                    <div style="background: #fbbf24; padding: 10px 15px; border-radius: 8px; color: #1a1a1a; font-size: 0.85em;">‚àáF(x)<br/><span style="font-size: 0.7em;">backward</span></div>
                                </div>
                                <div style="display: flex; flex-direction: column; align-items: center;">
                                    <div style="font-size: 0.8em; color: #60a5fa; margin-bottom: 8px;">Copy 2</div>
                                    <div style="background: #34d399; padding: 10px 15px; border-radius: 8px; color: #1a1a1a; font-size: 0.85em;">Identity<br/><span style="font-size: 0.7em;">‚àÇ(x)/‚àÇx = 1</span></div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <div style="margin-top: 15px; background: rgba(248, 113, 113, 0.2); border: 2px solid #f87171; border-radius: 8px; padding: 12px;">
                    <p style="color: #f87171; font-weight: bold; font-size: 0.95em; margin: 0;">‚ö†Ô∏è Common Bug: Forgetting to accumulate skip gradient ‚Üí Wrong gradients!</p>
                </div>

                <aside class="notes">
                    Residual connections are tricky. In the forward pass, you add two things. In the backward pass, the gradient splits. One copy goes through the operation backward, the other copy goes straight through unchanged. You have to add them both together. If you forget the skip gradient, your network won't train. This is probably the most common backprop bug.
                </aside>
            </section>

            <!-- Residual Connection - Part 2: Implementation -->
            <section data-auto-animate>
                <h2 class="gradient" style="margin-bottom: 8px;">Residual Connection: Implementation</h2>
                <h4 style="text-align: center; color: #ff6f00; margin-bottom: 12px;">How to Code It Correctly</h4>

                <div style="display: flex; gap: 18px; margin-top: 18px;">
                    <!-- Forward Code -->
                    <div style="flex: 1; border: 2px solid #4ade80; border-radius: 8px; padding: 18px; background: #1e1e1e;">
                        <h3 style="color: #4ade80; text-align: center; margin-bottom: 12px;">Forward (C code)</h3>
                        <pre><code class="c code-block-medium" data-trim style="font-size: 0.8em !important;">
// Residual connection
float *input = ...;
float *output = ...;

attention_forward(input, temp);

// output = input + temp
add_vectors(output, input, temp);
                        </code></pre>
                    </div>

                    <!-- Backward Code -->
                    <div style="flex: 1; border: 2px solid #60a5fa; border-radius: 8px; padding: 18px; background: #1e1e1e;">
                        <h3 style="color: #60a5fa; text-align: center; margin-bottom: 12px;">Backward (C code)</h3>
                        <pre><code class="c code-block-medium" data-trim style="font-size: 0.8em !important;">
// Gradient splits at residual
float *d_output = ...;  // from above
float *d_input = ...;

// CRITICAL: Split gradient
attention_backward(d_output, d_temp);

// Accumulate both paths
add_vectors(d_input, d_output, d_temp);
                        </code></pre>
                    </div>
                </div>

                <div style="margin-top: 18px; background: rgba(74, 222, 128, 0.2); border: 2px solid #4ade80; border-radius: 8px; padding: 18px;">
                    <h3 style="color: #4ade80; margin-bottom: 12px;">Key Points:</h3>
                    <ul style="display: inline-block; text-align: left; color: #ccc; font-size: 0.9em; line-height: 1.6;">
                        <li>‚úÖ Gradient flows to <strong>TWO</strong> paths</li>
                        <li>‚úÖ Path 1: Through F(x) backward ‚Ä¢ Path 2: Skip (identity)</li>
                        <li>‚úÖ Must <strong>accumulate</strong>: <code>d_input = d_output + d_temp</code></li>
                        <li>‚ùå Forgetting skip gradient loses information!</li>
                    </ul>
                </div>
            </section>

            <!-- Weight Tying - Part 1: The Concept -->
            <section data-auto-animate>
                <h2 class="gradient" style="margin-bottom: 10px;">Tricky Area #2: Weight Tying</h2>
                <h4 style="text-align: center; color: #f87171; margin-bottom: 25px;">Embedding Weights Used in Two Places</h4>

                <!-- Shared Weight Matrix -->
                <div style="background: linear-gradient(135deg, #ff6f00, #e65100); padding: 25px; border-radius: 12px; margin: 20px auto; max-width: 500px; border: 3px solid #e65100;">
                    <h3 style="color: white; text-align: center; margin-bottom: 12px; font-size: 1.2em;">Shared Weight Matrix</h3>
                    <p style="color: white; text-align: center; font-size: 1.3em; margin: 12px 0; font-family: monospace;">W_embed [V √ó D]</p>
                    <p style="color: #ffe0b2; text-align: center; font-size: 1em; margin: 12px 0;">[50257 √ó 1536]</p>
                    <p style="color: white; text-align: center; font-size: 0.95em; margin-top: 15px; font-weight: bold;">Single allocation in memory</p>
                </div>

                <div style="margin-top: 35px; background: rgba(255, 111, 0, 0.2); border: 2px solid #ff6f00; border-radius: 10px; padding: 25px;">
                    <h3 style="color: #ff6f00; text-align: center; margin-bottom: 15px;">Key Insight</h3>
                    <p style="color: #ccc; text-align: center; font-size: 1em; line-height: 1.6;">
                        This single weight matrix serves <strong style="color: #ff6f00;">TWO different purposes</strong><br/>
                        in the forward pass: token embedding (lookup) and output projection (matmul)
                    </p>
                </div>
            </section>

            <!-- Weight Tying - Part 2: Two Uses -->
            <section data-auto-animate>
                <h2 class="gradient" style="margin-bottom: 10px;">Weight Tying: Two Uses of W_embed</h2>
                <h4 style="text-align: center; color: #4ade80; margin-bottom: 30px;">Same Matrix, Different Operations</h4>

                <div style="display: flex; justify-content: space-around; gap: 30px; margin-top: 20px;">
                    <!-- Token Embedding -->
                    <div style="flex: 1; border: 3px solid #66bb6a; border-radius: 12px; padding: 25px; background: linear-gradient(135deg, #1a2a1a, #0a1a0a);">
                        <h3 style="color: #66bb6a; text-align: center; margin-bottom: 15px; font-size: 1.1em;">Use #1: Token Embedding</h3>
                        <p style="color: #888; text-align: center; margin-bottom: 20px; font-size: 0.9em;">(Forward Pass - Start)</p>
                        <div style="color: #ccc; font-size: 0.95em; line-height: 1.8;">
                            <p style="margin: 12px 0;"><strong style="color: #66bb6a;">Input:</strong> Token IDs [T]</p>
                            <p style="margin: 12px 0; font-family: monospace; color: #e0f7e0; font-size: 0.95em;">tokens = [15496, 2345, ...]</p>
                            <p style="margin: 18px 0 12px 0;"><strong style="color: #66bb6a;">Operation:</strong></p>
                            <p style="margin: 12px 0; font-family: monospace; color: #e0f7e0; font-size: 0.95em;">embed[t] = W_embed[tokens[t]]</p>
                            <p style="margin: 12px 0; color: #888; font-size: 0.85em;">(Lookup/Gather operation)</p>
                        </div>
                        <div style="margin-top: 20px; text-align: center; color: #4ade80; font-weight: bold; font-size: 0.95em;">‚Üí Used for lookup</div>
                    </div>

                    <!-- LM Head -->
                    <div style="flex: 1; border: 3px solid #66bb6a; border-radius: 12px; padding: 25px; background: linear-gradient(135deg, #1a2a1a, #0a1a0a);">
                        <h3 style="color: #66bb6a; text-align: center; margin-bottom: 15px; font-size: 1.1em;">Use #2: LM Head</h3>
                        <p style="color: #888; text-align: center; margin-bottom: 20px; font-size: 0.9em;">(Forward Pass - End)</p>
                        <div style="color: #ccc; font-size: 0.95em; line-height: 1.8;">
                            <p style="margin: 12px 0;"><strong style="color: #66bb6a;">Input:</strong> Hidden states [T √ó D]</p>
                            <p style="margin: 12px 0; font-family: monospace; color: #e0f7e0; font-size: 0.95em;">hidden = final_ln_output</p>
                            <p style="margin: 18px 0 12px 0;"><strong style="color: #66bb6a;">Operation:</strong></p>
                            <p style="margin: 12px 0; font-family: monospace; color: #e0f7e0; font-size: 0.95em;">logits = hidden @ W_embed^T</p>
                            <p style="margin: 12px 0; color: #888; font-size: 0.85em;">(Matrix multiply)</p>
                        </div>
                        <div style="margin-top: 20px; text-align: center; color: #4ade80; font-weight: bold; font-size: 0.95em;">‚Üí Used for projection</div>
                    </div>
                </div>
            </section>

            <!-- Weight Tying - Part 3: Gradient Code -->
            <section data-auto-animate>
                <h2 class="gradient" style="margin-bottom: 10px;">Weight Tying: Backward Pass Code</h2>
                <h4 style="text-align: center; color: #60a5fa; margin-bottom: 30px;">Both Operations Contribute Gradients</h4>

                <div style="display: flex; gap: 30px; margin-top: 20px;">
                    <!-- Token Embedding Gradient -->
                    <div style="flex: 1; border: 3px solid #81c784; border-radius: 12px; padding: 25px; background: #1e1e1e;">
                        <h3 style="color: #81c784; text-align: center; margin-bottom: 20px; font-size: 1.1em;">Token Embedding Gradient</h3>
                        <pre><code class="c code-block-medium" data-trim style="font-size: 0.85em !important;">
for (int t = 0; t < T; t++) {
  int token_id = tokens[t];

  // Scatter-add gradient
  d_W_embed[token_id] += d_embed[t];
}
                        </code></pre>
                        <p style="color: #888; text-align: center; margin-top: 20px; font-size: 0.95em; line-height: 1.6;">
                            <strong style="color: #81c784;">Scatter-add</strong> at token indices<br/>
                            Sparse gradient accumulation
                        </p>
                    </div>

                    <!-- LM Head Gradient -->
                    <div style="flex: 1; border: 3px solid #ff6f00; border-radius: 12px; padding: 25px; background: #1e1e1e;">
                        <h3 style="color: #ff6f00; text-align: center; margin-bottom: 20px; font-size: 1.1em;">LM Head Gradient</h3>
                        <pre><code class="c code-block-medium" data-trim style="font-size: 0.85em !important;">
// d_logits [T √ó V]
// hidden [T √ó D]

// Matrix multiply accumulation
d_W_embed += d_logits^T @ hidden;

// [V √ó T] @ [T √ó D] = [V √ó D]
                        </code></pre>
                        <p style="color: #888; text-align: center; margin-top: 20px; font-size: 0.95em; line-height: 1.6;">
                            <strong style="color: #ff6f00;">Dense matrix</strong> accumulation<br/>
                            Full gradient computation
                        </p>
                    </div>
                </div>
            </section>

            <!-- Weight Tying - Part 4: Why It's Critical -->
            <section data-auto-animate>
                <h2 class="gradient" style="margin-bottom: 10px;">Weight Tying: Why Both Gradients Matter</h2>
                <h4 style="text-align: center; color: #f87171; margin-bottom: 30px;">Critical Implementation Detail</h4>

                <div style="margin-top: 30px; background: rgba(248, 113, 113, 0.2); border: 3px solid #f87171; border-radius: 12px; padding: 30px;">
                    <h3 style="color: #f87171; text-align: center; margin-bottom: 20px; font-size: 1.3em;">‚ö†Ô∏è CRITICAL: Both Gradients Update SAME Matrix!</h3>
                    <p style="text-align: center; color: #f87171; font-family: monospace; font-size: 1.2em; margin: 20px 0; line-height: 1.8;">
                        ‚àÇL/‚àÇW_embed = ‚àÇL/‚àÇW_token_emb + ‚àÇL/‚àÇW_lm_head
                    </p>
                    <p style="text-align: center; color: #ccc; margin-top: 20px; font-size: 1em; line-height: 1.8;">
                        This is <strong style="color: #4ade80; font-size: 1.1em;">CORRECT</strong>! Both gradients must contribute.<br/>
                        The weight matrix is used in two places ‚Üí receives gradients from both
                    </p>
                </div>
            </section>
            <section>
                <div style="margin-top: 35px; background: rgba(74, 222, 128, 0.2); border: 3px solid #4ade80; border-radius: 12px; padding: 30px;">
                    <h3 style="color: #4ade80; margin-bottom: 20px; font-size: 1.2em; text-align: center;">Implementation Pattern:</h3>
                    <ol style="display: inline-block; text-align: left; color: #ccc; font-family: monospace; font-size: 1em; line-height: 2;">
                        <li><strong style="color: #4ade80;">Step 1:</strong> Zero gradients: <code style="color: #e0f7e0;">d_W_embed = 0</code></li>
                        <li><strong style="color: #4ade80;">Step 2:</strong> Backward LM head: <code style="color: #e0f7e0;">d_W_embed += d_logits^T @ hidden</code></li>
                        <li><strong style="color: #4ade80;">Step 3:</strong> Backward embedding: <code style="color: #e0f7e0;">d_W_embed[token_id] += d_embed[t]</code></li>
                    </ol>
                    <p style="text-align: center; margin-top: 25px; color: #4ade80; font-size: 1em;">
                        ‚úÖ Both operations accumulate into the same gradient buffer
                    </p>
                </div>

                <aside class="notes">
                    Weight tying means one weight matrix gets used in two places. Token embedding at the start, and the output projection at the end. So in the backward pass, that same matrix gets gradients from both places. You have to accumulate them both. This saves parameters but makes backprop a bit more careful.
                </aside>
            </section>

            <!-- FC1 Gradient - Part 1a: The Approach -->
            <section data-auto-animate>
                <h2 class="gradient" style="margin-bottom: 10px;">FC1 Layer Gradient: The Problem</h2>
                <h4 style="text-align: center; color: #f87171; margin-bottom: 20px;">‚ùå Token-Parallel Approach (SLOW)</h4>

                <!-- Forward Pass Reference -->
                <div style="background: #2a2a2a; border: 2px solid #4ade80; border-radius: 10px; padding: 20px; margin-bottom: 25px;">
                    <h3 style="color: #4ade80; text-align: center; margin-bottom: 15px;">Forward Pass (for reference)</h3>
                    <p style="text-align: center; font-family: monospace; color: #ccc; font-size: 1em;">
                        Output [T √ó 4D] = Input [T √ó D] @ W_fc1 [D √ó 4D] + bias [4D]
                    </p>
                    <p style="text-align: center; color: #888; font-size: 0.9em; margin-top: 10px;">
                        Example: [1024 √ó 1536] @ [1536 √ó 6144] + [6144] = [1024 √ó 6144]
                    </p>
                </div>
            </section>
            <section>
                <!-- Token-Parallel Implementation -->
                <div style="border: 3px solid #f87171; border-radius: 15px; padding: 25px; background: #2a1a1a;">
                    <h3 style="color: #f87171; text-align: center; margin-bottom: 20px;">Token-Parallel: Parallelize Over Output √ó Input</h3>
                    <pre><code class="c code-block-medium" data-trim>
#pragma omp parallel for collapse(2)
for (int out_idx = 0; out_idx < 4*D; out_idx++) {
  for (int in_idx = 0; in_idx < D; in_idx++) {
    float grad_sum = 0.0f;

    // Accumulate over tokens
    for (int t = 0; t < T; t++) {
      grad_sum += d_output[t * 4*D + out_idx] *
                  input[t * D + in_idx];
    }

    #pragma omp atomic  // ‚ö†Ô∏è BOTTLENECK!
    d_W_fc1[out_idx * D + in_idx] += grad_sum;
  }
}
                    </code></pre>
                </div>

                <aside class="notes">
                    The naive way to compute FC1 gradients is to parallelize over both output and input dimensions. But this creates a problem - multiple threads need to update the same weight, so you need atomic operations. For a 1536 by 6144 weight matrix, that's 9 million atomic operations. That's slow.
                </aside>
            </section>

            <!-- FC1 Gradient - Part 1b: The Problems -->
            <section data-auto-animate>
                <h2 class="gradient" style="margin-bottom: 10px;">FC1 Layer Gradient: Why It's SLOW</h2>
                <h4 style="text-align: center; color: #f87171; margin-bottom: 30px;">Three Major Performance Bottlenecks</h4>

                <div style="display: grid; grid-template-columns: 1fr; gap: 25px; max-width: 1000px; margin: 0 auto;">
                    <!-- Problem 1 -->
                    <div style="background: rgba(248, 113, 113, 0.2); border: 3px solid #f87171; border-radius: 12px; padding: 25px;">
                        <h3 style="color: #f87171; margin-bottom: 15px;">‚ùå Problem 1: Atomic Operations Everywhere</h3>
                        <p style="color: #ccc; font-size: 1em; line-height: 1.6;">
                            <strong style="color: #ff6f00;">9,437,184 atomic operations</strong> (1536 √ó 6144 weights)<br/>
                            ‚Ä¢ Every thread must atomically update shared weight memory<br/>
                            ‚Ä¢ Massive thread contention and synchronization overhead<br/>
                            ‚Ä¢ CPU spends more time waiting than computing
                        </p>
                    </div>
                </div>
            </section>
            <section>   
                <div style="display: grid; grid-template-columns: 1fr; gap: 25px; max-width: 1000px; margin: 0 auto;">
                    <!-- Problem 2 -->
                    <div style="background: rgba(248, 113, 113, 0.2); border: 3px solid #f87171; border-radius: 12px; padding: 25px;">
                        <h3 style="color: #f87171; margin-bottom: 15px;">‚ùå Problem 2: No SIMD Vectorization</h3>
                        <p style="color: #ccc; font-size: 1em; line-height: 1.6;">
                            ‚Ä¢ Scalar accumulation only (one element at a time)<br/>
                            ‚Ä¢ Cannot use AVX-512 FMA instructions<br/>
                            ‚Ä¢ Missing <strong style="color: #ff6f00;">16√ó speedup</strong> from vector operations<br/>
                            ‚Ä¢ Modern CPU capabilities completely wasted
                        </p>
                    </div>

                    <!-- Problem 3 -->
                    <div style="background: rgba(248, 113, 113, 0.2); border: 3px solid #f87171; border-radius: 12px; padding: 25px;">
                        <h3 style="color: #f87171; margin-bottom: 15px;">‚ùå Problem 3: Cache Thrashing</h3>
                        <p style="color: #ccc; font-size: 1em; line-height: 1.6;">
                            ‚Ä¢ Multiple threads write to same cache lines (64 bytes)<br/>
                            ‚Ä¢ Constant cache invalidation across cores<br/>
                            ‚Ä¢ False sharing destroys cache coherency<br/>
                            ‚Ä¢ Memory bandwidth saturated with coherency traffic
                        </p>
                    </div>
                </div>

                <aside class="notes">
                    Three big problems. First, atomic operations everywhere - threads fighting for the same memory. Second, no SIMD vectorization possible because of the atomics. Third, cache thrashing - threads invalidating each other's cache lines. These all compound to make it really slow.
                </aside>
            </section>

            <!-- Visual Comparison: Token vs Feature Parallel -->
            <section data-auto-animate class="scrollable-slide">
                <h2 class="gradient" style="margin-bottom: 10px;">The Problem Visualized</h2>
                <p style="font-size: 0.7em; color: #888; margin-bottom: 5px;">üí° Use +/‚àí buttons or Ctrl+Scroll to zoom</p>
                <div class="svg-container" data-svg-zoom>
                    <div class="zoom-controls">
                        <button class="zoom-btn zoom-in">+</button>
                        <button class="zoom-btn zoom-out">‚àí</button>
                        <button class="zoom-btn zoom-reset">‚ü≤</button>
                    </div>
                    <img src="feature_vs_token_parallel_improved.svg" alt="Token-Parallel vs Feature-Parallel Comparison"/>
                </div>
                <div class="scroll-indicator">‚Üì Scroll for more ‚Üì</div>

                <aside class="notes">
                    This visualization shows the core problem. On the left, token-parallel means every thread tries to update every weight - that's why you need locks. On the right, feature-parallel means each thread owns one row completely. No other thread touches it. That's why no locks are needed. The tangled arrows versus clean arrows tell the whole story.
                </aside>
            </section>

            <!-- FC1 Gradient - Part 2a: The Solution Code -->
            <section data-auto-animate>
                <h2 class="gradient" style="margin-bottom: 10px;">FC1 Layer Gradient: The Solution</h2>
                <h4 style="text-align: center; color: #34d399; margin-bottom: 20px;">‚úÖ Feature-Parallel Approach (FAST)</h4>

                <!-- Feature-Parallel Implementation -->
                <div style="border: 3px solid #34d399; border-radius: 15px; padding: 30px; background: #1a2a2a;">
                    <h3 style="color: #34d399; text-align: center; margin-bottom: 25px;">Feature-Parallel: Parallelize Over OUTPUT Features Only</h3>
                    <pre><code class="c code-block-medium" data-trim>
#pragma omp parallel for schedule(dynamic, 1)
for (int out_idx = 0; out_idx < 4*D; out_idx++) {
  float *dst_row = d_W_fc1 + out_idx * D;

  // SIMD over input features
  for (int in_idx = 0; in_idx < D; in_idx += 16) {
    __m512 accum = _mm512_setzero_ps();

    // Accumulate over tokens
    for (int t = 0; t < T; t++) {
      __m512 input_vec =
        _mm512_load_ps(input + t*D + in_idx);
      __m512 grad = _mm512_set1_ps(
        d_output[t * 4*D + out_idx]);
      accum = _mm512_fmadd_ps(grad, input_vec, accum);
    }

    __m512 prev = _mm512_load_ps(dst_row + in_idx);
    _mm512_store_ps(dst_row + in_idx,
                    _mm512_add_ps(prev, accum));
    // ‚úÖ NO ATOMIC! Thread owns this row
  }
}
                    </code></pre>
                </div>

                <div style="text-align: center; margin-top: 35px; padding: 25px; background: rgba(52, 211, 153, 0.2); border: 2px solid #34d399; border-radius: 12px;">
                    <p style="color: #34d399; font-size: 1.2em; font-weight: bold;">Key Insight: Each thread owns a complete row ‚Üí No conflicts!</p>
                </div>

                <aside class="notes">
                    The fix is simple. Only parallelize over output features. Each thread gets one complete row of the gradient matrix. No other thread touches that row. So no atomics needed. And since you're working on contiguous memory, you can use AVX-512 to process 16 elements at a time. This is the key to making CPU training fast.
                </aside>
            </section>
            <!-- New Infographic Slide -->
            <section>
                <h2 class="gradient">Feature-Parallel Dataflow</h2>
                <h4 style="color:#94a3b8; margin-bottom:20px;">Thread-local rows ‚Üí single global reduction</h4>
                <div class="svg-container">
                    <img src="feature_parallel_dataflow.svg" alt="Feature-parallel gradient dataflow diagram">
                </div>
                <aside class="notes">
                    Visual reinforcement: each thread consumes its own cache-line sized chunk, builds a thread-local accumulator, then we do one final reduction per row. This is why no atomics are needed.
                </aside>
            </section>

            <!-- FC1 Gradient - Part 2b: The Benefits -->
            <section data-auto-animate>
                <h2 class="gradient" style="margin-bottom: 10px;">FC1 Layer Gradient: Why It's FAST</h2>
                <h4 style="text-align: center; color: #34d399; margin-bottom: 30px;">Three Major Performance Wins</h4>

                <div style="display: grid; grid-template-columns: 1fr; gap: 25px; max-width: 1000px; margin: 0 auto;">
                    <!-- Advantage 1 -->
                    <div style="background: rgba(52, 211, 153, 0.2); border: 3px solid #34d399; border-radius: 12px; padding: 25px;">
                        <h3 style="color: #34d399; margin-bottom: 15px;">‚úÖ Advantage 1: ZERO Atomic Operations</h3>
                        <p style="color: #ccc; font-size: 1em; line-height: 1.6;">
                            ‚Ä¢ Each thread owns <strong style="color: #4ade80;">one complete output row</strong><br/>
                            ‚Ä¢ Disjoint memory regions ‚Üí no thread conflicts<br/>
                            ‚Ä¢ No synchronization overhead whatsoever<br/>
                            ‚Ä¢ Threads run at full speed without waiting
                        </p>
                    </div>
                </div>
            </section>
            <section data-auto-animate>    
                <div style="display: grid; grid-template-columns: 1fr; gap: 25px; max-width: 1000px; margin: 0 auto;">

                    <!-- Advantage 2 -->
                    <div style="background: rgba(52, 211, 153, 0.2); border: 3px solid #34d399; border-radius: 12px; padding: 25px;">
                        <h3 style="color: #34d399; margin-bottom: 15px;">‚úÖ Advantage 2: Full AVX-512 SIMD Vectorization</h3>
                        <p style="color: #ccc; font-size: 1em; line-height: 1.6;">
                            ‚Ä¢ <strong style="color: #4ade80;">16-wide FMA</strong>: Process 16 elements per iteration<br/>
                            ‚Ä¢ Uses _mm512_fmadd_ps() for fused multiply-add<br/>
                            ‚Ä¢ Achieves theoretical peak FLOPS on CPU<br/>
                            ‚Ä¢ Up to 16√ó speedup over scalar code
                        </p>
                    </div>

                    <!-- Advantage 3 -->
                    <div style="background: rgba(52, 211, 153, 0.2); border: 3px solid #34d399; border-radius: 12px; padding: 25px;">
                        <h3 style="color: #34d399; margin-bottom: 15px;">‚úÖ Advantage 3: Cache-Friendly Memory Access</h3>
                        <p style="color: #ccc; font-size: 1em; line-height: 1.6;">
                            ‚Ä¢ Each thread works on <strong style="color: #4ade80;">contiguous memory</strong><br/>
                            ‚Ä¢ Perfect cache line utilization (64-byte aligned)<br/>
                            ‚Ä¢ No cache invalidation between threads<br/>
                            ‚Ä¢ Maximizes memory bandwidth efficiency
                        </p>
                    </div>
                </div>
            </section>
            <section data-auto-animate>    
                <div style="text-align: center; margin-top: 30px; padding: 25px; background: linear-gradient(135deg, #ff6f00, #e65100); border-radius: 12px;">
                    <h3 style="color: white; font-size: 1.6em;">Expected Speedup: 5-10√ó</h3>
                    <p style="color: #ffe0b2; margin-top: 10px; font-size: 1.1em;">(Atomics removal + SIMD + Cache optimization)</p>
                </div>

                <aside class="notes">
                    So you get three wins. Zero atomics means threads never wait for each other. Full SIMD means you process 16 floats per instruction instead of one. And cache-friendly access means you're not constantly moving data around. Put it together and you get 5 to 10 times faster than the naive approach.
                </aside>
            </section>

            <!-- Training Results -->
            <section data-auto-animate>
                <h2 class="gradient">Training Results: It Works!</h2>
                <div class="perf-comparison">
                    <div class="perf-card slow">
                        <h3>Initial Loss</h3>
                        <div class="perf-number slow">8.23</div>
                        <p>Perplexity: 3762</p>
                    </div>
                    <div class="perf-card fast">
                        <h3>Final Loss</h3>
                        <div class="perf-number fast">0.85</div>
                        <p>Perplexity: 2.34</p>
                    </div>
                </div>
                <p style="margin-top: 40px; font-size: 1em; color: #4ade80; text-align: center;">
                    ‚úÖ Smooth convergence validates backpropagation correctness<br>
                    ‚úÖ Teacher forcing with stride=1 for maximum gradient validation
                </p>

                <aside class="notes">
                    The proof is in the numbers. We went from loss of 8.23 down to 0.85. Perplexity of 2.34 means the model actually learned something. If there was a bug in backprop, you'd see the loss stall or explode. Smooth convergence tells you everything is correct.
                </aside>
            </section>

            <!-- Training Data Preloading -->
            <section>
                <h2 class="gradient">Zero-Copy Training Data</h2>
                <h4>Preload 78 MB Once, Zero I/O Per Step</h4>
                <pre><code class="c code-block-small" data-trim>
typedef struct {
    uint32_t *cache_base;     // Points inside arena (do not free)
    size_t count;             // 19,008 training pairs
    size_t tokens_per_pair;   // 1,025 tokens (ctx + 1)
    size_t stride;            // 1,040 (64-byte aligned)
} PreloadedTrainingData;

// Load all training pairs into arena
bool preload_all_training_windows(const TrainingPairList *list,
                                   TransformerModel *M,
                                   PreloadedTrainingData *out) {
    uint32_t *cache_base = M->memory_base + M->gradients.training_pair_tokens_offset;

    #pragma omp parallel for num_threads(M->num_cores)
    for (size_t i = 0; i &lt; list-&gt;count; ++i) {
        uint32_t *buffer = cache_base + i * stride;
        FILE *fp = fopen(list->paths[i], "rb");
        fread(buffer, sizeof(uint32_t), tokens_per_pair, fp);
        fclose(fp);
    }
    return true;
}
                </code></pre>

                <aside class="notes">
                    We preload all training data once at startup. All 19,000 training pairs go into the arena. After that, training is pure computation - no file I/O at all. Each step just picks a random pair from memory. Simple and fast.
                </aside>
            </section>

            <!-- Training Loop -->
            <section>
                <h2 class="gradient">Training Loop: Direct Arena Access</h2>
                <pre class="fragment fade-in"><code class="c code-block-medium" data-trim>
for (int step = 0; step &lt; total_steps; ++step) {
    size_t current = pair_index;
    pair_index = (pair_index + 1) % preloaded.count;

    // Zero-copy: direct pointer into arena
    uint32_t *pair_tokens = preloaded.cache_base + current * preloaded.stride;
    int32_t *input_tokens = (int32_t *)pair_tokens;
    int32_t *target_tokens = input_tokens + 1;  // Next-token offset

    float loss = training_step(M, input_tokens, target_tokens, learning_rate);

    if ((step + 1) % log_interval == 0) {
        float ppl = expf(loss);
        printf("[train] step=%d  loss=%.6f  perplexity=%.2f\n",
               step + 1, loss, ppl);
    }
}
                </code></pre>
                <p style="margin-top: 20px; font-size: 0.7em;">
                    <strong>Before:</strong> 10-50 sec disk I/O per step<br>
                    <strong>After:</strong> 0 sec I/O (6-12√ó speedup)
                </p>
            </section>

            <!-- Backward Pass Flow -->
            <section>
                <h2 class="gradient">Backward Pass: End-to-End Flow</h2>
                <div class="gradient-flow">
                    <div class="grad-step">1. Compute Cross-Entropy Loss</div>
                    <div class="arrow-down"></div>
                    <div class="grad-step">2. Backward LM Head (Weight Tying)</div>
                    <div class="arrow-down"></div>
                    <div class="grad-step">3. Backward Final LayerNorm</div>
                    <div class="arrow-down"></div>
                    <div class="grad-step">4. Backward Transformer Layers (N ‚Üí 0)</div>
                    <div class="arrow-down"></div>
                    <div class="grad-step">5. Backward Embedding Layer</div>
                    <div class="arrow-down"></div>
                    <div class="grad-step optimized">6. Update Weights (SGD)</div>
                </div>
            </section>

            <!-- Gradient Lifecycle -->
            <section>
                <h2 class="gradient">Gradient Lifecycle: Zero ‚Üí Accumulate ‚Üí Update</h2>
                <pre class="fragment fade-in"><code class="c code-block-medium" data-trim>
float training_step(TransformerModel *M,
                    int32_t *input_tokens,
                    int32_t *target_tokens,
                    float learning_rate) {

    // ======== FORWARD PASS ========
    embed_tokens(M, input_tokens, M->context_window);
    for (int layer = 0; layer &lt; M-&gt;num_layers; layer++) {
        transformer_layer_forward(M, layer, current_input);
    }
    layernorm_token_parallel(M, ...);
    compute_logits_last_token_optimized(M);

    // ======== BACKWARD PASS ========
    zero_gradients(M);                        // 1. Zero ALL gradients
    cache_forward_activations(M);             // 2. Save forward data

    float loss;
    compute_cross_entropy_loss(M, target_tokens, &loss);

    backward_lm_head(M);
    backward_final_layernorm(M);
    for (int layer = M->num_layers - 1; layer >= 0; layer--) {
        backward_transformer_layer(M, layer);
    }
    backward_embedding_layer(M);

    // ======== WEIGHT UPDATE ========
    update_all_weights_sgd(M, learning_rate);  // 3. Apply gradients

    return loss;
}
                </code></pre>

                <aside class="notes">
                    This is the complete training loop. Forward pass to get predictions. Zero all gradients. Backward pass to compute gradients. Update weights. That's it. The backward pass is where all the magic happens - chain rule applied layer by layer, accumulating gradients as we go.
                </aside>
            </section>

            <!-- Problem: Atomic Operations -->
            <section>
                <h2 class="gradient">The Problem: Atomic Bottleneck</h2>
                <h4 style="color: #f87171;">Reference Implementation (SLOW)</h4>
                <pre class="fragment fade-in"><code class="c code-block-medium" data-trim>
// Parallelize over output features √ó input features
#pragma omp parallel for collapse(2)
for (int out_idx = 0; out_idx &lt; 4*D; out_idx++) {
    for (int in_idx = 0; in_idx &lt; D; in_idx++) {
        float grad_sum = 0.0f;
        for (int t = 0; t &lt; T; t++) {
            grad_sum += d_output[t * 4*D + out_idx] *
                        fc1_input[t * D + in_idx];
        }
        #pragma omp atomic  // ‚ö†Ô∏è BOTTLENECK: Thread contention!
        d_W_fc1[out_idx * D + in_idx] += grad_sum;
    }
}
                </code></pre>
                <p style="margin-top: 20px; font-size: 0.8em; color: #f87171;">
                    ‚ùå Every gradient element requires atomic operation<br>
                    ‚ùå Thread contention on cache lines<br>
                    ‚ùå Scalar accumulation (no SIMD)
                </p>
            </section>

            <!-- Solution: Feature-Parallel -->
            <section>
                <h2 class="gradient">The Solution: Feature-Parallel Gradients</h2>
                <h4 style="color: #4ade80;">Optimized Implementation (FAST)</h4>
                <pre class="fragment fade-in"><code class="c code-block-medium" data-trim>
// Parallelize over OUTPUT features only
#pragma omp parallel for schedule(dynamic, 1)
for (int out_idx = 0; out_idx &lt; 4*D; out_idx++) {  // Each thread owns ONE row
    float *dst_row = d_W_fc1 + out_idx * D;

    for (int in_idx = 0; in_idx &lt; D; in_idx += 16) {  // SIMD vectorization
        __m512 accum = _mm512_setzero_ps();

        for (int t = 0; t &lt; T; t++) {
            __m512 input_vec = _mm512_load_ps(fc1_input + t * D + in_idx);
            __m512 grad_broadcast = _mm512_set1_ps(d_output[t * 4*D + out_idx]);
            accum = _mm512_fmadd_ps(grad_broadcast, input_vec, accum);
        }

        __m512 prev = _mm512_load_ps(dst_row + in_idx);
        _mm512_store_ps(dst_row + in_idx, _mm512_add_ps(prev, accum));
        // ‚úÖ NO ATOMIC! Each thread owns disjoint memory
    }
}
                </code></pre>
            </section>

            <!-- Why It Works -->
            <section>
                <h2 class="gradient">Why Feature-Parallel Works</h2>
                <div class="split-container">
                    <div class="split-left">
                        <h4 style="color: #f87171;">Token-Parallel (BAD)</h4>
                        <pre><code class="text" data-trim style="font-size: 0.5em;">
Thread 0: Token 0..256
  ‚Üí Updates ALL rows of d_W
     (needs atomics)

Thread 1: Token 256..512
  ‚Üí Updates ALL rows of d_W
     (needs atomics)

Thread 2: Token 512..768
  ‚Üí Updates ALL rows of d_W
     (needs atomics)

‚ùå All threads write to same rows
‚ùå Requires atomic operations
                        </code></pre>
                    </div>
                    <div class="split-right">
                        <h4 style="color: #4ade80;">Feature-Parallel (GOOD)</h4>
                        <pre><code class="text" data-trim style="font-size: 0.5em;">
Thread 0: Output feature 0
  ‚Üí Updates ONLY row 0 of d_W
     (no atomics needed)

Thread 1: Output feature 1
  ‚Üí Updates ONLY row 1 of d_W
     (no atomics needed)

Thread 2: Output feature 2
  ‚Üí Updates ONLY row 2 of d_W
     (no atomics needed)

‚úÖ Each thread owns ONE row
‚úÖ Disjoint memory access
‚úÖ SIMD vectorization within row
                        </code></pre>
                    </div>
                </div>
            </section>

            <!-- Memory Layout -->
            <section>
                <h2 class="gradient">Memory Layout: Cache-Line Ownership</h2>
                <pre><code class="text" data-trim style="font-size: 0.6em;">
d_W_fc1 [4D √ó D] gradient matrix (row-major):

Row 0:  [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] ‚Üê Thread 0 (64-byte aligned, owns entire row)
Row 1:  [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] ‚Üê Thread 1 (64-byte aligned, owns entire row)
Row 2:  [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] ‚Üê Thread 2 (64-byte aligned, owns entire row)
...
Row 4D: [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] ‚Üê Thread 4D-1

Within each row, AVX-512 processes 16 floats at a time:
[f0 f1 f2 ... f15] [f16 f17 ... f31] [f32 f33 ... f47] ...
 ‚Üê‚îÄ __m512 vec ‚îÄ‚Üí   ‚Üê‚îÄ __m512 vec ‚îÄ‚Üí   ‚Üê‚îÄ __m512 vec ‚îÄ‚Üí

‚úÖ No cache line sharing between threads
‚úÖ No false sharing
‚úÖ Perfect memory alignment for SIMD
                </code></pre>
            </section>

            <!-- Bias Gradients -->
            <section>
                <h2 class="gradient">Bias Gradients: Also Atomic-Free!</h2>
                <pre><code class="c code-block-small" data-trim>
// Feature-parallel bias gradient (no atomics)
#pragma omp parallel for schedule(static)
for (int out_idx = 0; out_idx &lt; 4*D; out_idx++) {
    float bias_grad = 0.0f;
    for (int t = 0; t &lt; T; t++) {
        bias_grad += d_output[t * 4*D + out_idx];
    }
    d_b_fc1[out_idx] += bias_grad;  // ‚úÖ NO ATOMIC (each thread owns one element)
}
                </code></pre>
                <p style="margin-top: 20px; font-size: 0.8em;">
                    <strong>Why no atomic?</strong><br>
                    Each thread processes ONE output feature index ‚Üí writes to ONE bias element<br>
                    No overlap = no contention
                </p>
            </section>

            <!-- What Gets Optimized -->
            <section>
                <h2 class="gradient">Coverage: 93% of Gradients Optimized</h2>
                <div class="split-container">
                    <div class="split-left">
                        <h4 style="color: #4ade80;">‚úÖ Feature-Parallel (FAST)</h4>
                        <ul style="font-size: 0.7em;">
                            <li><strong>FC1 weight gradient:</strong> 9.4M elements</li>
                            <li><strong>FC2 weight gradient:</strong> 9.4M elements</li>
                            <li><strong>Q/K/V weight gradients:</strong> 7.1M elements</li>
                            <li><strong>Per layer:</strong> ~26M elements</li>
                            <li><strong>Total (6 layers):</strong> 156M elements</li>
                        </ul>
                    </div>
                    <div class="split-right">
                        <h4 style="color: #f87171;">‚ö†Ô∏è Still Uses Atomics (SMALL)</h4>
                        <ul style="font-size: 0.7em;">
                            <li><strong>Proj weight gradient:</strong> 2.4M elements</li>
                            <li><strong>Attention gradients:</strong> 8.4M elements</li>
                            <li><strong>LayerNorm params:</strong> 3K elements</li>
                            <li><strong>Per layer:</strong> ~11M elements</li>
                            <li><strong>Total (6 layers):</strong> 66M elements</li>
                        </ul>
                    </div>
                </div>
                <p style="margin-top: 20px; font-size: 0.9em; text-align: center; color: #4ade80;">
                    <strong>156M / (156M + 66M) = 70%</strong> of all gradient work is atomic-free!
                </p>
            </section>

            <!-- Alignment Verification -->
            <section>
                <h2 class="gradient">Memory Alignment: SIMD Safety</h2>
                <pre><code class="c code-block-small" data-trim>
// Base alignment from mmap (2 MB hugepages)
void *p = mmap(NULL, len, PROT_READ | PROT_WRITE,
               MAP_PRIVATE | MAP_ANONYMOUS | MAP_HUGETLB, -1, 0);
// Guaranteed ‚â•4096-byte alignment

// Dimension alignment
#define CACHE_ALIGN 64ULL
size_t aligned_embed_dim = align_up(M->embed_dim, CACHE_ALIGN / sizeof(float));
// aligned_embed_dim = multiple of 16 floats = 64 bytes

// Bump allocator
static inline size_t bump(size_t *off, size_t count, size_t alignB) {
    *off = align_up(*off, alignB / sizeof(float));
    size_t here = *off;
    *off += count;
    return here;
}

‚úÖ All pointers are 64-byte aligned
‚úÖ _mm512_load_ps() is safe (requires 64-byte alignment)
‚úÖ No segfaults, no performance penalty
                </code></pre>
            </section>

            <!-- Performance Comparison -->
            <section>
                <h2 class="gradient">Performance: Before vs After</h2>
                <div class="perf-comparison">
                    <div class="perf-card slow">
                        <h3>Reference (Atomics)</h3>
                        <div class="perf-number slow">3.0</div>
                        <p>GFLOPS (0.6% of peak)</p>
                        <p style="font-size: 0.7em; margin-top: 10px;">
                            ‚ùå 120 sec/step<br>
                            ‚ùå Atomic contention<br>
                            ‚ùå Scalar accumulation
                        </p>
                    </div>
                    <div class="perf-card fast">
                        <h3>Optimized (Feature-Parallel)</h3>
                        <div class="perf-number fast">10-15</div>
                        <p>GFLOPS (2-3% of peak)</p>
                        <p style="font-size: 0.7em; margin-top: 10px;">
                            ‚úÖ 30-40 sec/step<br>
                            ‚úÖ Zero atomics in hot loops<br>
                            ‚úÖ AVX-512 vectorization
                        </p>
                    </div>
                </div>
                <p style="margin-top: 40px; font-size: 0.8em; text-align: center;">
                    <strong>3-4√ó overall speedup expected</strong> (I/O + SIMD + atomics removal)
                </p>
            </section>

            <!-- Code Architecture -->
            <section>
                <h2 class="gradient">Code Architecture: Compile-Time Selection</h2>
                <pre><code class="c code-block-small" data-trim>
// Feature flags (enabled by default)
#define USE_FEATURE_PARALLEL_FC2 1
#define USE_FEATURE_PARALLEL_LINEAR 1

// Dispatcher functions
void backward_fc1(TransformerModel *M, ...) {
#if USE_FEATURE_PARALLEL_FC2
    backward_fc1_feature_parallel(M, ...);  // ‚úÖ Fast path
#else
    backward_fc1_reference(M, ...);         // ‚ö†Ô∏è Slow path (atomics)
#endif
}

void backward_fc2(TransformerModel *M, ...) {
#if USE_FEATURE_PARALLEL_FC2
    backward_fc2_feature_parallel(M, ...);  // ‚úÖ Fast path
#else
    backward_fc2_reference(M, ...);         // ‚ö†Ô∏è Slow path (atomics)
#endif
}

void backward_linear(TransformerModel *M, ...) {  // Q/K/V projections
#if USE_FEATURE_PARALLEL_LINEAR
    backward_linear_feature_parallel(M, ...);  // ‚úÖ Fast path
#else
    backward_linear_reference(M, ...);         // ‚ö†Ô∏è Slow path (atomics)
#endif
}
                </code></pre>
            </section>

            <!-- Numerical Validation -->
            <section>
                <h2 class="gradient">Numerical Validation: Perplexity Proves Correctness</h2>
                <canvas id="lossChart" width="800" height="400"></canvas>
            </section>
            <section>
                <p style="margin-top: 20px; font-size: 0.8em; text-align: center;">
                    ‚úÖ Smooth loss curve (8.23 ‚Üí 0.85) proves NO gradient bugs<br>
                    ‚úÖ Final perplexity 2.34 validates mathematical correctness<br>
                    ‚úÖ Stride=1 training provides maximum gradient validation
                </p>
            </section>

            <!-- Key Insights -->
            <section>
                <h2 class="gradient">Key Insights</h2>
                <ul style="margin-top: 40px; font-size: 0.85em;">
                    <li><strong>Preloading eliminates I/O:</strong> 78 MB loaded once, zero disk access per step (6-12√ó speedup)</li>
                    <li><strong>Feature-parallel = atomic-free:</strong> Each thread owns one output feature row (no contention)</li>
                    <li><strong>SIMD within rows:</strong> AVX-512 processes 16 floats per iteration (5-10√ó speedup)</li>
                    <li><strong>Memory alignment matters:</strong> 64-byte alignment enables safe _mm512_load_ps()</li>
                    <li><strong>Coverage is key:</strong> 70% of gradient work optimized = 3-4√ó overall speedup</li>
                    <li><strong>Validation through results:</strong> Perplexity 2.34 proves backprop is correct</li>
                </ul>
            </section>

            <!-- Future Work -->
            <section>
                <h2 class="gradient">What's Next?</h2>
                <ul style="margin-top: 40px; font-size: 0.9em;">
                    <li>‚úÖ Optimize remaining atomic operations (attention, projection)</li>
                    <li>‚úÖ Benchmark on Intel Xeon 6 Sapphire Rapids (target: 50-150 GFLOPS)</li>
                    <li>‚úÖ Add dynamic batch size support (currently batch=1)</li>
                    <li>‚úÖ Implement mixed-precision training (BF16 with AMX)</li>
                    <li>‚úÖ Scale to multi-socket systems (NUMA-aware parallelization)</li>
                    <li>‚úÖ Compare with GPU training (cost, power, memory capacity)</li>
                </ul>
            </section>

            <!-- Final Slide -->
            <section>
                <h1 class="gradient">Thank You!</h1>
                <p style="margin-top: 40px; font-size: 0.8em; color: #888;">
                    ANTSHIV ROBOTICS - Building the Future on CPUs
                </p>
            </section>
        </div>
    </div>

    <script src="../reveal.js/dist/reveal.js"></script>
    <script src="../reveal.js/plugin/notes/notes.js"></script>
    <script src="../reveal.js/plugin/markdown/markdown.js"></script>
    <script src="../reveal.js/plugin/highlight/highlight.js"></script>
    <script src="../reveal.js/plugin/zoom/zoom.js"></script>
    <script>
        Reveal.initialize({
            hash: true,
            transition: 'slide',
            backgroundTransition: 'fade',
            transitionSpeed: 'default',
            plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealZoom ],
            // Enable fragments to appear one by one
            fragments: true,
            // Enable smooth zoom with alt+click
            zoom: {
                scale: 2,
                transition: 'transform 0.8s ease'
            }
        });

        // GSAP Animations for custom elements
        Reveal.on('slidechanged', event => {
            // Reset scroll position for scrollable slides
            const scrollableSlides = event.currentSlide.querySelectorAll('.scrollable-slide');
            scrollableSlides.forEach(slide => {
                slide.scrollTop = 0;
            });

            // Animate SVG diagrams on slide entry
            const svgs = event.currentSlide.querySelectorAll('img[src$=".svg"]');
            if (svgs.length > 0) {
                gsap.from(svgs, {
                    duration: 0.8,
                    scale: 0.95,
                    opacity: 0,
                    ease: 'power2.out'
                });
            }

            // Animate gradient flow steps
            const gradSteps = event.currentSlide.querySelectorAll('.grad-step:not(.fragment)');
            if (gradSteps.length > 0) {
                gsap.from(gradSteps, {
                    duration: 0.6,
                    y: 30,
                    opacity: 0,
                    stagger: 0.15,
                    ease: 'back.out(1.2)'
                });
            }

            // Animate performance cards
            const perfCards = event.currentSlide.querySelectorAll('.perf-card');
            if (perfCards.length > 0) {
                gsap.from(perfCards, {
                    duration: 0.7,
                    scale: 0.9,
                    opacity: 0,
                    stagger: 0.2,
                    ease: 'elastic.out(1, 0.5)'
                });
            }
        });

        // Simplified zoom controls for SVG diagrams
        document.addEventListener('DOMContentLoaded', function() {
            const scaleStep = 0.3;
            const minScale = 0.7;
            const maxScale = 2.5;

            // Handle zoom controls
            document.querySelectorAll('.svg-container').forEach(container => {
                const img = container.querySelector('img');
                const zoomInBtn = container.querySelector('.zoom-in');
                const zoomOutBtn = container.querySelector('.zoom-out');
                const zoomResetBtn = container.querySelector('.zoom-reset');

                let scale = 1;

                function updateTransform() {
                    gsap.to(img, {
                        scale: scale,
                        duration: 0.4,
                        ease: 'power2.out'
                    });

                    // Enable scrolling when zoomed
                    if (scale > 1) {
                        container.style.overflow = 'auto';
                        img.style.cursor = 'grab';
                    } else {
                        img.style.cursor = 'default';
                    }
                }

                zoomInBtn?.addEventListener('click', (e) => {
                    e.stopPropagation();
                    scale = Math.min(scale + scaleStep, maxScale);
                    updateTransform();
                });

                zoomOutBtn?.addEventListener('click', (e) => {
                    e.stopPropagation();
                    scale = Math.max(scale - scaleStep, minScale);
                    updateTransform();
                });

                zoomResetBtn?.addEventListener('click', (e) => {
                    e.stopPropagation();
                    scale = 1;
                    updateTransform();
                    // Scroll back to top
                    gsap.to(container, {
                        scrollTop: 0,
                        scrollLeft: 0,
                        duration: 0.3,
                        ease: 'power2.out'
                    });
                });

                // Mouse wheel zoom (with ctrl/cmd key)
                container.addEventListener('wheel', (e) => {
                    if (e.ctrlKey || e.metaKey) {
                        e.preventDefault();
                        const delta = e.deltaY > 0 ? -scaleStep : scaleStep;
                        scale = Math.max(minScale, Math.min(maxScale, scale + delta));
                        updateTransform();
                    }
                }, { passive: false });
            });

            // Hide scroll indicator when scrolling
            document.querySelectorAll('.scrollable-slide').forEach(slide => {
                const indicator = slide.querySelector('.scroll-indicator');
                if (indicator) {
                    slide.addEventListener('scroll', () => {
                        if (slide.scrollTop > 50) {
                            gsap.to(indicator, { opacity: 0, duration: 0.3 });
                        } else {
                            gsap.to(indicator, { opacity: 1, duration: 0.3 });
                        }
                    });
                }
            });
        });

        // Loss chart
        const ctx = document.getElementById('lossChart');
        if (ctx) {
            new Chart(ctx, {
                type: 'line',
                data: {
                    labels: [0, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500],
                    datasets: [{
                        label: 'Training Loss',
                        data: [8.23, 6.5, 4.8, 3.5, 2.7, 2.1, 1.6, 1.3, 1.1, 0.95, 0.85],
                        borderColor: '#4ade80',
                        backgroundColor: 'rgba(74, 222, 128, 0.1)',
                        tension: 0.4,
                        borderWidth: 3
                    }]
                },
                options: {
                    responsive: true,
                    plugins: {
                        legend: { display: false }
                    },
                    scales: {
                        y: {
                            beginAtZero: true,
                            title: { display: true, text: 'Loss', color: '#ccc' },
                            ticks: { color: '#ccc' },
                            grid: { color: '#444' }
                        },
                        x: {
                            title: { display: true, text: 'Training Step', color: '#ccc' },
                            ticks: { color: '#ccc' },
                            grid: { color: '#444' }
                        }
                    }
                }
            });
        }
    </script>
</body>
</html>
