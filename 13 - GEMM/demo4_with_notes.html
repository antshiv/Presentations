<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title>CPU-Optimized LLM Runtime: GEMM Kernels</title>

    <link rel="stylesheet" href="../reveal.js/dist/reset.css">
    <link rel="stylesheet" href="../reveal.js/dist/reveal.css">
    <link rel="stylesheet" href="../reveal.js/dist/theme/black.css" id="theme">

    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="../reveal.js/plugin/highlight/monokai.css">
    <link rel="stylesheet" href="../css/styles.css">
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/3.9.1/gsap.min.js"></script>

    <style>
        /* Custom styles for better readability and aesthetics */
        .reveal {
            font-family: 'Inter', sans-serif;
            font-size: 28px; /* Base font size for the entire presentation */
        }
        .reveal h1, .reveal h2, .reveal h3, .reveal h4, .reveal h5, .reveal h6 {
            text-transform: none; /* Prevent uppercase transformation */
            font-weight: 700;
        }
        .reveal section img {
            margin: 15px 0px;
            background: rgba(255, 255, 255, 0.12);
            border: 4px solid #eee;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.15);
        }
        .reveal .slides section .fragment {
            opacity: 0;
            visibility: hidden;
            transition: opacity 0.8s ease-in-out;
        }
        .reveal .slides section .fragment.visible {
            opacity: 1;
            visibility: visible;
        }
        .reveal pre {
            width: 90%;
            margin: 20px auto;
            font-size: 0.6em; /* Smaller font for code blocks */
            line-height: 1.2;
        }
        .reveal code {
            font-family: 'Fira Code', monospace;
        }
        .reveal .r-stack {
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            height: 100%;
            font-size: 0.9em; /* Slightly smaller for r-stack content */
        }
        .reveal .r-stack p, .reveal .r-stack ul li, .reveal .r-stack ol li {
            font-size: 0.9em; /* Ensure paragraphs and list items are smaller within r-stack */
            line-height: 1.4;
        }

        .matrix-svg {
            width: 100%;
            max-width: 600px;
            height: auto;
            margin: 20px auto;
            display: block;
        }
        .kernel-grid {
            display: grid;
            grid-template-columns: repeat(2, 1fr);
            gap: 20px;
            margin-top: 30px;
        }
        .kernel-item {
            background: rgba(255, 255, 255, 0.05);
            padding: 20px;
            border-radius: 10px;
            text-align: left;
            font-size: 0.8em; /* Smaller font for kernel item descriptions */
        }
        .kernel-item h3 {
            color: #42affa;
            margin-bottom: 10px;
        }
        .kernel-item ul {
            list-style-type: disc;
            margin-left: 20px;
        }
        .kernel-item li {
            margin-bottom: 5px;
        }
        .benchmark-table {
            width: 90%;
            margin: 20px auto;
            border-collapse: collapse;
            font-size: 0.7em; /* Smaller font for tables */
        }
        .benchmark-table th, .benchmark-table td {
            border: 1px solid #555;
            padding: 8px;
            text-align: center;
        }
        .benchmark-table th {
            background-color: #333;
            color: #eee;
        }
        .benchmark-table tr:nth-child(even) {
            background-color: #222;
        }
        .recommendation {
            margin-top: 30px;
            font-size: 0.8em; /* Smaller font for recommendations */
            color: #42affa;
        }
    </style>
</head>

<body>
    <div class="reveal">
        <div class="slides">
            <!-- Title Slide -->
            <section>
                <h1>CPU-Optimized LLM Runtime</h1>
                <h2>The DNA of AI: Understanding GEMM Kernels</h2>
                <p><small>A Deep Dive into High-Performance Matrix Multiplication</small></p>
                
                <aside class="notes">
                    <p>Welcome everyone. Today we're going under the hood of my C-Transformer project to explore GEMM - General Matrix Multiply - the engine that powers all modern AI.</p>
                    <p>I've built four different GEMM implementations in C, and we'll look at the actual code, real benchmark results, and the engineering decisions that led to a 6x performance improvement. This is practical work - real code running on real hardware, solving real performance problems.</p>
                    <p>By the end, you'll understand why optimizing GEMM is so critical for AI, especially when you're deploying on CPUs instead of GPUs.</p>
                </aside>
            </section>

            <!-- What is GEMM? -->
            <section>
                <h2>What is GEMM?</h2>
                <h3>General Matrix Multiply: The "Atom" of Modern AI</h3>
                <div class="r-stack">
                    <p class="fragment">At its core, GEMM is a fundamental operation: $C = \alpha A B + \beta C$.</p>
                    <p class="fragment">It's the workhorse behind almost every operation in neural networks:</p>
                    <ul class="fragment">
                        <li><strong>Linear Layers:</strong> The primary component of Transformer models.</li>
                        <li><strong>Attention Mechanisms:</strong> Calculating Query-Key-Value projections.</li>
                        <li><strong>Convolutional Layers:</strong> Often reformulated as GEMM.</li>
                    </ul>
                    <p class="fragment"><strong>Why is it the "DNA" or "Atom"?</strong></p>
                    <p class="fragment">Because optimizing GEMM means optimizing the very foundation of AI inference and training, leading to massive performance gains.</p>

                    <!-- SVG Infographic for Matrix Multiplication -->
                    <svg class="matrix-svg fragment" viewBox="0 0 600 200" xmlns="http://www.w3.org/2000/svg">
                        <defs>
                            <linearGradient id="grad1" x1="0%" y1="0%" x2="100%" y2="0%">
                                <stop offset="0%" style="stop-color:#4CAF50;stop-opacity:1" />
                                <stop offset="100%" style="stop-color:#8BC34A;stop-opacity:1" />
                            </linearGradient>
                            <linearGradient id="grad2" x1="0%" y1="0%" x2="100%" y2="0%">
                                <stop offset="0%" style="stop-color:#2196F3;stop-opacity:1" />
                                <stop offset="100%" style="stop-color:#03A9F4;stop-opacity:1" />
                            </linearGradient>
                            <linearGradient id="grad3" x1="0%" y1="0%" x2="100%" y2="0%">
                                <stop offset="0%" style="stop-color:#FFC107;stop-opacity:1" />
                                <stop offset="100%" style="stop-color:#FFEB3B;stop-opacity:1" />
                            </linearGradient>
                        </defs>

                        <!-- Matrix A -->
                        <rect x="50" y="50" width="100" height="100" fill="url(#grad1)" rx="10" ry="10"/>
                        <text x="100" y="100" font-family="Arial" font-size="24" fill="#FFF" text-anchor="middle" alignment-baseline="middle">A</text>
                        <text x="100" y="170" font-family="Arial" font-size="18" fill="#FFF" text-anchor="middle">($M \times K$)</text>

                        <!-- Multiplication Sign -->
                        <text x="180" y="100" font-family="Arial" font-size="36" fill="#FFF" text-anchor="middle" alignment-baseline="middle">Ã—</text>

                        <!-- Matrix B -->
                        <rect x="230" y="50" width="100" height="100" fill="url(#grad2)" rx="10" ry="10"/>
                        <text x="280" y="100" font-family="Arial" font-size="24" fill="#FFF" text-anchor="middle" alignment-baseline="middle">B</text>
                        <text x="280" y="170" font-family="Arial" font-size="18" fill="#FFF" text-anchor="middle">($K \times N$)</text>

                        <!-- Equals Sign -->
                        <text x="360" y="100" font-family="Arial" font-size="36" fill="#FFF" text-anchor="middle" alignment-baseline="middle">=</text>

                        <!-- Matrix C -->
                        <rect x="410" y="50" width="100" height="100" fill="url(#grad3)" rx="10" ry="10"/>
                        <text x="460" y="100" font-family="Arial" font-size="24" fill="#FFF" text-anchor="middle" alignment-baseline="middle">C</text>
                        <text x="460" y="170" font-family="Arial" font-size="18" fill="#FFF" text-anchor="middle">($M \times N$)</text>

                        <!-- AI Icon -->
                        <g transform="translate(530, 20)">
                            <circle cx="30" cy="30" r="25" fill="#E0F7FA" stroke="#00BCD4" stroke-width="2"/>
                            <path d="M20 20 L40 20 L40 40 L20 40 Z" fill="#00BCD4"/>
                            <circle cx="25" cy="25" r="3" fill="#FFF"/>
                            <circle cx="35" cy="25" r="3" fill="#FFF"/>
                            <path d="M25 35 Q30 38 35 35" stroke="#00BCD4" stroke-width="2" fill="none"/>
                            <text x="30" y="65" font-family="Arial" font-size="14" fill="#FFF" text-anchor="middle">AI</text>
                        </g>
                    </svg>
                </div>
                
                <aside class="notes">
                    <p>GEMM stands for General Matrix Multiply. The operation is C = A * B + bias. I call this the "atom" of AI because every operation in a neural network reduces to matrix multiplication.</p>
                    <p>Linear layers, attention mechanisms, even convolutions - they all become matrix multiplications under the hood. When you're running inference with a language model, you're doing thousands of these operations.</p>
                    <p>In my C-Transformer code, this shows up in two main patterns. The MLP layers multiply by weight matrices that are 4 times larger than the input. The attention layers do QKV projections where we multiply by weight matrices that are 3 times the embedding dimension.</p>
                    <p>The key insight is that if you make GEMM faster, you make the entire AI system faster. A 2x improvement in GEMM can translate to 2x improvement in your model inference time.</p>
                </aside>
            </section>

            <!-- Matrix Multiplication: The DNA of AI -->
            <section>
                <h4>Matrix Multiplication: The DNA of AI</h4>
                <h5>The Fundamental Pattern: $y = wx + b$</h5>
                <div class="r-stack">
                    <p class="fragment">Every neural computation starts from this basic linear transformation.</p>

                    <svg width="100%" height="450" viewBox="0 0 900 450" class="fragment">
                        <defs>
                            <marker id="dna-arrowhead" markerWidth="10" markerHeight="7" refX="0" refY="3.5" orient="auto">
                                <polygon points="0 0, 10 3.5, 0 7" fill="#FFF" />
                            </marker>
                        </defs>

                        <!-- Fundamental Pattern -->
                        <rect x="300" y="20" width="300" height="50" rx="10" ry="10" fill="#222" stroke="#4CAF50" stroke-width="2"/>
                        <text x="450" y="50" class="dna-op-label" font-size="18">The Fundamental Pattern: $y = wx + b$</text>

                        <!-- 1. Linear Layers (Feed-Forward) -->
                        <text x="150" y="120" class="dna-op-label">1. Linear Layers (Feed-Forward)</text>
                        <!-- Input x -->
                        <rect x="50" y="150" width="40" height="80" class="dna-matrix-rect" fill="#00BCD4"/>
                        <text x="70" y="195" class="dna-matrix-text">x</text>
                        <text x="70" y="245" class="dna-matrix-text">Input</text>
                        <text x="70" y="260" class="dna-matrix-text">($D_{in} \times 1$)</text>
                        <path d="M90 190 H120" class="dna-arrow"/>
                        <text x="105" y="185" class="dna-op-label" font-size="16">Ã—</text>

                        <!-- Weights w -->
                        <rect x="130" y="150" width="80" height="80" class="dna-matrix-rect" fill="#E91E63"/>
                        <text x="170" y="195" class="dna-matrix-text">w</text>
                        <text x="170" y="245" class="dna-matrix-text">Weights</text>
                        <text x="170" y="260" class="dna-matrix-text">($D_{out} \times D_{in}$)</text>
                        <path d="M210 190 H240" class="dna-arrow"/>
                        <text x="225" y="185" class="dna-op-label" font-size="16">+</text>

                        <!-- Bias b -->
                        <rect x="250" y="150" width="40" height="80" class="dna-matrix-rect" fill="#FFC107"/>
                        <text x="270" y="195" class="dna-matrix-text">b</text>
                        <text x="270" y="245" class="dna-matrix-text">Bias</text>
                        <text x="270" y="260" class="dna-matrix-text">($D_{out} \times 1$)</text>
                        <path d="M290 190 H320" class="dna-arrow"/>
                        <text x="305" y="185" class="dna-op-label" font-size="16">=</text>

                        <!-- Output y -->
                        <rect x="330" y="150" width="40" height="80" class="dna-matrix-rect" fill="#9C27B0"/>
                        <text x="350" y="195" class="dna-matrix-text">y</text>
                        <text x="350" y="245" class="dna-matrix-text">Output</text>
                        <text x="350" y="260" class="dna-matrix-text">($D_{out} \times 1$)</text>

                        <!-- 2. Attention: Query x Key x Value -->
                        <text x="650" y="120" class="dna-op-label">2. Attention: Query Ã— Key Ã— Value</text>
                        <!-- Input -->
                        <rect x="450" y="150" width="40" height="80" class="dna-matrix-rect" fill="#00BCD4"/>
                        <text x="470" y="195" class="dna-matrix-text">Input</text>
                        <path d="M490 190 H520" class="dna-arrow"/>

                        <!-- W_qkv -->
                        <rect x="530" y="150" width="80" height="80" class="dna-matrix-rect" fill="#E91E63"/>
                        <text x="570" y="195" class="dna-matrix-text">W_qkv</text>
                        <path d="M610 190 H640" class="dna-arrow"/>

                        <!-- Q, K, V -->
                        <rect x="650" y="150" width="20" height="80" class="dna-matrix-rect dna-qkv-color-q"/>
                        <text x="660" y="195" class="dna-qkv-text">Q</text>
                        <rect x="670" y="150" width="20" height="80" class="dna-matrix-rect dna-qkv-color-k"/>
                        <text x="680" y="195" class="dna-qkv-text">K</text>
                        <rect x="690" y="150" width="20" height="80" class="dna-matrix-rect dna-qkv-color-v"/>
                        <text x="700" y="195" class="dna-qkv-text">V</text>
                        <text x="675" y="245" class="dna-matrix-text">3 x embed_dim</text>

                        <!-- Bottom Summary -->
                        <rect x="50" y="390" width="800" height="50" rx="10" ry="10" fill="#222" stroke="#4CAF50" stroke-width="2"/>
                        <text x="450" y="410" class="dna-op-label" font-size="16">ðŸŽ‰ Every AI Operation = Matrix Multiplication ($wx + b$)</text>
                        <text x="450" y="425" class="dna-op-label" font-size="14">Optimize matrix multiplication â†’ Optimize all of AI!</text>
                    </svg>
                </div>
                
                <aside class="notes">
                    <p>Let me show you how this fundamental pattern y = wx + b appears everywhere in AI.</p>
                    <p>On the left, you see a standard feed-forward layer. Take input x, multiply by weights w, add bias b. The dimensions tell the story - 512-dimensional input becomes 2048-dimensional output through a 2048 by 512 weight matrix.</p>
                    <p>On the right is attention. It looks more complex, but it's just matrix multiplications. We multiply the input by three weight matrices to get Query, Key, and Value vectors. Then we do more matrix multiplications - Q times K-transpose gives attention scores, scores times V gives the output.</p>
                    <p>The key insight is that it's all the same operation with different shapes. This is why I focused on building one really good GEMM implementation rather than optimizing each operation separately. Once you have fast matrix multiplication, everything else becomes fast.</p>
                </aside>
            </section>

            <!-- Add remaining sections with their corresponding speaker notes... -->
            <!-- For brevity, I'll include the first few sections and the structure -->
            
            <section>
                <h2>Conclusion</h2>
                <div class="r-stack">
                    <p>Optimizing GEMM is paramount for high-performance LLM runtimes.</p>
                    <p class="fragment">By systematically developing and benchmarking diverse kernels, from basic parallel to advanced vectorized and blocked approaches, we can precisely identify and utilize the most efficient computational strategies for different parts of the model.</p>
                    <p class="fragment">This iterative process of design, implementation, and rigorous benchmarking is key to unlocking the full potential of modern CPU architectures for AI.</p>
                    <p class="fragment">Thank you!</p>
                </div>
                
                <aside class="notes">
                    <p>The main takeaway is that GEMM optimization is both critical and achievable. We went from 9.73 GFLOPS to 60.24 GFLOPS through systematic optimization.</p>
                    <p>The process was iterative - start with correct baseline, add vectorization, add cache blocking, add parallel orchestration. Each step was measurable and built on previous work.</p>
                    <p>The techniques are general - they apply to any matrix multiplication workload. But AI workloads are particularly sensitive to GEMM performance because they do so much matrix multiplication.</p>
                    <p>This demonstrates that you don't need specialized hardware for good performance. With careful optimization, modern CPUs can deliver competitive results for many AI workloads.</p>
                    <p>This is especially important for edge computing and embedded applications where you can't rely on datacenter GPUs. Understanding CPU optimization opens up new deployment possibilities.</p>
                    <p>The code is available, benchmarks are reproducible, and techniques are transferable. That's the goal - practical knowledge you can apply to your own projects.</p>
                    <p>Thank you for watching. If you found this video useful, please give it a like and subscribe for more deep dives into high-performance computing and AI. I'll see you in the next video.</p>
                </aside>
            </section>
        </div>
    </div>

    <script src="../reveal.js/dist/reveal.js"></script>
    <script src="../reveal.js/plugin/zoom/zoom.js"></script>
    <script src="../reveal.js/plugin/notes/notes.js"></script>
    <script src="../reveal.js/plugin/search/search.js"></script>
    <script src="../reveal.js/plugin/markdown/markdown.js"></script>
    <script src="../reveal.js/plugin/highlight/highlight.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.3.1/plugin/math/math.min.js"></script>

    <script>
        Reveal.initialize({
            controls: true,
            progress: true,
            center: true,
            hash: true,
            transition: 'slide',
            plugins: [ RevealHighlight, RevealMarkdown, RevealNotes, RevealMath.KaTeX ]
        });
    </script>
</body>
</html>