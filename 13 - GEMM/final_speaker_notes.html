
<!-- Final Speaker Notes for YouTube GEMM Presentation -->
<!-- Optimized for video delivery - combines the best of both versions -->

<!-- Slide 1: Title Slide -->
<aside class="notes">
  <p>Welcome everyone. Today we're going under the hood of my C-Transformer project to explore GEMM - General Matrix Multiply - the engine that powers all modern AI.</p>
  <p>I've built four different GEMM implementations in C, and we'll look at the actual code, real benchmark results, and the engineering decisions that led to a 6x performance improvement. This is practical work - real code running on real hardware, solving real performance problems.</p>
  <p>By the end, you'll understand why optimizing GEMM is so critical for AI, especially when you're deploying on CPUs instead of GPUs.</p>
</aside>

<!-- Slide 2: What is GEMM? -->
<aside class="notes">
  <p>GEMM stands for General Matrix Multiply. The operation is C = A * B + bias. I call this the "atom" of AI because every operation in a neural network reduces to matrix multiplication.</p>
  <p>Linear layers, attention mechanisms, even convolutions - they all become matrix multiplications under the hood. When you're running inference with a language model, you're doing thousands of these operations.</p>
  <p>In my C-Transformer code, this shows up in two main patterns. The MLP layers multiply by weight matrices that are 4 times larger than the input. The attention layers do QKV projections where we multiply by weight matrices that are 3 times the embedding dimension.</p>
  <p>The key insight is that if you make GEMM faster, you make the entire AI system faster. A 2x improvement in GEMM can translate to 2x improvement in your model inference time.</p>
</aside>

<!-- Slide 3: Matrix Multiplication: The DNA of AI -->
<aside class="notes">
  <p>Let me show you how this fundamental pattern y = wx + b appears everywhere in AI.</p>
  <p>On the left, you see a standard feed-forward layer. Take input x, multiply by weights w, add bias b. The dimensions tell the story - 512-dimensional input becomes 2048-dimensional output through a 2048 by 512 weight matrix.</p>
  <p>On the right is attention. It looks more complex, but it's just matrix multiplications. We multiply the input by three weight matrices to get Query, Key, and Value vectors. Then we do more matrix multiplications - Q times K-transpose gives attention scores, scores times V gives the output.</p>
  <p>The key insight is that it's all the same operation with different shapes. This is why I focused on building one really good GEMM implementation rather than optimizing each operation separately. Once you have fast matrix multiplication, everything else becomes fast.</p>
</aside>

<!-- Slide 4: Memory Layout Optimization -->
<aside class="notes">
  <p>Here's where theory meets practice. The math textbooks show A times B-transpose, but that transpose operation is expensive - you're jumping around in memory instead of reading sequentially.</p>
  <p>In my C-Transformer implementation, I store the weight matrices pre-transposed. Instead of transposing at runtime, I store them in the layout I need. This way, when I'm computing the dot product, I'm reading both matrices sequentially.</p>
  <p>Look at the memory layout. Matrix A - my input tokens - stored row by row. Matrix B - my weights - stored pre-transposed so the columns become rows. Sequential access for both matrices means better cache performance.</p>
  <p>This is exactly what you see in my layout_transformer function. I calculate all the offsets ahead of time, align everything to 64-byte boundaries, and store everything in one contiguous block. The mathematical view says A times B-transpose. The implementation view says smart storage means no transpose needed. If you find this kind of low-level detail interesting, let me know in the comments below. It really helps me decide what to focus on in future videos.</p>
</aside>

<!-- Slide 5: The Four Kernels -->
<aside class="notes">
  <p>To find the best performance, I implemented four different GEMM kernels. Each one builds on the previous approach and targets a different bottleneck.</p>
  <p>Naive Parallel is the baseline - three nested loops with OpenMP parallelization. It's a direct translation of the math into code.</p>
  <p>Simple AVX-512 introduces vectorization. Instead of processing one float at a time, we process 16 floats simultaneously using Intel's AVX-512 instructions.</p>
  <p>Fine-Grained Blocked breaks matrices into 64x64 blocks that fit in L1 cache. This improves data locality by keeping the working set small.</p>
  <p>Token-Parallel Orchestration is different. Instead of parallelizing within the matrix multiplication, I distribute the input tokens across CPU cores. Each core gets its own slice of tokens and does matrix multiplication independently.</p>
  <p>You can see all four implementations in my main.c file. Each one targets a different performance bottleneck.</p>
</aside>

<!-- Slide 6: Optimization Differences (Naive vs. Vectorized) -->
<aside class="notes">
  <p>Let's look at the actual code. The naive implementation is three nested loops doing scalar operations. The outer loop is parallelized with OpenMP, but the inner computation processes one element at a time.</p>
  <p>The AVX-512 version uses vector intrinsics. _mm512_loadu_ps loads 16 floats, _mm512_fmadd_ps does 16 multiply-adds simultaneously, _mm512_reduce_add_ps sums them up.</p>
  <p>Modern CPUs have 512-bit wide vector units. If I'm only using 32 bits at a time, I'm leaving 15/16ths of my computational capability unused. But vectorization requires sequential data access, which is why the memory layout optimization was so important.</p>
  <p>The benchmark shows AVX-512 gives about 20% improvement. That's less than the theoretical 16x because we're memory bound, not compute bound. This tells us we need to look beyond just computation.</p>
</aside>

<!-- Slide 7: Optimization Differences (Blocking & Orchestration) -->
<aside class="notes">
  <p>Fine-Grained Blocked addresses the memory bottleneck. Instead of working with entire matrices, we use 64x64 blocks that fit in L1 cache. The code structure changes to six nested loops - outer three iterate over blocks, inner three within blocks.</p>
  <p>There's a challenge here - when multiple threads update the same output element, you need atomic operations. That's the pragma omp atomic in the code.</p>
  <p>Token-Parallel Orchestration takes a completely different approach. Instead of parallelizing within the matrix multiplication, I distribute work at a higher level. Each core gets a subset of input tokens and does its own matrix multiplication.</p>
  <p>This was the key insight that led to the 6x performance improvement. Instead of cores competing for the same data, each core works independently. Core 0 handles tokens 0-21, Core 1 handles tokens 22-43, and so on.</p>
  <p>This avoids atomic operations, reduces cache conflicts, and maximizes CPU utilization. It's like giving each worker their own tool instead of having them fight over the same tool.</p>
</aside>

<!-- Slide 8: Pipe and Buckets Analogy -->
<aside class="notes">
  <p>I like to think of GEMM optimization as water flowing through pipes. The pipe is your computational throughput, the buckets are your cache hierarchy.</p>
  <p>Naive Parallel is like a narrow pipe with disorganized buckets. The water - your data - gets stuck and flows slowly.</p>
  <p>Simple AVX-512 makes the pipe wider, but the buckets are still disorganized. You get some improvement, but data still gets stuck.</p>
  <p>Fine-Grained Blocked cleans the pipe and organizes the buckets. Now water flows smoothly because data is where you need it when you need it.</p>
  <p>Token-Parallel Orchestration gives each core its own dedicated pipe and bucket system. No interference, maximum throughput.</p>
  <p>The analogy helps explain why different optimizations work. It's not just about computation speed - it's about the entire data flow being efficient.</p>
</aside>

<!-- Slide 9: Real-World Performance: Analogy Meets Data -->
<aside class="notes">
  <p>Here we see the analogy mapped to real benchmark data. The pipe diagrams on the left correspond to the performance charts on the right.</p>
  <p>For MLP workloads, Token-Parallel Orchestration achieves 60.24 GFLOPS - 6.19x faster than naive. For QKV workloads, 65.63 GFLOPS - 6.29x speedup.</p>
  <p>What's interesting is that Fine-Grained Blocked gives us most of the benefit - about 4.8x speedup. Token-Parallel adds another 25% on top of that.</p>
  <p>The optimizations are cumulative. You need vectorization, cache blocking, and parallel orchestration working together. No single optimization gives you the full benefit.</p>
  <p>Also notice the absolute numbers are similar between workloads. This suggests we're hitting memory bandwidth limits, not compute limits. This is typical for AI workloads on modern CPUs. If you're enjoying this breakdown, hitting that subscribe button is the best way to support the channel and see more content like this.</p>
</aside>

<!-- Slide 10: Environment Context -->
<aside class="notes">
  <p>These benchmarks run on an Intel Xeon Platinum 8468V with 192 logical cores. It's a high-end server processor with all the modern features - AVX-512, AMX tiles, VNNI instructions.</p>
  <p>This is a virtualized, shared environment, so absolute performance numbers might vary with system load. But that's not what matters.</p>
  <p>The key insight is the relative performance improvements. The 6x speedup from Token-Parallel Orchestration should hold across different systems, even if the absolute GFLOPS numbers change.</p>
  <p>This is why I focus on optimization strategies rather than raw numbers. Understanding why Token-Parallel works better than Fine-Grained Blocked is more valuable than the exact GFLOPS on this particular system.</p>
  <p>The consistent pattern across both MLP and QKV workloads gives me confidence in the conclusions.</p>
</aside>

<!-- Slide 11: How Benchmarking Guides Kernel Selection -->
<aside class="notes">
  <p>The benchmark framework answers practical questions. Given a specific LLM architecture and CPU, which GEMM kernel should I use?</p>
  <p>The process is systematic. Same input data for all kernels ensures fair comparison. I validate accuracy by comparing against the naive implementation. I measure both performance and correctness.</p>
  <p>The benchmark tests realistic workloads - MLP layers with 4x expansion, attention with 3x QKV projections. These are actual matrix shapes from transformer models.</p>
  <p>Using the model's own memory layout is crucial. In production, GEMM kernels operate on data laid out exactly this way. Testing with artificial layouts gives misleading results.</p>
  <p>The outcome is actionable. For this CPU and these matrix shapes, use Token-Parallel Orchestration. For different architectures, the answer might be different.</p>
  <p>This benchmark framework is part of C-Transformer because it's not just about having fast kernels - it's about automatically selecting the right kernel for your specific use case.</p>
</aside>

<!-- Slide 12: Benchmark Results -->
<aside class="notes">
  <p>Here are the detailed results. MLP GEMM tests matrices of size 4096 x 32768 x 8192 - about 2.2 billion floating-point operations.</p>
  <p>Naive implementation takes 226 seconds. AVX-512 reduces to 190 seconds - 19% improvement. Fine-Grained Blocked drops to 47 seconds - almost 5x improvement. Token-Parallel gets to 37 seconds - over 6x improvement.</p>
  <p>The QKV results follow the same pattern with slightly smaller matrices.</p>
  <p>We're achieving 60-65 GFLOPS, which is respectable for CPU implementation. A high-end GPU might achieve 500-1000 GFLOPS, but CPUs are more accessible and don't require special hardware.</p>
  <p>The key insight is that optimization matters more than raw hardware capability. Well-optimized CPU implementation can compete with poorly optimized GPU implementations, especially for smaller batch sizes.</p>
  <p>These results validate the optimization strategies and give confidence in the approach.</p>
</aside>

<!-- Slide 13: Conclusion -->
<aside class="notes">
  <p>The main takeaway is that GEMM optimization is both critical and achievable. We went from 9.73 GFLOPS to 60.24 GFLOPS through systematic optimization.</p>
  <p>The process was iterative - start with correct baseline, add vectorization, add cache blocking, add parallel orchestration. Each step was measurable and built on previous work.</p>
  <p>The techniques are general - they apply to any matrix multiplication workload. But AI workloads are particularly sensitive to GEMM performance because they do so much matrix multiplication.</p>
  <p>This demonstrates that you don't need specialized hardware for good performance. With careful optimization, modern CPUs can deliver competitive results for many AI workloads.</p>
  <p>This is especially important for edge computing and embedded applications where you can't rely on datacenter GPUs. Understanding CPU optimization opens up new deployment possibilities.</p>
  <p>The code is available, benchmarks are reproducible, and techniques are transferable. That's the goal - practical knowledge you can apply to your own projects.</p>
  <p>Thank you for watching. If you found this video useful, please give it a like and subscribe for more deep dives into high-performance computing and AI. I'll see you in the next video.</p>
</aside>
